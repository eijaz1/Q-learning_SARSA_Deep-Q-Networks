{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from gym.envs.registration import register\n",
    "from IPython.display import clear_output\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Reshape\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.optimizers import Adam\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set output format for fonts and graphs\n",
    "np.set_printoptions(precision=2)\n",
    "plt.rcParams.update({'font.size': 18})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set frozen lake environment parameters\n",
    "\n",
    "register(\n",
    "    id='FrozenLakeNotSlippery-v0',\n",
    "    entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
    "    kwargs={'map_name' : '4x4', 'is_slippery': False}\n",
    ")\n",
    "\n",
    "env = gym.make('FrozenLakeNotSlippery-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initialize q-table using environment's state and action size\n",
    "\n",
    "#16 states\n",
    "num_states = env.observation_space.n\n",
    "\n",
    "#4 actions\n",
    "num_actions = env.action_space.n\n",
    "\n",
    "#initialize q-table\n",
    "q_table = np.zeros((num_states, num_actions))\n",
    "\n",
    "q_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up our parameters for training our agent with q-learning\n",
    "\n",
    "#training duration\n",
    "num_episodes = 5000\n",
    "max_steps_per_episode = 100\n",
    "\n",
    "#q-learning parameters\n",
    "learning_rate = 0.01\n",
    "discount_rate = 0.8\n",
    "exploration_rate = 0.9\n",
    "exploration_decay_rate = 0.9999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explore count: 16\n",
      "Exploit count: 29998\n",
      "\n",
      "Q table after training\n",
      "\n",
      "[[1.01e-01 3.28e-01 2.26e-02 1.18e-01]\n",
      " [7.77e-03 0.00e+00 7.65e-02 9.41e-04]\n",
      " [1.16e-03 2.26e-01 3.33e-05 5.69e-03]\n",
      " [2.46e-03 0.00e+00 4.67e-06 5.55e-06]\n",
      " [1.58e-01 4.10e-01 0.00e+00 1.24e-01]\n",
      " [0.00e+00 0.00e+00 0.00e+00 0.00e+00]\n",
      " [0.00e+00 5.31e-01 0.00e+00 3.89e-03]\n",
      " [0.00e+00 0.00e+00 0.00e+00 0.00e+00]\n",
      " [2.12e-01 0.00e+00 5.12e-01 1.65e-01]\n",
      " [2.07e-01 3.37e-01 6.40e-01 0.00e+00]\n",
      " [2.93e-01 8.00e-01 0.00e+00 2.01e-01]\n",
      " [0.00e+00 0.00e+00 0.00e+00 0.00e+00]\n",
      " [0.00e+00 0.00e+00 0.00e+00 0.00e+00]\n",
      " [0.00e+00 6.31e-02 7.29e-01 3.27e-02]\n",
      " [3.43e-01 6.20e-01 1.00e+00 4.10e-01]\n",
      " [0.00e+00 0.00e+00 0.00e+00 0.00e+00]]\n"
     ]
    }
   ],
   "source": [
    "#train our agent using q-learning\n",
    "\n",
    "rewards_all_episodes = []\n",
    "steps_all_episodes = []\n",
    "explore_count = 0\n",
    "exploit_count = 0\n",
    "\n",
    "#training intructions for each episode\n",
    "for episode in range(num_episodes):\n",
    "    \n",
    "    #reset environment to starting state\n",
    "    state = env.reset()\n",
    "    \n",
    "    done = False\n",
    "    rewards_current_episode = 0\n",
    "    steps = 0\n",
    "    \n",
    "    #training instructions for each step in an episode\n",
    "    for step in range(max_steps_per_episode): \n",
    "        \n",
    "\n",
    "        \n",
    "        #agent choosing to explore or exploit environment\n",
    "        exploration_rate_threshold = random.uniform(0, 1)\n",
    "        if exploration_rate_threshold > exploration_rate:\n",
    "            #exploit: agent chooses action with highest q-table value\n",
    "            action = np.argmax(q_table[state,:])\n",
    "            exploit_count +=1\n",
    "        else:\n",
    "            #explore: agent chooses random action\n",
    "            action = env.action_space.sample()\n",
    "            explore_count +=1\n",
    "\n",
    "        #agent takes action\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "\n",
    "        #q-table is updated based on action taken\n",
    "        q_table[state, action] = q_table[state, action] + learning_rate * (reward + discount_rate * np.max(q_table[next_state, :]) - q_table[state, action])\n",
    "\n",
    "        #update state and reward\n",
    "        state = next_state\n",
    "        rewards_current_episode += reward\n",
    "\n",
    "        steps +=1\n",
    "\n",
    "        exploration_rate = exploration_rate * exploration_decay_rate\n",
    "        \n",
    "        if done == True:\n",
    "            break\n",
    "            \n",
    "    # Add current episode reward to total rewards list\n",
    "    rewards_all_episodes.append(rewards_current_episode)\n",
    "    steps_all_episodes.append(steps)\n",
    "    \n",
    "#     if episode % 100 == 0:\n",
    "#         clear_output(wait=True)\n",
    "#         print(f\"Episode: {episode}\")\n",
    "\n",
    "print(f'Explore count: {explore_count}')\n",
    "print(f'Exploit count: {exploit_count}')\n",
    "\n",
    "#print updated q table\n",
    "print('\\nQ table after training\\n')\n",
    "print(q_table)  \n",
    "\n",
    "#[6] - reference list in report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate and print the average reward per hundred episodes\n",
    "#uncomment print statements to visualise\n",
    "\n",
    "rewards_per_hundred_episodes = np.split(np.array(rewards_all_episodes), num_episodes/100)\n",
    "count = 100\n",
    "# print('Average reward per 100 episodes\\n')\n",
    "for r in rewards_per_hundred_episodes:\n",
    "#     print(count, \" : \", str(sum(r/100)))\n",
    "    count +=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate and print the average steps per hundred episodes\n",
    "#uncomment print statements to visualise\n",
    "\n",
    "steps_per_hundred_episodes = np.split(np.array(steps_all_episodes), num_episodes/100)\n",
    "count = 100\n",
    "# print('Average steps per 100 episodes\\n')\n",
    "for r in steps_per_hundred_episodes:\n",
    "#     print(count, \" : \", str(sum(r/100)))\n",
    "    count +=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABKEAAAIGCAYAAACMDpxLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAIABJREFUeJzs3XmcZGddL+DvLwkEEQSvhKuAGOSiqEhEArKIIiiiCCji7hVcwKuieF1BvBACyCaLLKIQMYAssgYQJCGBQIAEEkISyALZV7InM5lMZsnMe/+oqklNT1V3VXdVd3X183w+5Zk+5z3nfc+pU8j58r7vqdZaAAAAAGCa9lvrBgAAAAAw/4RQAAAAAEydEAoAAACAqRNCAQAAADB1QigAAAAApk4IBQAAAMDUCaEAYARVdVFVte7nJxYp1ytzp9Vs3zh6bVzrdkxTVT2wqj5WVddX1e7uOT96hP2qqn6jqj5RVVdV1Y6quq6qzqmq91fVs6vqoFU4BcZQVUf2/fYW+/ziBOp6evdYR06g6RO1EX7bAKxvB6x1AwBgHfqHJD++1o1gsG4A+F9JvjvJSUnOTbI7yZVL7HdAkvcl6QUVX07y2SQtyf266385yfnd4/f2OyzJC5K8sLV22OTOhGU4Pclpi2y/ZLUaAgDsSwgFAOPZmuSRVfWE1trH1roxDPSQdAKoE1prQ3utDfDH6QRN30zyc6210/s3dntA/XqSqybVUCbuqFUIAj+UTri5acr1AMDcMRwPAMbzhu7yxVVVa9oShvnu7vKCMff71e7yhQsDqCRprV3TWnt9a+3kFbWOda21tqm1dk5r7Ztr3RYAWG+EUAAwnncmOTPJj+S20GJJi83VUlUHd7dfNGx9Ve1fVX9TVWdX1S3ddS/sDiFLVX1Pd16cb1bVtqo6taqesESbqqr+pKrO6B7z6u4x7rXIPvepqn+uqvO69dxYVZ+uqqcMKd+bS+vgqvrVqvpcVW3qrrvrkheuc4zvraojquri7hxN13bne/qZBeUe3b3Gb+uuelrfXEDHj1DV3bvLRYftLTy/dIbiJckLFsw/dNiCsneuqr+vqq9U1U1VtbWqTquqv6qq2w84dm+eo6dX1aF9c1zd3L2OvzCkTfesqldV1VlVtblb1/lV9aGq+uUxzu34bv2Prqqf6f69qXvMY6rq4Yvsu5JzfXBVHdW9H3dPYh6nIW3s/33drtveb3Tv68uq6nVV9e0D9hs6J1RVPaWqju3uv72qrqyqU6rqH2vAXGJVdUhVvbuqruje21dW1fuq6tBF2n3f7j7XdK/r6VX1RyOc71jfCQBMgxAKAMazO8n/6/778Kraf5XqfU+33nOTHJfkbkmen+RNVfW/knwpyaOTnJjkq0kelOTDVfVTixzztd3P1Uk+nGRbkqcl+WJVHbywcFU9LskZSf4oyc4kH0tn/p2HJflAVf3DInX9bZL/TOe/e3wsnfmWlpxAuaoekeQrSX4/naGQH0gnBHx8kmOq6jl9xa9MJ4D6fPfv87t/vy3JJ5aqK8ml3eUfjvFQ/v505iFKd/m2vs+euYmq6nvSOecXJfmf6cw19akk90ryyiRHL1Lnw5J8Lsl9kxydzvV4ZJKPVtUf9hesqnt0t/9FktsnOTbJf6czhPBxSZ4x4nn1++Vuvd+W5ONJzkvyM0k+MygIW+G5Piqde/gH0rnPj0vnXpumSmcusOel8/v6aDrX7k+TfG5QEDXwIFUvSef+fFSSr3f/fVqSuyT5y3S+v/7yT0nnd/vr6QwBfX86c1Y9NcmJVfVbA+p4QJKTu/tsTud3e32SN1TVaxdp20q+EwCYnNaaj4+Pj4+PzxKfJBelE5o8oPv3l7p///6Ccq37udOg9UOOfXB3+0VD1rckZyX5zr5tP5Rke5Jd3W2vS3JA3/aXdvf79ID6ese8KcmP9a2/fTphV0tyzIJ97pXOHDg7kvzqgm3377s+jxly3XYkedyY1/wO6QRDLclhSapv22PTCc12J3nEgv2e3t3nyDHr+9W+a3N5kjcl+d0kP5xkv0X2O6zXxiHbK53goCV5eZID+7bdNZ2ArCU5fMF+R/a15+ULzv+Xut/9tiTf27f+Bd3ybxjQjjslefgY1+P4vvr/ZMG2P+uuvybJXSZ8ri/oP9cR29rbf+B3sMTvrvd9369v2537zv/NS91f3Xv1lnSCoe8dUNchSe7e9/d3dcu2JL+3oOxvd9dvTXKfBdu+0t32piT7961/VJKbe+czifvPx8fHx8dnGh89oQBgef6uu3x+VR24CvX9WWttzzCx1tqZ6fRK2S+dB+C/bK3d2lf+5d3lI6vqdkOO+YbW2hf7jrkjnd4fW5P8TFX9QF/Z/5tOT5gXt9be23+Q1to56fT0SJJnDanrra21YxY7wQF+NZ3w6+x0HpD39JxqrR2X5M3pPGA/e8zjDtQ9rz9NJxy4R5L/k+St6fT+uraq/qWqvnuRQwzzC0kOTScQ/NvW2va+Om9MJ+jakeSPqwbOM3Zpkr9fcP4fSvLBJAcm6e8N1RtSeOyA89vSWjtxGe0/qbX2xgXHel06wcbdkvxm36aVnuvZSV7Uf65jWjgkcq/PIvsd3lo7t6+tN6VzL7ckv1NLDx29czq/wwtaa/vMRdZaO721dnXfqmd09zm6tfbWBWX/I53eWN+Szj2YJKmqn0hnGPC16fzed/Xtc0KSfxnStpV+JwAwMUIoAFiG1tqxST6d5N7pe1Cckp3duhY6r7s8vrW215Cl7sPltUlul05QMMi7F65orV2TpBcW9b9Z7vHd5QeGHOsz3eXDhmz/0JD1i+nV/67W2u4B24/sLn9yGcceqLX2hnQmNn9akn9PZ2jj7iTfnk7Yc0ZVPXTMwy567Vpngutzk3xHkvsNKPL+hd9v1390l/3f0ynd5cu78xPdacy2DrLPfdL1zgH1r/RcPzzkux7VwiGRCz/DDPotfK17vAOT/NhilXZ/NxcnOaSqXr0gwB2kd83+Y8j2I7vL/nu79++jWmtbB+zzjiHHWul3AgATI4QCgOXr9Yb6uwk97A9zZX+vhz43d5eXDdmvt31YT62Lh6y/qLvsn6D84O7ya0N6mFzT3b7P5MtdlwxZv5h7dpcXDtne63HyP6s7QfsktNY2t9be3lr7vdbaA9PpXfSnSW5MZ/jSkWMe8uDu8g2L9ND5oW6ZQddvnO+pF7Z8Xzqhw41V9eXuxNg/Mma7l1P/wd3lcs91OfdJv6Naa08f9hmyz42ttc1Dtl3UXQ6drL/P/05yRTq9Bs/qTqz+war6g6q644Kyo97b9+xb1/v3RRls2PqDu8vlficAMDET+y9sALDRtNZOqqqPJnlikj9P8uJlHmqp/1FoqZ4hy+05Mmx4Ug3Y3mvju7K8iaJvWcY+Sw0NakP+PVGttevSeYC/LJ0eXT9QVffrH761hN61+1Rum/x8mOsGNWFI2X2+p24voqdX1SvSGYb16CSPSPKjSf6yql7UWnv+iO0eu/6s/FyXc5+s1GL3zqBzHHyQ1k6oqvsl+dnu58fTmbvrl5L8v6r6idZaL9AbddjboHqHtWXY+pV+JwAwMUIoAFiZ56XzsP9XVfXGRcrtTHK7qrpTa23Lgm3LmWdoEr4nneFmg9Ynncmaey5NZ6jO81tr50+7YV29Hl73GbL9e7vLq4b0FJu04/r+fVA6Q5hG0Xvwf1dr7d+WUe/3LLH+8oUbWmtnpTNh/Su6vcSemk4Prr+vqne31s6eUv0rPde18O1V9W1DekMNvcaDdIfJfaj7SVXdO525mn4uycuS/Ea36GVJvj+de/vz+x5pzz1/Rd+6XhsOHlL9sN/JevxOAJhThuMBwAq01r6azhvl7pLkbxcp2nuY/P4B2x436XaN6NcXrqiq70jyM90/T+jb9Inu8qnTblSfz3aXv1lVg/47y9O6y88M2Da2ESZl/l99/+4fArmjuxz2P+6t9No9dchww96E4J8dsG2P1tqtrbX3dMtVkgeMWf8+90lXL1Dpr38t7pNJ+LWFK6rqB9N5q932JF/cZ48RtNYuyW09JB/Yt6l3zX57yK6D7u3ev3+xqr5lwD6/NeRY6/U7AWAOCaEAYOWen+TWdOYNGqY3sfjz+gOFqnpcOnPIrIU/raqH9LXldklel+Rbkxzb7U3T86okNyU5rKp+f2EoVFX7VdVjqupnJ9i+96XT++MH0nkL4Z6QqKp+Kp2JwluSf5pQfR+tqr+oqrsv3NDt0XJE98+TuuFCT6+HyrDJqI9K8pUkj6+q11TVtw04/gOq6neH7H/vJIcvOP8npRMqbE/yr33rf6eqHjTg+PdK581qyfjzLj2iqvaafL+q/iSdybqvTWeIZs9Kz3WtvKCq7tv7ozvH2xvSCe3e0Z3of6iq+p7u7+LOAzY/sbvsv+5vSbIlyc8uvBZV9RtJnpRkW/Z+490JSU5LpxfeK6tq/759Hpnkj4Y0b71+JwDMIcPxAGCFWmvnVdW/p/Pa9WFeluRX0pkf5uyqOi2dYTUPTvLyJM+ZdjsHODLJF6rq+HTChEekE3h8M8kz+wu21i6uqqekEwwdkU4YdWaSTelM2ny/dB6OX57k6Ek0rrV2S1X9WpKPJ3lBkl+rqq8kuUeSR6XzP6Y9t7X2hUnUl855vCqdIWxnpjPcbld3/Y8l2T+dHlBPX7Df0Um2JnlKVX02yfnd/T7SWvtIa213Vf1ikv9OZ+6wp1fV6elc57unM4zqPun0tvn3Ae361yR/meSXuuf/3UkemU5A8uettQv6yj4lyduq6tJ03uy2qVvHo5LcIcl7W2vj9up5Q5J/rqo/SPKNdHrz/Wg6wevvttY29QpO4FxX6her6uBFth/VWjtqwbpL0gl3vlZVx6XzXT46nfv57CR/M0K9357O7+KN3e/oonT+e/YD05kkfks693CSzhvpqupp6fSifGtVPSvJOen0tntoOvfPM1prF/bt06rqd9LpEfUn6YRKJ6dzXX8yyRuT/NnChs3AdwIAewihAGAyDk/n7Vh3GLSxtfb1qvqJJP+QTtjz8+mEBE9OZ16mtQihnp3kvHR6FD0qnZ5Ob0/yvNbaPm/ca60dW1U/1N3v59KZeHm/JFem8xD/sXRCqolprX2+qn40yXPTmez5qd12Hp3kta21YyZY3S936/jpdIKWxyS5czpBzonpnN8/L5w7qLV2ZVX9Qjo94h6UznWpdAKrj3TLXFJVh6YT7v1KOsO8HpHOWwUvSfLODL92JyV5azr32M8nuX23PS9trf3XgrKvTudtdo9IJ8y4a5Kr05l36IhF6ljMB7rn/ne5rVfPsUleMCgAXOG5rtQh3c8wF6XTM6hfS6edf5fO8LjvTieUfUM653jDCPWen+Qv0gmvHpDkh9MJki5L8tok/9Rau2ivSlv7YFU9NJ3f/qPTCayuT+d6v7y1dvLCSlprX+32XnxxOsNmfzGd3/CfZ0gI1d1vLb8TANijWpvay2QAAFimqjoynbmBfre1duQa1H98Oj1sfqq1dvxq1z9t3R5TFya5uLV28Jo2BgA2CHNCAQAAADB1QigAAAAApk4IBQAAAMDUmRMKAAAAgKnTEwoAAACAqTtgrRuwmu52t7u1gw8+eK2bAQAAADA3vvzlL1/bWjtoqXIbKoQ6+OCDc8opp6x1MwAAAADmRlVdPEo5w/EAAAAAmDohFAAAAABTJ4QCAAAAYOqEUAAAAABMnRAKAAAAgKkTQgEAAAAwdUIoAAAAAKZOCAUAAADA1AmhAAAAAJg6IRQAAAAAUyeEAgAAAGDqhFAAAAAATJ0QCgAAAICpE0IBAAAAMHVCKAAAAACmTggFAAAAwNQJoQAAAACYOiHUnLnshq05+Dkfyye+9s21bgoAAADAHkKoOfO1yzcnST546uVr3BIAAACA2wihAAAAAJg6IRQAAAAAUyeEAgAAAGDqhFAAAAAATJ0Qak61tW4AAAAAQB8h1JypWusWAAAAAOxLCDVnmi5QAAAAwAwSQs0pHaIAAACAWSKEAgAAAGDqhFBzyqg8AAAAYJYIoeaMickBAACAWSSEAgAAAGDqhFAAAAAATJ0Qas40k0EBAAAAM0gINadMDQUAAADMEiHUnNIhCgAAAJglQqg54+14AAAAwCwSQgEAAAAwdUIoAAAAAKZOCAUAAADA1Amh5lQzMzkAAAAwQ4RQAAAAAEydEGpOeUseAAAAMEuEUAAAAABMnRAKAAAAgKkTQgEAAAAwdUKoOeXteAAAAMAsEULNGfORAwAAALNICAUAAADA1Amh5oxReAAAAMAsEkLNqTIuDwAAAJghQqg5ZWJyAAAAYJYIoeaMDlAAAADALBJCAQAAADB1QigAAAAApk4IBQAAAMDUCaHmjPnIAQAAgFm05iFUVbUhny0Dyn5/VR1VVTdU1c1VdUJVPWYt2g0AAADA6A5Y6wZ0nZDkzQvW7ez/o6rum+QLSW5N8ookm5I8I8nRVfVzrbVjV6Ohs87b8QAAAIBZNCsh1AWttf9YosxLk9w1yYNba6clSVW9PcmZSd5YVfdvrRmNBgAAADCD1nw4Xk9V3b6q7jRk27cmeVKS43sBVJK01rYkOSLJ9yV5yKo0FAAAAICxzUoI9dQkW5PcVFVXV9Xrq+oufdsfmOTAJCcO2Pek7lIIBQAAADCjZmE43peSvC/JeUm+LcnPJ3lWkp+sqkd0ezvdo1v28gH799bdc9oNXV+MTAQAAABmx5qHUK21H1uw6u1VdUaSlyR5dnd5x+627QMOsa27vOOAbamqZyZ5ZpLc+973XnF7Z12VqckBAACA2TMrw/EWemWSHUme0P17a3d54ICyd1hQZi+ttTe31g5trR160EEHTbaVM8jc7AAAAMAsmskQqrW2M8kVSe7WXXVFdzloyF1v3aChehuYHlEAAADA7JjJEKqq7pDkXkmu6q76ajpD8R4+oPjDustTVqFpAAAAACzDmoZQVfUdQza9KJ35qj6aJN3JyT+a5NFVdUjf/ndK8gdJzk1ngnMAAAAAZtBaT0z+91X1sCSfTnJJkjul83a8n0ryxSSv7yv73CSPTXJMVb0myeYkz0hnON4TmsmQFnA5AAAAgNmx1iHU8Ul+MMnTknxHkl3p9Gp6XpJXt9Z6b75La+28qnpkkpcleU6S2yc5NcnjW2vHrnK7Z5a34wEAAACzaE1DqNbah5N8eIzyZyd58vRaBAAAAMA0zOTE5CyfUYkAAADALBJCzS3D8gAAAIDZIYSaW3pEAQAAALNDCDVnTEwOAAAAzCIhFAAAAABTJ4QCAAAAYOqEUAAAAABMnRAKAAAAgKkTQgEAAAAwdUIoAAAAAKZOCAUAAADA1AmhAAAAAJg6IRQAAAAAUyeEmlOtrXULAAAAAG4jhJoztdYNAAAAABhACAUAAADA1Amh5oxReAAAAMAsEkLNqTIuDwAAAJghQigAAAAApk4INae8HQ8AAACYJUKoOWMUHgAAADCLhFAAAAAATJ0QCgAAAICpE0LNGVNBAQAAALNICAUAAADA1Amh5oyJyQEAAIBZJIQCAAAAYOqEUAAAAABMnRAKAAAAgKkTQgEAAAAwdUKoOdXWugEAAAAAfYRQAAAAAEydEGpO1Vo3AAAAAKCPEAoAAACAqRNCAQAAADB1Qqg5ZWJyAAAAYJYIoeZMmQwKAAAAmEFCKAAAAACmTgg1Z5pxeAAAAMAMEkLNKaPyAAAAgFkihAIAAABg6oRQc8qoPAAAAGCWCKHmjLfjAQAAALNICAUAAADA1AmhAAAAAJg6IRQAAAAAUyeEAgAAAGDqhFAAAAAATJ0QCgAAAICpE0IBAAAAMHVCKAAAAACmTggFAAAAwNQJoeZUa22tmwAAAACwhxBqzlStdQsAAAAA9iWEmjM6QAEAAACzSAg1p0qXKAAAAGCGCKEAAAAAmDoh1JwyMTkAAAAwS4RQc8YoPAAAAGAWzVQIVVV3rKoLq6pV1RsGbP/+qjqqqm6oqpur6oSqesxatBUAAACA0c1UCJXk8CR3G7Shqu6b5AtJHp7kFUn+OsmdkhxdVT+9ai0EAAAAYGwzE0JV1Y8m+fMkLxhS5KVJ7prkZ1trL22t/XOSRyW5Iskby+vgkiSmggIAAABm0UyEUFW1f5K3JPlEkg8O2P6tSZ6U5PjW2mm99a21LUmOSPJ9SR6yOq0FAAAAYFwzEUIl+b9J7p/kWUO2PzDJgUlOHLDtpO5SCBUTkwMAAACzac1DqKq6T5IXJjm8tXbRkGL36C4vH7Ctt+6eE24aAAAAABOy5iFUkjcluTDJqxcpc8fucvuAbdsWlNlLVT2zqk6pqlOuueaa5bcSAAAAgGVb0xCqqn47yeOS/J/W2s5Fim7tLg8csO0OC8rspbX25tbaoa21Qw866KDlNxYAAACAZTtgrSquqgPT6f308SRXVtX/6m7qDau7S3fdtem8Aa9/W7/eukFD9QAAAACYAWvZE+pbkhyU5AlJzu37HN/d/tvdv/8gyVfTGYr38AHHeVh3ecoU27rutLVuAAAAAECfNesJleTmJL8yYP1BSf45ySeS/FuSM1prW6rqo0meUlWHtNZOT5KqulM6IdW5Sb60Os0GAAAAYFxrFkJ154B6/8L1VXVw95/nt9b6tz83yWOTHFNVr0myOckz0hmO94TWms4/fWqtGwAAAADQZy17Qo2ltXZeVT0yycuSPCfJ7ZOcmuTxrbVj17RxAAAAACxq5kKo1tpFGdKRp7V2dpInr2qDAAAAAFixtZyYnCkyNhEAAACYJUKoOVNmgwIAAABmkBAKAAAAgKkTQs2ZZiAeAAAAMIOEUHPKoDwAAABglgihAAAAAJg6IdScMigPAAAAmCVCqDnj7XgAAADALBJCAQAAADB1QigAAAAApk4IBQAAAMDUCaEAAAAAmDohFAAAAABTJ4QCAAAAYOqEUAAAAABMnRAKAAAAgKkTQgEAAAAwdUKoOdXaWrcAAAAA4DZCqHlTa90AAAAAgH0JoeaNHlAAAADADBJCzanSIwoAAACYIUIoAAAAAKZOCDWnTEwOAAAAzBIh1LwxDA8AAACYQUIoAAAAAKZOCAUAAADA1Amh5o25oAAAAIAZJISaU2VuKAAAAGCGCKHmlLfjAQAAALNECDVv9IACAAAAZpAQCgAAAICpE0IBAAAAMHVCKAAAAACmTgg1p8xLDgAAAMwSIRQAAAAAUyeEmlNekgcAAADMEiEUAAAAAFMnhAIAAABg6oRQAAAAAEydEGpOeTseAAAAMEuEUHPGhOQAAADALBJCzRk9oAAAAIBZJISaU3pEAQAAALNECAUAAADA1Amh5pRheQAAAMAsEULNGcPwAAAAgFkkhAIAAABg6oRQAAAAAEydEAoAAACAqRNCzRkTkgMAAACzSAgFAAAAwNQdsNIDVNUBSZ6c5H8k+Whr7coVt4pl83Y8AAAAYBaN1ROqql5RVSf3/V1Jjk3y3iT/muSrVXXfyTYRAAAAgPVu3OF4j09yQt/fT0zyE0lemeQ3u+ueM4F2AQAAADBHxh2O991Jzu37+4lJLmytPSdJquqHkvzWhNoGAAAAwJwYtyfU7ZPs6vv7p9IZjtdzQZLvWmmjWLnWvCcPAAAAmB3jhlCXJnlYsqfX0/cm+Uzf9rsn2TKZprEcnWm6AAAAAGbLuMPx3pPk/1XV3ZP8UJLNST7et/1BSc6fUNtYBj2gAAAAgFk0bk+olyY5MsnDk7Qkv9NauzFJquouSZ6U5LhJNpDl0SMKAAAAmCVjhVCtte2ttd9vrX1Ha+17W2sf6dt8UzrzQR026vGq6vur6p1VdXZVbaqqrVV1TlW9uqr2mVuqW/6oqrqhqm6uqhOq6jHjnAMAAAAAq2/c4XhDtdZ2J9k05m73Sie4+lCSy5LcmuSHkzwzya9X1Y+01q5Okqq6b5IvdMu8olvXM5IcXVU/11o7dsDxAQAAAJgBY4dQVXWHJH+W5JfSmZg86bwV70NJXt9au2XUY7XWjsuA4XtV9dkk703y9HQCp6QzFPCuSR7cWjutW+7tSc5M8saqun8zIdIeLgUAAAAwS8YajldVByU5OcnLkvxAksuTXNH998uSnNwts1IXd5ff3q33W9OZb+r4XgCVJK21LUmOSPJ9SR4ygXrXPXNBAQAAALNo3InJX5nkB5P8RZK7t9Z+tLX2oCR3T/KX6YRRrxy3EVV1h6q6W1Xdq6oel+Rfu5t6b957YJIDk5w4YPeTukshFAAAAMCMGnc43hOT/Ftr7bX9K1trO5K8pqp+KJ1heuP6gySv7/v7oiS/3Vo7ofv3PbrLywfs21t3z2XUO3cMwwMAAABm0bgh1O2TnLrI9lOS/Noy2nFUknOS3CnJg9IZetc/rO+O3eX2AftuW1BmL1X1zHQmOs+9733vZTRtfTIsDwAAAJgl44ZQJyf50UW2PzjJl8ZtRGvtsnTejpckR1XVB9KZX+pbWmsvTbK1u+3AAbvfobvcOmBbWmtvTvLmJDn00EM3TDchPaIAAACAWTLunFB/meSpVfWnVXW73sqqOqCqnp3kKd0yK9JaOyPJV5L8cXfVFd3loCF3vXWDhuptOHpAAQAAALNo3J5Qr0pyXZLXJjm8qi5I0pLcN8m3JTk/yasXBCGttfbYZbTtW5L8j+6/v5rOULyHDyj3sO7ylGXUAQAAAMAqGDeE+t50QqdLun/3QqIbu5/bJbnPqAerqu9srV05YP1PJXlAkuOTpLW2pao+muQpVXVIa+30brk7pTOp+blZxjBAAAAAAFbHWCFUa+3gCdf/pqr6riSfSnJxOvM7PTjJrye5KXsP7XtukscmOaaqXpNkc5JnpDMc7wnNJEgAAAAAM2vcOaEm7d3pDO/730n+KcnLkjw0yb8meWBr7bRewdbaeUkemeSkJM9J8o9Jbk7y+Nba0avc7jV35aZtOfeqmyZ2vJ27duekC66b2PEAAAAA+o07HC9JUlX3SadX0v9M8s7W2kVVdfsk35nkytbajlGO01p7b5L3jlpva+3sJE9eRpPnzsNeelyS5KKXPWEix3v5f5+TIz53YT7yrEfmgfe660SOCQAAANAzdk+oqnp5km8keXOSw9OZJyrpDKU7K7e90Y515NyrtyQQmaKXAAAgAElEQVRJrrt5pPwQAAAAYCxjhVBV9YdJ/jrJG5M8Lsme1+C11jYn+UiSJ06ygQAAAACsf+P2hPrjJB9qrf15kq8M2H5Gku9fcasAAAAAmCvjhlDfl+STi2y/Jsndlt8cAAAAAObRuCHUtiTfusj270ly4/KbAwAAAMA8GjeE+lKSXxq0oarukOR/J/n8ShvF8tXSRQAAAABW3bgh1CuTPLyq3pHkgd1131lVP5vk+CT3SvKPk2se42pD1n/xguty8HM+lq9dvmlV2wMAAACQjBlCtdaOTfJHSZ6a5Nju6nck+XiSQ5I8o7V24kRbyER88qyrkiQnnn/dGrcEAAAA2IgOGHeH1tqbq+ojSX4lyf3TGQF2bpL3ttYun3D7AAAAAJgDY4dQSdJauzLJ6yfcFgAAAADm1FjD8apqV1X95iLbf62qdq28WQAAAADMk3EnJl/q5WtezrbGfAEAAADALBo3hFrKvZPcNOFjAgAAALDOLTknVFU9OcmT+1Y9s6p+ekDR/5Hkp5N8bkJtY4LaWjcAAAAA2NBGmZj8R5I8vfvvluQnup+FtiT5QpJnTaRlLIuwCQAAAJhFSw7Ha629sLW2X2ttv3SmHPrt3t8LPt/WWntca+286TebcZkrCgAAAFhLo/SE6nefJNf0/qiqA5I8NMk9k5zZWjtrgm1jGYRNAAAAwCxasidUVT26ql5XVd/ZWru4tba1u/4+Sb6c5IQk70ny1ap663Sby9QZzwcAAABMwShvx3t6kie31q5csP5tSX44nXmgXpPkrCRPq6qnTbSFTIRsCQAAAFhLo4RQD0ny0f4VVXX/JD+e5LOttUe11v4qnWF55yb5nYm3ktVjPB8AAAAwBaOEUN+V5BsL1j06nc41R/RWtNZuSfKuJA+cVONYWmstx551VXbtbjnloutzw9YdA8vJlgAAAIC1NMrE5AcmuWXBuod0l59ZsP7SJHdZaaMY3X9/7cr88TtPzd/9/P3zDx8/Z8/6ttzxd8btAQAAAFMwSk+oS5L80IJ1P57k6tbapQvW3zHJjZNoGKO55qbtSZJLr1+YEwIAAADMjlFCqBOS/E5V/XCSVNUvJblfkv8eUPaHk1w+ueaxlP326wy027Wg61MtGH83cgcn4/YAAACAKRglhHppOkPyTquqq5O8P8mOJK/qL1RV+yd5UpLPTbqRDLd/N21qyx5/t4DheAAAAMAULBlCtdYuTPKTST6e5Lp0ekA9urV25oKiP9Xd/uFJN5Lh9u9+g7t2S48AAACA2TXKxORprZ2S5IlLlDk2neF4rKL9uj2hdu3ee/2yO0YZjgcAAABMwSjD8Zhh+3fnhNq9ROrUny0td+hea21yw/4AAACADUUItc71QqilhuP1tp56yQ25z3M/ni9ecN3iBRfYfuuu3Oe5H8+rP/mNZbYUAAAA2MiEUOtc1eC34w3zufOu3Ws5qq3bdyVJ3nHSxWPtBwAAAJAIoda93tvxdi/oCVXLndtpyH4G4QEAAAArIYRa58Z9O555xwEAAIC1IIRa5257O97eIdSw0XlLRlW6PAEAAABTIIRa5/ZMTL5Kb63TkwoAAABYDiHUOrffiG/HG9mwOaFWKeQCAAAA5pMQap3bf8hwvGWTNa2Ks67YnD9/z1eya3fL3x/11Zx4/nVr3SQAAACYKiHUOtd7C964HZUWdnha9tv0WJZnvevUHHXaFbnoupvzHyddkt94y0lr3SQAAACYKiHUBrUwsxo1xCpp1UTocAYAAMBGI4RibzImAAAAYAqEUOvc2POFd8sPzZp00QEAAACmQAg1J9oK06OlRtnJpqZDxzMAAAA2CiEUSZbRo4qJcNkBAADYKIRQG023682lN9ySL198w9DtTFeT+gEAALDBCKHm1NDhed3VH/rK5fnlN31h6PZhZFQAAADAcgih1jn9adY3HaIAAADYKIRQc2JhmFHL7bOkq9Mqk0IBAACwMQih2NuwUXyyEgAAAGAFhFBzYqUZUY3YA2rUcixOpgcAAMBGI4Ra54a9ZW3oxORDjzOJ1jAu1x0AAICNQgi1QWzbuWukcrfubtm5a/eUW0PPRsqgdu7a7d4CAADYwIRQ69woIcZxZ1+Vd37xkpGO94y3n5IfPuzoAfVspLiEaXjIS47NIS88Zq2bAQAAwBo5YK0bwAqNkA199hvXLFmmf66nbTv1VlktG2k43o1bd651EwAAAFhDekLNixHDjGHFNlIYMgtcbwAAADYaIdQ6N2yYXGVar7HzerxJMswRAACAjUIINaeEGwAAAMAsEUKtc5Ma1lU6OK0Jw/IAAADYKIRQ69y4IcawrGnJ4whLAAAAgBUQQs2JUYff3bT91r3+/ty512bL9lvzmRHeoMfkGC4JAADARiOEWudWGmX83pEn51PnXD2RtjA+w/EAAADYKIRQc2qccKONUdjcUZOlRxQAAAAbxZqGUFX1fVV1eFWdVFXXVNVNVXVaVT2vqr51QPnvr6qjquqGqrq5qk6oqsesRdtnRS9AWnaPmhFDJVEJAAAAsBJr3RPq95L83yTnJzk8yV8n+XqSFyf5QlV9S69gVd03yReSPDzJK7pl75Tk6Kr66VVu98zohUMLeyiN02PJkLC149oDAACwURywxvW/P8lLW2ub+tb9S1Wdm+R5SX4/yRu661+a5K5JHtxaOy1JqurtSc5M8saqun8bZ1zZnFnumY+aVW3cKzsdricAAAAbzZr2hGqtnbIggOr5z+7yAUnSHZr3pCTH9wKo7v5bkhyR5PuSPGTKzZ1JkwgzxpmXyJRQAAAAwHKs9XC8Ye7VXV7VXT4wyYFJThxQ9qTuckOGUCtVlezevXS5aU2gfeWmbfn0172dDwAAAObdzIVQVbV/kucnuTXJu7qr79FdXj5gl966ew453jOr6pSqOuWaa66ZaFtnQ+v7v31rR8yMao37Nj3pDZ/L7/77yWvahrVkWB4AAAAbxcyFUElem+RhSZ7fWvt6d90du8vtA8pvW1BmL621N7fWDm2tHXrQQQdNtqUzoBdiLBYl1RKzlI+Sg0wrLLn6pkFf6cYxrR5mAAAAMGtmKoSqqhcleVaSN7fWXtq3aWt3eeCA3e6woMyGtJIoYwPP5w4AAACskpkJoarqsCR/n+Tfk/yfBZuv6C4HDbnrrRs0VG/uDYuPluj8NNIxxi3D+OR/AAAAbBQzEUJV1QuSvCDJ25P8Qdu3a85X0xmK9/ABuz+suzxlei2cX1UZK2EaJ9xiuN4dLoMCAABgo1jzEKqqnp/ksCTvSPK7rbV93tXWWtuS5KNJHl1Vh/Tte6ckf5Dk3CRfWpUGz5g9YcaC3G6cHjajzEtkyB4AAACwEgesZeVV9SdJXpjkkiTHJvnNBZNoX9Va+2T3389N8tgkx1TVa5JsTvKMdIbjPWFA76kNYfcKT7sy3SFhu3e3bNlxa77tDrfbs27TLTtzl2+53T5lt+64NQfst19uf8B+2XHr7ty6e3fuePs1vUUnYvO2nbnzgQcMnCB+I9y2W7bfmjscsOZ5N0MM+z0CAABM2lo/GT6ku7x3krel0xuq//O8XsHW2nlJHpnkpCTPSfKPSW5O8vjW2tGr2OaZ8pKPnb2i/atqqm/He8XRX88DDzsmm7ftTJKc/c3NOeSFx+QDX75sn7I/+Pyj86v/emKS5Elv+Fx+8Pnr/2u9evO2PPCwY/LPx58/cPvu+c+g8oAXHJ1n/+dpa90MBvj0OVfnkBcekxPPv26tmwIAAGwAaxpCtdae3lqrRT6PXlD+7Nbak1trd22t3bG19uOttWPXqPkz4crN21Z8jHECpsp4k0J95LTOfPGbb+mEUOdcuTlJ8tlzrxlY/rRLb+yWu2msembVNzd1vp+jz7xySInOxZ/3ubY+dsY317oJDHDSBZ3wqfe7AwAAmKa17gnFGlur7GPOM5eRbYDReAAAAJBECDU3VpJljDYx+QoqGOE4G2FupEE25lkDAACwEQmhNroab16icYeN9Q7dm5S7lzUtnKR7XjOopU5rz/WYektguHkfDgoAAMwGIRQjGaW31GIWPuMu/HtOM6glbdQeYAAAAGw8Qiim2g1p4aFFLh298Mn1AAAAYKMQQs2pL5x/XZ721i8tWa4yWhAySk71R//x5bz35EtHOFr26Qq1Ht/O9YXzrs0vvvHz2blr97KPsZL874zLbszP/dMJ2brj1r3Wb791V570hs/li903n61nHz7t8vzekSdP5djXbdmen371Z3LRtTdP5fgAAADsTQg1JwaFGZ/5xjXL3nc5/vtrV+ZvPnDGsvZ90X+dNZlGrKK/fv8ZOe3SG3PV5m1Dyyw11c5Khjm++GNn5+xvbs4Zl23aa/3F123NGZdtyt8f9bVlH3tSVjrc8NnvOS2fOufqCbVmbx/76jdz3tVb8m+fu3AqxwcAAGBvQqgNrqpGCgpWmlP1Jj42B9ICE7gcs3xJZ7ltPSud7wwAAIDRCKEY6xF83JdoDXvArzl6H9xKgpbdQ94WOIqle1mtvVlowzDzcwcu3yx/PwAAwPwRQm1wVaOFKCvtwdQLnTbaQ++w8217lvN9RdZDz7d10MSpE8gBAACrQQg1J5b7HD3qxOSTtrDjz0bNAaYZgMxCsDDT3+syep8BAACwfEIopjon1D6HnulUYjyTyDAmcTnmvTcVAAAA80EIxbJccM2WfHqMt5YtDGzGzW9Ovuj6nH7pjWPutbidu3bnP066OLt2ryzEWRi0tdby7i9dkpu33zrCvp2dl5NnrYeOPIa6dXz09Cty9SJvUVwrazFc8tot2/Ph0y5f9XoZz7adu/LOL16c3Sv8z0cAAOh3wFo3gPVh4bPqY171mSTJRS97wuL77fP38h5ofuVfThypvnG8+bMX5JVHfz37VeU3f+zeY+8/LAQ66YLr89wPfjWnXnxDfuth37PoMab5eDcLj47roZfWtFt48/Zb86fv/krud/c75ZN/8ZNTrm32/f7bTsnpl96YR9z3bjnozgeudXMY4p+OOzdvOv783PkOt8uTDrnHWjcHAIA5oSfUBldV2T1Gb4jlvMWtX6+qWejFs+mWnUmSm7btXNFxFgYt23buSpJcfdP24fvcNjP5ys1wzjPLPaFW6xbc1b0IV26avZ5Qa+HKTbckyYp7IDJd12/ZkSQj9egEAIBRCaHmxTKf9kd/EJ/sA2MtrHkN04pJ17zffp1zGyXcW0lPoX2uISzTLITCzCa3BgAAkySEYlXyn96DzLCq5qlPxP7dJ/pRenrs3t1ZTjIEmKWHxlnuCdWzHtoIa8XPAwCASRJCbXBVoz1kLPdBfdh+s9TzYrlNGdYTab/ur2qUEGoaD3iz9NA4y3NCrfY9OLtXAgAAYHUIoVjVniDz2Otk4Tn1ekL1D8cblndM4u1ks3xJ18f3vS4aOTfWxz1Bzwz97wUAAMwBIdQ61JtQezJqpN4qk3pu7NU1qV4oO3ftzvU379jz93VbtufWXbvHbFPH1TcNnzj6mpu273lVea/csHPoTUQ9rCfU5m07c0t38vLlXNdtO3dl0y07h9Y/Sw+Ng85v09adeyZvn7ZNW3dm+62dujbdsne9qz2n1nJr27275dot+05yv+PW3bmh795fjmkGQjdu3bHn2rP+zHIvRgAA1i8h1Dp0yAuP2WfdSh4XJv0getqlN/YffUipvR/Jl9uGv37f6fnRF30yu3e33LJjVx784mPzgo+cOdK+/S34wnnX5qEvOS6f+NqV+5S79PqtechLjs2bPnN+PnbGN/PQlxyXky647ra2Lyj/jLedkiTZNeScHnjYMXuCxD1vCxwjovjlN31h4D2wXhxy+DH5zbectGp1/fYRX+z8+4XH5Imv/9yq1DvIcn9mrz3u3Bz64mP3ebven737K3nQiz658oZNyY8c/sn83pEn77N+lobisjTfFwAAkySE2gAm8RAxTkh07lU37bv/Mo4zig+ffsWe49+8o/Mq8f8eECQN0t+UMy7flCQ59ZIb9il3+Y2dV8p/5hvX5OSLrk+SnHXF5qGx0c07Or0/do/0CvrxL8iZV2wee5+1Mmy44amX3Dhw/TScfNFt3+m5V29ZtXon5bizr0rS6Y3X7xNnjnafr6XPn3fdPusMxwMAgI1LCLUBLPbQVzWZeYn2PuZt8cx6mJh8pYZdv5EmJp/ApZ/lh/oZbtoes3z9+k3zN7PaQxMBAICNSQjFSA/hK50fpFfHpJ/3+9s+7mP0tB+7d49wYffkVMtozHoI8mY54FkP1y+Z7jWc4a+HNTbLv10AANYvIdScGPbAsFQvp7GDmxF2GOWYC8usPOSazDS6k8wlRukJNffWwSXwsL021ksIuNHpJQcAwCQJoRgpJxjnQX3Qw+W8vWmpau9hh4PsGuGiTeK6zPK1neW2rZdH66Wu4KSH00KyLvJjAADWISHUnFvq+bRqtGFj4+jPZvY58pQemKf5wLRUk4dtHuVU572zlHxk5degFzINyzwncY31SgIAAFaDEGrOLfV8OmyoxdtPvGjv44zTE2rAMRfuv/Cht3/7H77jlJHquaX7FrqF+6/0efrIz1+Y3zripCTJ5m078xtvOWlguaXquXX37iXrmrdeLJffeEsOffEnc9G1NyeZzd4UnzrnqjzmH4/Pzm4COEpvrUuv35pDX/zJXHr91mk3L8eceWUe+6rjc+uuve+fYb/VSVzjObsNAQCAGSWEmhOLPUgv1cth0APo8z985rKOtbDMwpBllGfdo8+8aoRSyfnXbBmp3Cj6m3nYR8/a82r5r1xy41j7jrJ+UmZxrpajvnJ5rt2yI+85+dIksxmyPe9DX8sF196ca2/aPvI+7/vyZbl2y4584NTLxq9wzEvwNx84I+dfc3Nu2nbraIefwWvMHJm9/5gBAGAdE0LNuVEeUEeaE2rCfVomHaCspH2LXaJFW7mMcG9YmZVcjVnKIHoB5CzPBbXQLF2/fgubNXQ43gTqMhyPhWb1dwEAwPomhGJVnjZ6NUyyqv1W+uQ8sQfvFQRgK9h36R5uq/8UuSdcbHstNrRxv+N93hy51JxkK7jI07pF9M6aH/JJAAAmSQg155acE6qm8Xa82x5bFu621CTL49h72N8yHqgHlV/lJ655e1avvTOoiZ3feg41JtX04T2hZu/arOOviwV8lQAATJIQiok/MA56Vl4YIkwi65nUEKJRjzNuda21JcOTSVz7YYeoNRhjtV8vhGqjT/q9VmZ9CNqo13AWA5/F3rg5g80FAABWiRBqTix3cuzKFOZ7WuThflhNy3mQXvFwvGXU3R8qLVX7KIfdPcGeYfvUv4bD8fZUPbGeUJM5zsBjT7kB4+45LDycxYnoh5l070rWzvq56wAAWA+EUBvAJ88a/ra5qhpvqN2CR5L3nnxp/uuMKwaW2blrd27cujPJvg+cVZUvXXj9nr/P+ubm0Ruxp569vfeUS7vHHv8Aw4K4YcfqX33ECRdm+6279inzzU3bptrr44Rzrx257K7dLUeccEG27bytna21HPn5C7N5286JtWmf4XgL1ve74sZb8v4vL+Ntc107d+3OESdckB237l7W/ssJdcbdZ+eu3XnLCRckGT8UHHVI46Dt7//yZbn8xlvGqm8Ul92wNR8c4Q2Bi/WE6l3BWe4lt5qOO/uqnHnFprVuxj58PwAATIMQas61tFx2w+IPoyvptfA3Hzgjz3rXV/Za1wscjvz8RYse51f/9cQRah6uv8fIVZu35ZVHf33Rtu7bmPHq2/dtZZ3633PypfmX4y8YuM/xX79mkk1Ytg+celle/LGz84ZPnbdn3RcvvD6HffSs/P2Hvjbx+nrfwWLfxW+85aT81ftOz9Ydty59vAHr3nHixXnxx87OWz9/4fIauQzjPpi/7QsX5U3Hnz/WPsNirlHnhNq2c1f+6n2n59ffvLLf1yBPfdOJ+Yv3np5du5c/RFC0sbfff9specLrPrfWzQAAgFUhhGLies/KN21fOlxYUT19D+U7d63to+2wIGX7zn17SPWbxJC5febbGhBWbO1+Fzf19Xq6pdu2G2+ZZE+o7nC87D2f0aD85JqbtnfKLPMS3LStc043r/A+m+awsC19bVvuHF1LNW9h+3u9kK69accIxx7v5K++adtI5Ua5pobjAQDAxiOEmnOTehgc52F1OXNCLUd/NXvN1bSM4Xi3rRq+88It/X/vt98yA4a2dL3LPebSBXt1T86eoVYj9IS6bVjW0gaFdYsFXKNYTiY07vfU3+yxh+PtuYaLn+e+b6Acq5rOsUe8GKPfWqMMx2M9WIsXHAAAML+EUHNiJb0KVnPujz0P1BN4rumfmHyJ0UFTt8wMaiYexNfqGbP3cLvY/EEjHmhFu49z/6/Gb2XPvFp73o639/p92jTk+o1zWcYPyBYvv9jvcRbueUbgiwIAYAqEUIz0sDHeM+q+T7/7TEw+gf43qxGejNrO/Zc71GoCD3oj9SQauG7yT5n7DQlQBlnYa2oxG/F5eN9zHnyPTfKNk0sZ9ZijhFpr8fZGxqcfFAAAkySEmnNLPedVjfeAP0rWslo9a/oDohX3plmhZQ/H6w0pW61r1lfRbUMBJ3/8297s1vZav3fhXqHl1bUWX/kkh00uVkvSd08v4+14Y9c45g24VJVr3TMRAACYTUKoDa4quXLT0pMNj/NMef3NO3L1Tdtyw823TYx86Q1b96l3ubbt3JVTL7lhr+BppQ/i4+5/zpU37fX3fkuc0NXdSbjHrffaLdtz6fVbF518e/MtO7Pplp3ZtLUzwfjI8/b0QqhF2n7z9ltz7ZbBbR/ktqFkneUtOzqTnw96m1qv1psHTOq+e3fLZX33TGvJ5m07c+PW2+6pPcPUktxw8469Jl1fzDe79/ti31hrLZdev3WREsml128dqzdP7zqPut/CIpffeMvg/Ras6v3WqpIbt+7I5hGuy5WbbsnOXbuXLNffti3bb811w+6NvjZtv3VXrtq873/GrHaIuHXHePcy688VN453HwMAsPqEUHNulCFXHzn9ionW+dwPfjUPfclxecdJF+9Z91tHfDEfOf2KiTx4PvZVn8lT/vkLeeOnz9uzbrXmterV8p6TL825V2/Zs37/IT2heuW/uWnbkMm1F3foi4/No17x6TzuNZ8dWubZ7zkth7zwmBxy+DFLHG2wxcKYJ7zuhBz64mPHPlbv+/jbD5wxvGw3lPmZV39mn22v/9R5+fGXf3qvdYe88Jj8yOGfHHCc5EEv+mQe/KLR27mXAV/Ce06+NI96xafz5YtvGLjLly++Po96xafznydfOvKhW2s59ZIb8qhXfDrv/tLw/XpB3sLefU9765fyri9dMqCO28p96pyr8vjXnrDn7x85/JM55IVL3xdvOeHC/L+jvrZkuX6PfuWn8+Ah90Z/25/1rq/kx/7huLGOPQ2/8LrPjXUvb3TrrTPbpq0784iXfSqHfeTMtW4KAACLEELNieU+MIw6vGgS87eccemNK36jWdLpEZIkJ15w3Z5105qYfdQeW0v1hEqGtHHEhvfOeRQDO8ss3YFmoIuuW7w30D56w/G6Bz/1khuXKpqbu72l+n3+/Gv3+rul7XsOC1bsGLMHxJ5eWwO29cKn86/ZMmBrcl43gDz1ksEh1TAXXHNzkuSUi69fsuyg0XiDQrH+y3DWFZsX3b7YtuPOuXrJNu3ZLy3XbtmxyPbbfPKsq0Y+7jRdcO3Na92EdWm9vBzvpu2dHn/Hf/2aNW4JAACLEULNuUkPeZml55H+c9v7PEdr5aAAbrkPXPsP+SUtdbhp9DYYtVfYJN9U2NM71ChzAi1a7RgXZhrzNC31u1nu76rXYW73MidNGnSuk7qHJnkVB83RtjDINi85k+R+AgBYH4RQG9yoAcQk/vv9NP8X9eUMx5vkEL6RekINWreGD063VT25L+a2y7D0iY0zGfZye3It16g99sYNwHrDNhfLoG4L8rpvGOw7+UGXbO/ty/8ux9l1qft2cAi14O91N+ALAABYKSHUnBg2XG5WH/MmHUit9du4RhuOt28jew/rk7wcowZb0wjAeqHMKMee1DkPu/STGEK6klCn24iBxxvlbY6DigxqzbAjjdvyUe7hkS0SGuqxAgAAG5cQas5N4kG8c5yJHGZqD6DLOc9JDuMaVntbosw0H8iXqnsaxskxJpzvjL39tnLjd7Na7vXsDcdbrG0LJyZvA7bt1ZYJfbmTHY637zrD8daXSf3/DgAA6CeEYtX09ypZcQ+T7P2QNK2eUKO2cpQ5flZrSNngicmHz/80jTmhRnt+XVnFSw2Zm8gQ0iWONe6122+cnlC9ZV/RwXNCLT5cb1QrHR651/YBV6y3ZrEJ4Zk962Vi8p711l4AgI1GCDUn/j971x0fR3H9v3un4t67sS3cK9jGxgUMxgVsINQAgeAACSQkQCAhv8QQaugETDUt9E7oBIPBRe69V8mqlmzLKlbv0t3+/ribvdndmd3Zvb3TnbzffIh1u1Pevpl5M/PmzXtpx6uYz8c9ZBye/bBw9LPAlpEXLW3mv1MtlCKO1LQipCxcYrnkF1dk4PJX1jNT1jQ0Y9xDP2FVuj4a2Kursrg1bM4pxfGKeua7x344iDpGlDczOG1tUNPQjAtfXAsgpEDJL63Fo0sOBp9JKKluwPD7fsTO/ECktWUHCrE+s4RVHBPvbcjFrGdXMd+FFAzm31VS3aB79uKKDFy2WN9uWjY9/sNBLE7NUtUJAOc8nYqPNh9GysIlGHLvD4b1a8vcmluKkff/iLIaftQ3LT7dmo/d+eV4f2MuZj2zCvd+vRd//HA7AOCa1zfixZWZStqaRh8amwMR/HwcpeU3O4+isLIhSB+xhDJRMsnAVa9twKJlh4Rorm5oxtgHf8LqQ85FEUtNK8K4h35CbWMzALZS+JevbcSiZYeYCqzXV2fhwhfW6p5f/sp6vLA8wzE6gcAYv/UWytkAACAASURBVPglfV2fb8vH1MdXhD0mn/kpHVe/vjGsMmIBIlx4fXUW5jPaLZZw9lMr8fHmvJYmw4UgZFnG1MdX4L9b81uaFBcuXLhw4cJFhJDQ0gS4aB3gKae0IPs70cPqp5amWSqXwGhDnlVcjar6Zq7izghG4b8Pl9ZgZJ9OfBoNtnVOWIYBQA4Vhp7U9r89x1RpNmSdQGOzH2+tzVGevbAiA2cN7SFUx4Pf7ee+s+ITigXSbmemdDNM98aabObzvNJa/PPrfUJ1aUl8JTUT9U1+7Mgrs6QsfXt9Dr7dFeBxNsX/zTmlurRFVQElJs9w7r5vQrQLXycEsDW3DFtzy/D3eSNM02cUVqG6oRmLfk7HhIFdleeWHJNrOPTU0jRU1Tcjt6QWo/t1YipydueXY3d+OXp0SA6UQaV54kf2ON+ZV46deeW4c84wceJMwJMp//hyD/xyoG28YQzHl1MzzRO1EvDaLZZwpKwO9369F9dNGdjSpLgQgCwDxyvr8fcv9+DqyQNamhwXLly4cOHCRQTgWkK5EILj7kEc0Lm0hM+jRp+f+87MxxT7mly4FJlDe1XQ6Gpe2FD8GYVXjFbJEclIamxfXWweKZZeNslJTvCqyteCro55Hc/EJ5SInzNyJVBLgaN+yQV8XsXqdTzXF1J8wm221gFfsCE97rVKFy5cuHDhotXCVUK5iBokRE6ZQCtazDbTRj6Too1ohKlv1miElIh8tI8uh+oK+VCKHj/DtSLTNj1dnLbocLtJUkJA5Ir4hGKnEfcJxeOL1vF5qOTwfUKJKOliXVkQ4+RFHU4GcIgG6G7vKhTjD6z5yYULFy5cuHDRuuAqoVwIwXlDKGcXmFbo06bl7VNkWXY0ZFg0LKE8jIW73hLK2TppSBEyc4nmXjKSdXmD/PGZKHFoOqxYQqmf86yt2FcmrV3HM4aQ4/UY1Q/EKl3RRmvgQ2v4hpMNVq/su3DhwoULFy7iD64SykVU4eSmgN7oRmKzYbVMOwe3focX3CwamjVWYhFVQgX/DbeKaGwe9UqYEPOciobHy8+LpkjTwFLkcPyS26JBltWKKic3ffG894+mFZ8L52AUkdFF/IAEbXANoVy4cOHChYvWC1cJ5UIIjigFDK452QFNUzjXLqK12GVvkpzdJrG+xafhjXLdwSRfOPU7fQ3GqDS7tIvwXmuxp1gn2Ww3omTiWQqxLKF470Pp2Ioks+ss2uJZVnTcvCbtK2IJFasqAtd6Jr5Bj1mxfugilhCan1wtlAsXLly4cNFa4SqhXAjhvY25YZexZE+B8vcX24+EXR69vbjp3a2GaSvqmvDKqkz4/TI2Z59QvXt6aToaGfejrG5fvt99TPdMNrHWyi8ViyoYDnyabztYEIgKSOscNmWXoqS6AQBwtLwO72/MNS037XglZFnGm2uzlahvpMxvdh3DFkZ0OFFYUSw9vTTdNE1qWhH33Xe7j+HppWnYmBXoF4cKq5VodyJgkbo1l/3tr6wKRE4jG63Momp8vo0fivxoeR2OltdRdTF8QlHNy4tWdqiwCl8Gxxzh7cGCSuw7VhlKZMDzr3cewd3/3c18t+xAoRJp8nAwSqbI3r+osgFvr8tRPfty+xFkFOqjVhZV1psX6BBkGTimGQMl1Q34z5psyLKMtRnFWJdRosv3w94C7M4vV35/vDkPeVTUUFmWseCtzfiOISeMkFFYpcjLzdknsDKtENsPl+L6NzejoKLOJLce767PsZQvXAX1extycaycXR+Ry1nF1fh0S154FTGg7YeZRVWG480qjL7NhT34SeyPONJBVdQ24dVVWY4cvmzLLcXyA4UOUBV7+H7PMew9UtHSZLhw4cKFixiAq4RyYYqiqnqVAskujpTVKYu04qqGsMujF3wNzfyodQDw8Hf78fTSdKzOKGZu1N/bkGtYvgheXGkcmp1V2idbnNsQAWxrFm29b6/P0aUBgP/7PKBkuOHtLXjg2/04UW3cRvOeX4uMomo8uuQgbv94JwC1kuTq1zdaoNwY4S7uzZSUr6zKQl2TDwDw1NJQ/+BFxzPDVa+xvz27uAZA6BrmBc+vwf99sUd579Fcx3tjdZZpXSKsOf+5Nbg72L50HdsPlyl/G33aXz7bjS93hBTHdJW3vL9N+fvWD7cHaTIn6tYPt+Nf3x9AVnG18uzuz3dj7nNrdGlJ/4oGZMi48Z3AGCBy6i+f7cJjPxzE/mOVWPDWFlz/1mZdvj99tAOXLl6v/L7367345WsblN8bs05gbUYJ/vyJtW+Z+9wa/C3Ydte8sQm/fXcbrnx1I9ZlluDm97aZ5FbjeEU9HvrfAdz0jvF4AJyxUyuqqseD3+3Hje9sYb5/9PsDeHppOmY/uxoLv9rrQI1qaC2h5j6nHm/hoKgy8G0ivHQhDtJm8RQd775v9+GppWlYl6lXTlvFL1/biJvftzau4wW3f7wTv3h5XUuT4cKFCxcuYgCuEsqFKXwc/zUtDSt6iaqGZgBAI0dZVR9UQKjKh7NXAgw35k5dh2M84y3mtd9WWR/gUXltI4CQosQITb4AP6uCeZ261tcSPqGilTeQP1CAdmxJqjQhvhKw2tLq1UBeG1mJRmX2/SIUkf7G849Fg4zfaECWgbLapuDfAdoq6wK/tZEmzVAezAcADT5jRbkdVNY3mSeiQK7mVtRZy2cGnmwj/ZtXXx1D7tqnwZk0omgKfpvVNnBhjHi8jlcd7ANNERjjLly4cOHCRWtEiyqhJEm6R5KkzyVJypYkSZYkKdck/RRJkpZLklQlSVKlJElLJUkaHyVyT1o4uXB3tCzOc6Olq1loeZG0sQzWd9CWL8a8id0PjgRlLfm1PMUu3X5Mx+QsSzcLH+Lzy1HZ3DntiyfRG70NaeyOgvChBA5w+CPNDip4fS7Jq16COCGDzMaQU5AVi534UZbEA3xxaAnlwoULFy5cuLCGhBau/3EApQB2AOhilFCSpKkAVgE4CuCB4OPbAayVJGm6LMvO2/K7iGlY2bCYxz1jlO/AdlTlPD3s0kSgX7l7OKt57WNHfM+7GzIh8Pfs6p7qtHvvxmY/3xLKQjlmlTq990+I4o7UScVFpKm2qlAkChOnAyLYNZZN1Cih/DLgpL4xknp1UrYr8pxFiK8uY124cOHChYvWipZWQg2RZTkbACRJ2gegg0HaFwE0AjhHluWjwTz/BXAQwLMAzo8wrSctnFzHWy3LaBNhpSw6LD3zvYnDZycQhdt4HEsoXlp25Lew6g+/CAD6tm0pI61IbYS0ClS/X4bHI2mi48lCilZupD3Gs8ZmP7eNnPxUpy1QEjzRM9plkW73ayK9j7ZaPknv9A1rXnubdYPEBK0MClyCtgNWVZEUG37XEioiYEVvdeHChQsXLly0LrTodTyigDKDJElDAUwG8DlRQAXzHwXwOYA5kiT1iQyVLpy8pmW1KKMTexFfMgREycS9wsdV1AhXwYTM/REZsMj1Cn6ETvFjg2Cn9mOx4oeM9zk0dXZI1eZhbbxYY4XFXyu8avD5DK6xWvAJZdI3nBAZtNxJiOJ1PKNPs0pFrPm1sXIdz4rcN1M68uSCVrnoxLCnq4rkdTxCq6uDchaKPHP56sKFCxcuXLRaxItj8snBf1khpzYhsFw5I3rknBzYmVeGFQcLUVnnnFPg3BM1jpVVWc+mq4iKvFfd0Ixj5XWmGwXWa1m2vpmmI30BQGlNo1C+2kYfjgZDfdc1+pBfWot9R9WhjFmh67Wgo/PklwbCw+eXhUKI0/yv1jh7Lq9txIasUH4/5WOV8DGzSE2Dlj92rQIam/0qp646vlH1VDnlCDiMDWpWUaid7UR61CqOapsCbU6zr7K+CVtzy1TpjpXXoaahGZlU/c0+8+8gvnf2H6sU8kdlBlPH5Ibv1S/TjrP7NV1Gbomx3MgsqkZmUZUlhRxPyVJQWafqYxmFVWzrKFlWxkMOh766Jh/KGDKAzktg9o0i8PllnQxSQWljGU0+v0J3fmkt6hp98PtlVd8CgJLqAP1E/lTWN+FIWa2K/oxCdp2EbQUV9cozIksAIClBq4SSkVNSo5IFeSdqdYEjMouqhQ4heP2Q1fYZhVUqnvDqK6gI9I+G5gBNdmXe4RM1ShlGkGVZSPaHi+ziajQbONYurmpg9mUjBMaOcTsdr6hXOXeXGTqoQL8U40FxVQNKaxpxpKwWtY321y7Hyut0c6QoymoaHYkArAUZpzzkltRwg68YwW6+WAJLdoUDmifZxdVIO17JTcuS57GK+iYf8k7UtjQZUUFRVb0SaMeFdTQ0+xxZl5xMEF2buAggXpRQ/YL/HmW8I8/6szJKkvR7SZK2SZK0rbi4OCLEtVZc/soG/O69bbjzU+fCo3+9k9WEfGjXr4YbLAaufGUDpj+5klseAe/alVVroNnPrlb9pr9Xhoyl+wqY+Zr9Ms4K0nnz+1sx4+lUXPySOpTx3OfW4Ps9xwzrf+Db/crfTT4ZpTWNKhqWHyzi5j18ohbX/WezsumkrQguW7we059ciTmL1jDzSpp/reIvn+1S0Z5Xql4kvbshV/l73EM/26xFDSstS/ebDZklKno2Zp+wXLfWQuP372/DjKdTVZYzv313m6KYJPhpfyHGPPgT5iwK9TMRxQvp3je9sxUvrczgpHEwEqQBdxs0m507PtHLl31HK1Q8OlZRj5VphczyDhZUYs6i1ZizaA3325g0ckic9/xa1DcFaFx1qBhzn1uDvUGFMM2i9zbkYs6iNfhhbwHOe2YVt54JjyzTPftkSz7mLFqjUvrOfGYVKmqtKVi1Lfbsz+mY/exq/sIx+M2yDDz03X6c98wqFFc1YMbTqfj9B9vw6uoszFm0GvuPhRTgj3x/AN/vOYa5z63BT/uPY+6i1Tj7qVSVLLh08Xqkc5SJBP/bHZBdl768TpHJWl9fxVUNOO+ZVXj4fwFZ0NDswzn/TsVdn+5S0hw4FmjvV1dnqT+N0aA8BYj28dJ9BZj73BpMe2IFzntmFYqqQkqz9ONVmLNoNV5amQkAmPbESlz44lr8/Ys9AGBL0VHb2Ixz/70Kf/t8j2nar3Ycxdzn1iA1nS+7w8XhEzWY9exqPPPzIW6ayY8tZ/ZlHjZklmDuc2vw2dZ8w3RTn1ihmjcVq1BqsL25LhtzFq3B3iMVuvwsOic+sgxnP5WKX7+5WZheLaY/uRJXvrLBVt4JjyzD5MeW266bhxlPp+KW97cx352obsDMZ1bhwe/2M9/zUFrTGMy3zwkSWwyvrQnILu3hnR2UBXly/zf70Njsx6xnV2Pe82txnFKm0/h82xHMWbQG6zJKmO9jCXd/vhvn/DuVGRG6teHMx1ZgogWZ5UKNe7/ah5nPrHIjwAqCrBVeTs1saVLiBvGihGoX/Jd1tFSvSaOCLMtvyLI8SZblST179owIca0dGQ6eLlmFdrN+tKyOk5KN9OAJsuITyqpjcgcV2rIMHCgwPy1bn8lXapht9LQoC+MUiOa96AmjXT3G8oOFWJ/JX8DtyCvjvosktN8jAchy4GRIuwnelF0aeG6jwzX7zU+wTx8QivuwLZfNSytNZ0alkV5Mq4Ri4Vh5na4MnrXN8crQxmD7YfF+IsLpLIN+vye4Id6VXy5UH81fotTSWt1UNVhUQmk66OacQD8qqWZbYcjUvxuyAnKGWH2tzSjBzuA408pZInfSCqpQWMku+1iFsWwmlgRZxaFv1jomJ/KK0Eas/NZkhA6QiGJ2h6atybfRPOEpGrXzCrHGIwr4yrpQOxQEv2s7JYPyS+uU9jeyTOGhIajkXJthfjB2sCDAt0xO/3cCxHp4W26pY2VmB/v2bgHFEW01RJTqtH5yx+HAGDtabs2CY2ee2NjkIT0KFmhWsY4zT1YE++wmi4cipK+TMRevIG2tPbixg6qglf36rBLV/Fpex15PHQiO0VjsL1qsSQ/InEYDq8fWBNcoxT7Imryac+vEhRpkDWRlHXqyI16UUGTlkcx410aTxkUrgnYT0WRz4lR8QnEdk5vXHS7kSBRqArv8AtTX8cwQ7leZKzViYyXhVBvyvsdOc3F5Y9NptRMwuoIj2idF2zyR8itkpWlEytemYPl4cs6NdvhdK2RFwqlVsYQKVaRepId859GPyZUznwGBViyRCBI1vr6U5LLqH2aZ2m+04lBe5/9ONv5thEhvckLO5CPo38pv3G/swBvUIlm9mhDytUVHClVUjE6Q5jiUbhvBvmA2luxWHRszq3Nwsg1ExwORY+Gst1y4iDUQGR4rPlpdtD7EixKK3EFiXbkjz6zd83IRF9AuvJsE/N8woVhCGb+nod2IhQtZlqN+KiPiL4iHqCp+ZBMn9DGihHIK/GhiNiyheG1ssShrPqGMCzfq56KfKJrO62FtVs3hWJ9qwaiC2qpZG3gahD90+9BtyYsiSpRQRu3OUlqbRsfTWELJ2n9Zii3lL+036tPy+OmkPIm0bCK8j+TcEVLxONeZPTaVZ0ZBGk5mJ/CtbAqMadDziAjfiRwz8qnmwkW8wSyquAsX4SJelFBbg/9OY7ybisAaanv0yHERLWgX3vYtoay/l2U5AtZQ0ZXm4ZhcG1k98GCXXzKMeW3FKku4TgFatZsyCXBkJ8TbUNrZ0Fo9peJGgrQUHS/8FGYQtoSirGmssM85HZQY31iKIW3esBUNMrnKxFFCmVhCqXJRz4muyKivsa2WDGiF3jE5oUv5l0EXTyHBdh7PodWELivNEI6iXwSk30Ry7oiEkkfEeo4FljVfrO+BFJ+IEVSSicpDuyTEu37PSfpDzvElob5H5FhjhGWBk3AVCy7MoFhCuZ3FRYQQF0ooWZYzAWwDcJUkScRJOYJ/XwVgpSzLx1uKPheRg3bhHa65M+8kn7VBlBn1h1U3zDeZdixhjEB8j9iBFVqU0O+2azNGJKwNRNpWt9F1qG7+dTwbllC8PKqdu3k5Tm6gnLDaEF34JHjtTWNiSkjNbwaP7PBN1FeRKTR1E757ODT5KeUOGd90naFvUdPhEViMsmg3+x6tY3Jt7aFrYvrOzLODUiuseBaH7LxG4JXVFAkNOQXCoohe9SI8dVAGJHhtXsdT2KlXLtuNRNgaYL52CP5hkUWh663xzVue7Aq3TBGZTCyh4uo6nqtXcGEC9zqei0gjoSUrlyRpAYBBwZ89ASRJknRf8PdhWZY/oJLfCSAVwFpJkl4KPrsDAUXa3dGg10X0oV3f27aEMllgsV7LsrMLf1k2X9A4LezDsYSK5rxjxuuT5TqeHZ471WcsOSYX3RCFAVmw67KsZITKD3cV7sCeTW/NY9GqTfOb9CszSyj601VKKLCvfnklc4WC8dU5Th7Ob1KUcr1QVU/wWRg+oazIE7O5w05ft5LFI8D7sEFZfjgFQjdXSc5BqA/TT9mKx5MJ5vLKHo9s6q5iDmZ+P62ALkKkvCSihBIIuhEriLZVvov4A5n3XSWUi0ihpS2hfgfgkeB/vQB0oX7/jk4oy/IGADMB5AJ4NJgmE8A5sizvjhrFLloUjTYneRIe/IXlnPD0jGfbDzsXKQiAULS9of/80fB9ASdEMA83vL3FUnoaF76w1jTNxS+tU/22uwBs9suGUW22ciK6hYPFqVmmaeqbfEhZuEQJuSoBePKHg2HXnV9ah2lPrNA9r26wHoVERDG7hYp6xYtuRu/qf/P2FqQsXILbPtrBTPq4AQ/eXpfjiNJQpIzVh4px6eL1yu+N2SeQmhYKZf/l9iNIWbgEZz+1klF+4N/bPt6BG99hj5PX12Rz6/5qR8ANocjmLWXhEmzNCbXBp8Gw9fd8tVeVbs6iNfhu9zGkLFyClIVLLEcrI9908Uvr8NrqLEx6dDlmPL0Sz/6crkpHW2Ve9GJoDJNQzLd9vANL9hYoz5/4MU1VPgu3frgDKQuXoJCKVngv9X2LU7OQq4kGyHMInldai0sXr2f2gdCmWc35/24L8LTZL+PKVzfgnq/24Nr/bAqk1TTSmAd/UqLesQhhdb3K+makLFyie54cvIpz3X824a//3aV7P+PplVik4T/9Xde+sQl//UyfjyDkmJybBBuySpCycIkSFXHWs6vw1NI0fgYNSNkbOZHVeGVV1DUhZeESLD9QqDxbdqAQkx5dhsMnArFitLw868mVzHn4211Hceo9S1AXDB2vuo4XLOPm97dhZVqhLq9TmPjIMryxxnxeaAnQfLzl/W245f1thunzTtQiZeES7DtqHp3QSaSmFSFl4RJU1FoP6742oxgpC5egtEY8qu8Nb2/B7R+z5ykA+GL7EQy99wdLa0fFOoz62wjkSjjv0G/RskPMOYjg8235GHrvD1G1pBLRK+zMK1PGd8rCJTr5bYYDxyqRsnCJLgqsET7cdBgj7vuRqXS//s3NuPPTnYb5DxVWIWXhEmQWhSIVLnjLPJ8L4N31OcraI7+0NnSlOspKKL9fxsj7f8QHmw5HtV4eahoCc/+P1JrIKazPDMzdJzgRjVs7WlQJJcvyTFmWJc5/MxnpN8qyPFuW5Q6yLHeUZfkCWZb5s4+LuIdW+IUrCrM5kyHrtPu9DYedPSsy10GZQhs6PZKweoLdGnG8Uq/0q7ERkp0FqwpFHpyyEqNHwJpDgTDOSziT7tc7+XEg/vX9AUdOo0W+6/XV+g3jK6sylb+f+DGgLDvCGDek/CV7CrAqGLbaDkRvsby3MVco3T+/Dilu3tsYWoSxNkNauUWnefLHNJRUNyC/tA4vrcwMvg+888syc1OWdrxKlU4LkcXoRirUu1apoe1P+jYO/d6dX87ciPAsod5alwMAqG1sxvbDZfhkSz6yi/mbH6uh7DM04devnzoQAHDtmYF/N2SdUBSTNPJL6/DiykzVM/q7N2afwFcG40nEJ9RbawPfvisYpj67uAavrhJXpqgdMevr4ZVFeEKPuS+256OkuhEZRdUA9H3maHkdnlt+SFfW00vTIctAUVBJTisZ6RJe0vDSSZTWNOLxH8SVd9EE3WeWHSjEsgNqZZy22ZYfDLz/YvsRw3KdNjImfSFdM15E8EZQ6b/XguJs9aFifL8nJFe0n/P4DwfR7JdRVW9dKSZJkpCyhlxX5vHyxRUZzDlIT6P1Qyi7EJlfP9qcBwD4S1BJTvqUKL7aEeh7yw+I53vou/1oaPYzr36vyyzBt7uOMXKFQA6cf9gb8tCyNsM8nwvgof8dUP5elV4UuoYf5b1Ak9+P+iY//vW//VGtl4e80sCBygsr2EYM4YDIvD1RPiyIFbS0JZQLF4ZoyWtYPr/suI+mcK9UxLoJdazTZxXeOPCTYTtipAY8P0J24MS4sTtU6AWTERmxesOTjvbno+4ji9Br7nBbVtKxxqppGHgBIoyU11r5x7OEUtIb3BUKd2havXrG+3QnfYKx4BGwhAoXND1WaCM00X2W/E0eWXVqS9LTfKXXASfr1RBTn1DBf637dmLceY1DRCqSF933uDKAvLe5/lEUzVGYlEgNImtrxR9d5MjhorW5YIg7SJISkCTSfg+1IDI+VnwAul0xcnCVUC5iGtEa+yxR1+z3O1q/jPC/5yRdf7cYYmQONIRTJvy8zYudhbET3dTugpzWyRmVEI0FPw3RrkQrPmkFI4tank8oHpToeJCZssRMvogoFIzClPtUGzr9lk37W4mUpkpjvd1YvDeKvkbXwWs38pm2fEJZUkJFfoMqc/42A9ks0LJDK0esHrzIjDan4ZTSPd4Qqeh4rWWD5eRcrRoPIvwhftts8jLSgV2YEKgs5CMw+p2ktfTLeIWElvMJRWR8zCihoJ/nXDgDVwnlIqahXXhHamJiyRanQ2874eg82htnUUTqFLKlEQ/Bbnj91Knp0k6bOuMTymY+wYx2SGRGxxPktGh1tFWJkUKHRY8Z35XoeDJblpgHTjB8DQBoErSEYlmaaqtnLX6V63gtbLpBrNTsyGRLjtFt5LEKumwr9ZC0tOLUqyjNAr+tWkIpikeqTLoIszFhF3Q7xuI8a0aS3f7Bcv4fz3DCGlumeEL3Bd4eVFEi2VVCKdaOke93IXkikLYF13WuJVTLQpJC10yjHfUxZAkV1Wq5kKMhI0/S7u4qoVzENKI3D+nFS5NfdlQwBKwPwivwZL2K0FLwRdkM2Q7CiYBIgzfB2ulx0fIJZZbPaDNpp3yW4sPpwzGVEspv/C1acWB+HS/0L1OWmOV30BLKz1DKa9sk5P+J4R+Iw3cWiawTTKNTTZGuQdrGjki20vfIRiCiop8q244SykOtJD2aDbXPpA9rQboPz1ItUr4K6WJjcf9rxjs/g28iYCn94hFmSmmL9nikUDGfUGFbK5IImDaz24DIOBfxRxcpuGvdloUESTlQcPpA3gxkDREzllDKOqRl6WiNcJVQLmIaVk9R7YIlXHx+v6OTrzMb8/DLiCRinDzLiAfn7E5ZBvAm2EhbevBgdxEqmi/aLSu6fqEXXvQJJItendLGpGzlOp7Mvo5nll+Et0YLVrqr+mW9Ul5rxcaS/2bXtUT7XrinrH5FCWW9J1nJEg1rBBG/N+x8gX/pPquNqKS2shKnRR0dL5QxUqfydq3BogUz3tmlOVqb/WhZl/GqsXfoIOgTKkxZYtd/WjgQU0KRtBEmhoE4WHq1angoS6jmKB/GEmtqT4yYQpF9YESVYrHxqVFHQksT4CL+sDg1ctFptNBuSg4WVEat7mPl9fjtu8ZhkK3gwe/2h72R2JVf7gwxDmP/sUp8u+so7vyUH2o8HqFdoB8ORsmIJTzPidhRaTHSztbcMvz23a1I04wxW5ZQNvLQ+P0H23XP1mWW4IkfA5Gr5o3pg8QEDzZk6SOcpR2vwj1f7cVVk05BmUGYcL8s4xsqKpnIovc1RjQ+0Whdou1xtDwURWlTdqnytzYaFhCIgvbl9iPo16UtiqrqDUNhf7n9iLLJafLJqKjT86a6wZhGkVDbb6/PwaxRvZgRB+mNz8j7lyLJqz4He/qntScWOQAAIABJREFUdNVvEunI75fxzE/paJfsRU2Qxu/3FKCxeRuG9Oqgug7GanOfX8ZZT6pDpO89WoG2iV48tuQg5o7urXr32uospHRvj+LqBmzJCbRBXZM6KuY3wWhL2n7z/sZcdG6biG25ZVi6PxShSZZlLFp2iNtfft5/HEkJHvTu1AbbDpdhwdRBqG5oxqKfDyn5j5TV4sZ3tuLG6Sm4fuog7DlSrpoT3990GJtzQmPim51HcayiDl5Jwh/OHQIgEHXw5eAc/rfzR2D1oSJVRLjfvL0FHikwji4+rS+mDe6hovP55Yfw51nDsPpQMW56dyuAQPSpyY8tR/f2SUqEReLXY33mCbyzPgdnntoNTy9Vt29WcbWOD/d9vQ9AYMG/72gF9h6tUM2bPr+M2sZmLE7NxLTBPXCipgFej4Tu7ZMxbUh3Jm8zi6qwLqMEN551KmRZxs3vbcPEQV2RnODB7FG9kX68CmnHQ3z8eEue8ndRZT16dWqjKq++yYfbP96JyvomvHb9Gap3JLIoADz47T4cq6jHg78YDZ9fxptrczC2fyd8ujUf7ZK8mDSoG+6cPQyZxdXYlH0CV048RVVWbkkNHvvhIIb07ICbZ5yq+65V6UVobPbj/DF9KP9pEt5Yk4Vj5YHoq1/uOILpQ7ojOdGL0/p3xgsrMhSFUNukBHRqmxDMF8CSPQXo2j4R04cE2n35gUJUNzQju7gaf549DH/4YDvaJHlx/ZRB+HFfASaldMMlp/fD7R/vwKk92uto/HRLHsb274zRfTspz/JLa/HD3gKlTwJAY7MfazNKlDw9OiQhOcGDdRkluGF6Cl5dnYVLx/dH/y5tQ21TRUWYDX7AHZ/sRHKCB+2TE1BZ14TSmkYAQEOTH//+KQ23nTcUazNK0C7JixnDesLvl/HCigxMH9Id2/PKUNvgw4Xj+gIAsoprcPN7oTVgs0/Gsz+n4+YZg9G5bSIAoKq+Cc8qYzRETt6JWvxnbTbG9e+sPCuracQ7G3Jx1+xhqGlsxuurszGwWzsUVQUiQvoELE6q6puwaNkh7Mgrx03TU3DZhP7Ku515ZcgsqsZVkwYoz95cm40+ndugttGH5QcKURWUnze/tw1/nDkE88f2xQsrDmForw7YmVeOWSN7If14lTLXkrYBAtFNu7RLwmdb8/D+b6egbZJXRZvfL+PFlRm4fuogVNQ14Z0Nucq7hmYf5j2/Fn84ZzBKqhvQsU0ipg/pjmG9O+KTLXlI6d4ePx84rhz8/WdNNv52wQjkltTgrXU5+NsFI1R17T9WgRve3oqf7pqB7h2STfnGQnVDM15dlYm75gxHomYuemd9DmYM64mhvTqo2s3jkZBVXI1rXt+I/91xNvp2bqvwec6o3kgJjoHlBwqxJqMYM0f0xKyRofllZVohHvn+IJbeNQPJCSH+fbvrKF5YkYHXrj8Dw3t3BABU1jfhN29twS0zBuOi0/qq6Gts9uOFFYfQLikBF4zpjaG9OirvPtuah9F9O6Ooqh6PLTmIHzV1Nfn8eGF5Bv5w7mB8u+sYzhjUFbs1ewtJgi1LqMMnavDT/uP4/TlDUFBRh692HMWfZg7RWVquPlSMhiYfhvbqgNT0Yvx6ykDMfW41Hr98nBJN0atRQi1adgjf7z6Gn/9yDhK8HtQ1+jD3udV4+srTMH2oep7S4r9b8zGyb0ecdkoXHD5Rg5/3F6Kh2Yf0wmo8feVpaJvkRXFVAz7afBh3zh6m0LvmULESSXfv0Qo0NPtUvCTYf6wCn2zO0z3XgrTbn2YORU5JDVYH54vV6cVo9sm6tUhrh6uEcmEZ/9ZsFCIJrej7dGt+1OomixenwNpEtia0NgUUoLeEMgt33RIoN1C0WMXKtCLdM6uKU0mKzKk32aAAUG3uWfhkSx4+2WK8IJBl4K7PrPXZ73a3XJjnP320g/n87s93C+UXTWeEbYfLTNMUVNTj6tc24gRDfmqVutqrpFol+6Jlgc1dVUOzojih8fOBQkBQrtLKPQB4fXU2Xl8dCI/8kWbxaC2ct/qbHviWHVY6u6TGUGGpVbwumDoIzy07RF37k3HjO1uRWVSN+77Zh+unDsIlL68HAMwe2QtAYMFMK0Ho/n35hP7o1akNLl28XnnGmpOI0g0APtyUhw83qXnz/PIMTBvcXVFAERRXNaA4uJkG1BZLD1Ohv2n88tUNumekT0gALn5pHQDgrKEh5VKzX8YrqVlYHPyPRu6TFzHrufTl9ahp9OGG6SkoqmrAirQirAjKurfW5aCgol6Vnm7Dv/x3Fz66earqfdrxKiVc/U8aWfSbt7cof7+38TAA4EhZHc4Y1EXHy/WZJ3Dmqd3wm7e3wOeXcaRM3UcXp2Zi2YFCLEMhZgXbmMaN72xVvpsMrfK6RpVSsaq+Welbl5zejyvDyB7xto93KGUCwM3vh5QwPTsmK3xbsqcAAPD+xsOYO6o3vg/+npzSVVXuwq/2AgAyH5uvPLv2P5twpKwOl03oj95BBR+tkPxx33H8uO842id5UdPow3kje+HppelYsqcAS/48Q0n3189CMo3errIOMT7YdBhvrMmGR5KUcZj75EXYkHUCL6zIUIVfr20MKZwPUEren/Yfx8upmSisrMfTvzwdAPDMT+koqQ70e1rJ/vqaLJ1cuf/bffh+TwEmDOyCVWlFSv8gELGEevbnQ3g3qNy567NdKiXU5a8ExhOthHp0yUFmOWnHq3Dnp7tQd4VPNY7e19BEY2deOXbmlSvfd9ec4ar3W3NL8fzyDOw7WoGtuWUqef/RpjzklNQo/QEAkhI8OPTofNxDPSN4OTUTf7tgBF5bnYVPt+Zj6mC1gvmiFwOy4Z9f78NrC87Q5RfBop8P4e31ORjUrT2unhzimd8v4+H/HUDHNgnY+9AFqnY7b0QvzH52NQDgDx9sx3e3n42q+iY8uuQg3lqXg433zAYQGjfvbzyskkvkYPvjzXm46ayQYpmsn+e/sBZZj18IAPj30nTsyi/HbR/vwEWnqWXbVzuOKO22ODUTB/41T3n3jy/V/Px0Sz5umJ6i/P7f7mN4OTUTlfVN3PaWIClKICs3Aha8tQV5pbX45RkD8McPd2BXfrlOSQYANwTlZPf2SThR04iK2kbkl9ZhwVsh+ak1hHoxOEaX7j+Oi0/rh9dWZ+FIWR2ue3MzV/YT/P3LPQACY/66/2xWrQdO7dEef507HH//YjdS04tx1tAemJzSDYBangPAZ1vz8ZtpKbrySX80wxfbA+3W5JPxxpps5fm7G3Lx7oZc0+9obXCv47mIaUTLOjlGrD5dxBgi5QQ3nmD1SmrAmWtkaHES8UBjvIJnURWL15yAwCm9XYjeVLBzjayesr7yy0B5rf2DESOH8VYhslkm1hNGMLJUpEFXJ8v89uIFJagJKhVYV89ohQML1Q3693Q/FvnO+iYf6hrZ6Rqb/Qpd9Rpru8p68QOG0PVHfhqzbzVDfRPnGwT6Nk0W+U6ad6y2Ie1Gxk6NRq7UNIpb+5I6GzTtxepL2noISKj6GqpP0BaS9BewyiD89/lknWUlIOaDUttHwoVdn5JaPgIhZUVtow9Vmr7Lqkdk7JC5hHclLBzZTdpIOy+R76gKWi/XUe1Gg7wnT6stWJ/zvp0eB0b8oflpNq61fY2MJ8O+ZNMxOaGl2e+n+MtPr/CYQQvPT11DUA6x+qAItH2T9CGjeYIg3OvLjcG6Ghwex/EKVwnlIqYRLT8CseIAz0Vs4WQNB07DuiWUFBf+HGJVIdKaIRq5MNoIhyzRfmTHuavaL5LYpo0Hkas+TiLcgAn0nKxmscydr82UYz5Z1skz7ZUPHQz8kgFiVgIeCfByVttG+WkWmi1RzHylmUEk2iSPvyKbVDoraT+6jxhv/EyLN3WsziufxX+eYoPwiKcQYX0jDSeij8Yr7B70mrVrOFKNtL3W95CoTCd9So5QuznlE8mO4kQC4A1Wb2XuInLOap2svs/7/JaMnmc6X7iwBFcJ5SKmEaN7FhcnCdwILdahDWsdq4h9ClsfYtXRfzgKSdFPsmoJ5ffLKsWAX5bDUopH3blsuDtqVXQ8jXNok80JDVoW+fyyjg9mmwoWx+lqRBSrCR4PvB72cpumWUs/TbtZPeR1JEcYzzJYRDlKjzGioKH7iJFscKLv8opn8ZWnQCX6EF7XVs17jG5l1lWiPUbDgeXDKZvqUTI8efWFs0YjfTJBIwNE5ylSd6Qcymvpsgsej4w+U5IkXZAJESQE5VxTs5gNPUnFsgL0mhw2tITtgFMGC7G5Eoo+XCWUi5iGa63goiURq5vmaMLqEPTEiyVUPBDZyhCrLA9nmhG9rmpVlmg3NrIsh6XYibZVZzhWW4B6D6+6jgf+hpa1WaIf+fyy7lSft9EJ5WcptkJ/0+3K60cej8S1hKJp1pLvs2BxpVzHsznIRPZWvD5Et7VI9UTx19RMfzs/o4huxox83sEIi6+8vqtEsRMgiNVHzQ5nnNZBxdJhkN29O8nG6x/hKKFI22sV0aJlKkqoCE1sTlnd8GSHkawIWG8Gx6mFjkl07Y0+a9fNWIo8SWUNq5eFdhWb4cApxaCLAFwllIuYRrTmUFesuGDB9QllQxEsAe45jwsWWqPiT3R4sBRIRvzQbmz8cnhK8XCVQpbrc/A6Hi2DZFnmbmhZ/KGtSwKWUBollJklFIPlfpV1ld8wLRDYuCRwLKGaVfn1bU5gpoAkeSNllRGgh6OEomgj/Uz/LaHfpP3ofEZXfkQ2wWZKDr41COs6Hk8JFbQM4ZBKP2bRY6ascNoSSkRcxLpMJooIHpnhyEQydu0qoZrjRAnFa2Mj3kmUEsrKdTwi56z6a2LxkBaZqsMEEryiJSyhXCWUo3CVUC5iGuEuZEVhdu/cxcmJb3YdbWkSWhza6E9maGz2Y/lBfZS9WIOTUQVdqMFbgG7NLWU+j2eIKmlPVOudilcaOLItrWlE+vEq5fehwirV+71HKpS/s0tqTOv/ZtfRqFpGZBeb00RDG72QnpJpCxyD23jILKrSPaPpqG/yI1fDK7ONXl2jD/uOViAjyP8jZbVITQ/Jt7LaJuSeqA3UVVLNLONAQSXfjxW1uyqiogvmlNQg/XgoMpvRRjCzqFqJ4ma0IebRx8O+oxWq37xNazYV2Y74U6qsb0YOxesf9obmEcKLnJIa7Mgrw8GCSmQw2k5LR+6JWtuOucn4IZHsCFh8zSxi84m0IFFe+P0y9h0NtdGm7BM4UlaropnGhqwThjTmldZi/7EKJUpoQUUdiqrq4fPLzPIAYENmCfx+WTd+jlfUo6CijpmHht01dpPPj4PBPrfvaEWQFwEaaxp9KqVBWW0j9h+rZBVjCsJz3oEgHc0zv7QWZQbBG45X1KOoMhQJk7S9V5KQfrxK6Vu03DVCSXUDUtOKUEs5yK+obcLhE2zZlxeUEwCQXljF7cs+v4zN2SdwmEpPy/uymkZszmHPpcc1kT6BwLgtrWlEfmmt6vleTp8CAlZGROFy+EQtNmefUL4zq7hacRhfVFWvqpOIU3oNQPpJdjAf3QYEGxljwytJSl20XCNySBtNlIWckhpU1Bmv9ZqaZYVGGtq5AlBbQh0rr9PJEwAorKyHLMvYklPKnJNchJDQ0gS4sIZYMq9tTUj0ukooF3q4jsmBv/53t3kiDb7YfiQClDiLe7/Wh4V2EVmIKEviDaKn4Hd8stNSude8sRH5paFFtnbT8YuXQyGhcwT4+ta6HIzp18kSDVxEQSzSVy1oCxG/n28JdeWrG3XP5r+wVvn7oe/2Y6lGqW6mhMouqcHFLwV4nfbIPJz9VKrq/VvrcpS/H/8hjVmGzy/jx30F3HcEK9NCyq3znlmlSmdkCTVn0WpmeVqYKQaXHyhU/t6aW4qrXlPzk1f2rR/uoOgMpLklGKKe4G+fh+YRwnL6mREe+Ha/8vc/vtyDF341QZfGbAW3O7iJ/2qH+mCJ9U0FjI08oLd8eXNdtqL8I/nOfioVry84A2mCigwad366S/l7872zMe2JlQCAv84djkXLDuG728/S5bnuzc344tZp+CXVVrIsY+oTK4TqtGsh+da6HLy1LgcvXTsBd3yyE/+YNxJPLQ30/91BJRrBK6uybNUBhA6IjaJ7bj9chjMGdcWMp1O5aQAoPMl98iIAoQOEiromXPD8GlwxsT+evOI0XPufTap8RuLupne34oxBXZXfF720lqscOeffIfq+2nEUPr/M7MuvrsrEMz8fUj37xcvr8PaNkzBrZG/MfGYVV7HCanef348ZT61ETaNP+XbAeN6QpNBV5bfX5+Dt9TmYOaIn3r3pTMx+djUmDOyCr/90Fs58TM1TYgnV2OxXLLDu/HQXLh3fH7OeXY3xA7ooSlYgJC9yT6gVZEBAWT372dWYNKgrPvjdFOU5abevd5ofEp/3zCoM793BMA35vgHd2qqez9TIYEA9X0x/MjA+aZ4CQNrxKny46TDuD8qtzMfmI4F3H/skh8sVFy7gRjxw4eJkg9EpYDzgscvHtjQJLhC5a260AsopiCirYgW0oqlZZwllb75mWeJZMYK2GxIc4Cs2RJWYohYrZj6ueEhK8KgspVj0ivgkExkP4Vxp2ZzNtgCxu8mzcn2RfD9pM9oKigbPEsYKKiklw54jxDKK3Ye04e2tXA8LV36lBa31CI1Og3RnrSXU3NG9lb+1VmCiIErF0qD11M68cluWYdsPlyl/i1jnEPD68u4j7LVJTklAUWNm2aNFs19GTaM1C0JZBhI0h/Prg1Z3QIBXLJC9VGOzXyUviQHFrnzxfkJ68bbDZaqDCKtXMA8VilmA1jbYs7JkgVZOR+tGTzzCVULFGVxDqMjAZasLFy7iCf27tDVP5CLiiKcFZqJDp7HRni9Vmw7ZfmhupvNbC/nDjvjHgKgSRNQvi13PAolej0q5l8SwDhehgVzHM4JdRRnA/z67fdvKhpZYbRAlD48Wnv8vK1CXLVH/z0ireWNFsdYQZp9O8noB2Fdmmd3uCF2BlJnPwwEps74pQHuiV2J+R6SOqHn9h9/O9mDH75dflnVXiCVIpvMdUVw1NvvVvuJs9DO1BSxFm+D3WP1uIkMMgyQIj60Q76LtjzGe4CqhXLgAXC2UCxcu4grtktzb9LGAeFpgak+27SJSjnhp0BsgWrEhA7Y1LT6GEsWKP8hItHWTYJmiTqvtWhlpoz6xFCkiyg0hS6gI+OC061LBSvAR0gZmujiRcWZmzWdliGk3xtG0hCLfWmvR0obAjFTSVbSuEZyQQIRvxDdTotcTV/JcFHaCFfj8MtPowUyZRMZ2o8+v4qUdvtLyWh1gQiy/VcUX6ct+P18u2NHZtsY+5RRcJVScwdWVRAYuX124cBFPaJvobWkSXCAy1jGRQpJDllDhRmCTJPNTalpPUUdtcGVZtm0REK4lVCQ2E6IbJVH/hHZ5o7UkYilSWEo8LUS+JxJxYOxaH1lp06ZmYgllYg1iQotf1kdpDIeullRCEdQ08oMsGMHMsoQoNbRKAe03ai2qREQUsewjSqgEEyWU5UjBFKKhuHeybllmW6mZ9ZcE6joenZZYm1kB7QeMlt0+v1/IP7LV69PK1U+/nyvHzMa+E3ScTHCVUC5cuHDhwkWcoW2Sq4SKBcTTKadTvg9FlBFGSPCYX+ugFRU1lK8Ov2xficFSGFqxyonE1UvR/iOq7LRrZaRVOrEUlk0CGzCRDVcsWUJZUkIRSyiTLGaWUH5ZNu1LVjausaCEItHSrEJUsaN1TK79Rq1STxY4WtZaQiV5JTT6+BZd4SihWnKeoHklGtzKL8vM7zX7Dg+thKL6eI2N/kErHrXR8USu0VrlOflcvyxz89qyhIqjg6pow7XnjzO40fEiA5evLly4iCe4SqjYQEOz2KlsLMApMq06htXC65EsbbJpx8t+WbbtmDzcaKe8kOrhoKHZD0kybxtRJZRd/Y5XklSbTpbCUsQnlEgfC8dlEs9Cw07LygLKIBqk/zT7AmOe961mPq+afDIaTPqS2rdWsF7OFSltu1gZn+FaaRDlgB0lA2DcX3x+WSmfVkjIsqxTrGi/w4gFpP8QPtUFrXS8HklnsSPLIXVWk082lPW8NwH69O1ttS/L4O9VjOii+0dgvuImDeXxyzoeypCZzsbpv0nfb2j2qdqost6aM3Ut3fTfWisrLS1KOsbYNmy/4Ksmn74/KXQwrLDYZVI8bwrlkZU8XDJOKkjxsnhyApMmTZK3bdtmnjCG0eTzY9g/f2xpMlodBnZrh7xSfYhQFy5cuIhF7Lx/LiY8sqylyXDhwoULFy5cuHARJnKfvKilSXAEkiRtl2V5klk61xLqJMOXf5yOK1/d0NJkxBzCMbONZ9x89ql4c11OS5NxUmN47w7cELKzRvbCyrSiiNQ7qm8nHCxgh5jmISkhPMedI/t0RK9ObbAhs4R5Wjuid0ekF1bZLt8ID/5iNB7+3wGhtCnd2yH3RGwqpQf3aI87Zg9FciLbnKB3p2QUVjaYlpFdEn4YcSAQKnvZgUIAwI3TU7A2oxgzhvXEZ1vzdaHDtbhh2iC8t/EwAGDKqd2wOYcdspqgfZIXXdsnKWGwB/doj05tE3Vhn1n96PRTOnNDX9tF+ySvcOjrG6en4N0NuQCAC8b0xk/7Cy3V1SE5gXvlZe7o3lh9qBgzhvbACoa86NQmAZX17LxG5T58yRhszS1FQ7NfaWMtZgzrAQBITvBi+cFAmuunDkR5bRMmDuyKDzYdRk6wr80c0RO1DT5MG9IdL6zIMP7gMDFpUFdsC4ZOXzB1ED7YdDis8maO6InxA7ogu7gG3+0+BgDo1TEZRVWhsTZ3dG8cLatDn85tFLntkUJWGddNGYi6Rh++3nnUFg1zRvXC8oOh9u3buQ06tklgzh+DurfDYYYM+/WUgfhoc57y+7RTOqNTm0RMSukKAPhu1zGcNbQHl1+XnN4PFXVNWH2oWPX8l2ecggSPhF6d2uBFi2179tAeaJPoQb8ubbElpxRpxwNj9w/nDEZmUTUafX74ZRnrM0+o8l0+oT+2Hy4zPECcOLALdlDh5GcM64G1GSWG9EwY2EUJQX/eiJ5Ym1GCySndMGVwN+SW1GBQ9/aK9dqWnFJszA7QNaBbW+SX1ln6dqv4zbRByCyqRmZRtarvXTq+H3x+Gd/vKVCl79Y+CaU1jcrvP5wzGPuOBeQgzc8rJvbHVzvY/fKsod1RVNmAzm0TlTF1x6yh2JlXjnWZAV5OHNgF3donKzLgrKHdkZzgRWZRNWaP6oV31ucCAM4d3lPpO5eOD/SlVenqvkRbBl51xin4fPsR1fu75gwDAOzOL0dqMO/c0b2RlOBB9/ZJ+O+2fMWiafyALpg+pDu8HgkFFfXILKrGrvxytE/y4pZzBuP55Rm6snfnl6PR58fY/p1xuKQWS/cfV6W5bspA/Ly/EOeP6Y21GcW45PR+2H+sEqP6dkJdow+FlfX4cV8gT/skL349dRDeWJOtKuOc4T0xrn8nxRfbK6lZiuXOnbOHKfLxvBE90aNDMo6U1Sn9bPbIXiisqkddow8p3dtjaK8O+HRrPirqmnD6gC7w+f0Y178zPtmSr2tLwjv6u3t0SEJJdSPz9x/OGYzjlQG+nT2sB9omerHvaCVO7dEO7ZMTUFBejyZ/wLqKyLXLxvdDSo/28MvAm2uzcd6IXhjWu4OSr6KuCQUV9SioqMecUb2xMfsEdgfn8N9MG4Qf9hZg2pAeGNKzPfYdrUCPDsn4dGvgW35xej8M6dle9Q0Xn9YXQ3t10H0rABw4VokB3drhWHlALndum4iGZj8OFlRClgMWfIVV9cgvrUOfTm0wpFd7TE7pBgB4cUWGIrsvG98PAJDSI1B3YWW9wt8rJ56CL3cE+ujIPh1x2imd0a9LW6w+VIyu7ZIwqm9HLE7NUvi/JacUZ57aDZV1zThaXouq+mY0+2UM6NqO+Q2tGa4SKs4Qrq5kaE/2QI13/H3eCDy9NN12/pZ0GtiSuO/i0a4SqgVw7ZkDlAls3ti+OFTIXrQ/fvk4TH1ihWl5yQkey2b19180Cte9uZn5bt6YPrqFFwBcPekUfLgpj5FDDPPH9sWdc4bhs615+MeXe3XvP/jdmTjzcfPvNQNLEXPVpAHILq4R2owu/vVEXPTiOvTt3AYFFfVh0wMAo/t2wgETpV/H5ARUmVxreOAXozFzRC+Vs+bLJ/TH1zuPonv7JDxxxTj89l1ji9/bzhuKuz/fLU68AZ6/ZjzGPPgTAOChS8Yoz4f0bI/7v92vSktvLi6f0B8PXzoWJ2oa8f2eAlw3ZaCpEuqDm6dgZJ+OGP1AoL6Vf5uJA8cqceGLa1WKtRevnYALnl+jyvvt7Wdj+YFC3Py+c9bQF47rq9sgsfDPC0fhlnMGK0qo1xdMwrn/TmUqCQBg6uBu2JSt5sVHN0/BpYvXM9P/5zehA8eUhUtU7249dwguOb0fLnxxLTPvE1eMwx2f7FR+Tx/SHRuyApudG6an4IbpKQCA2z7egSWaDS4AfPC7Kbq6H71snPKsXZIXC78KjPV3bzpTeW5HCaXdkF4zaQA+25aPv88bgY5tEnH/N/uUd2/dOBmnP/wzAOCRy8aGrYSiaV+XWYLSmkbcd/Fo/JniHd0ON7+3DcsPFuLu80fg3z8F1ib3XjgKHZITlM0aT/H6+oIz8IcPtuuev3nDZFX7brxnNlLTinDTu1t1aZ+56nQ881M6NueU4p0bJytp7pw9TKWE+u72s1X57pozHACwNTegDLr2zIH4ZEsgff8ubfHitRMCm90gb+n6CLYfLlUpOC4d3w/f7goo7m49dwheW52lyvvhzVNUv8k33nPhKOZzouy59syBeO6a8crz5645HX/5LCTXxvTrhC9unY7B9/6gPPvHvJFYm7EOXdoloryWfUVo8XUTMf3JlQCAd6h254HU//0dM3R8AQIKmVuihDu5AAAgAElEQVRmDMaN72xVja+f7joHly5ep7oC9udZQ9G7cxv88+t9Kt4T3DA9BUN6dkBVfRPGPRSoq12SFy/8agIA4Fj5epXS7bHLxmJ9Vgk+3JSHRy4dgwXTUpR3RAZ9fMsUTB/SA+cO74k7P92lo/+5q8ejV6c22Jpbiqte24jhvTvg7vNHAAAufmkt9h2txF1zhuOc4T0VXnx081RVGUQJ9fgV43BWkLf3XTQaq9KLsCq9GOP6d8beoxXo36Ut1i+chYmPLENpTSN+deYA1Zjv0SFZ6aOlNY2YGLQGPn90b1w1aQAA4OpJA3DxS+vQuW0ivrntLBUdP+4twB8/2oEJA7virjnD8cqqLNXBGimb4NtdR7F0/3HVmujWc4bg8cvHwQhXvbYBW3PL8O+rTsf0Id1VSqhTurbF+79V9yuvJOHZZYdw23lD8Je5wxX5SPc/wpNbZw5RFCUEhwqrkJpejN/PGIyLTuuL8tpGnRKK5h1R4Fw/dSAevWycMjdOGtQVf5k7HL9+czNSurfTjUEe1mWU4OudRzGsVwc8H+yLAPDXucMNcgXQ2OzH8PsCN3z+delY/OvSsar3xyvqFSXUS9eGyibf8PAlY9C9Q7IQnSx8vi0f//fFHkwf0h2LrhmvPB/TrzNueX8bZgzrofomANhzpFzh7zWTByhKqKV3naOkIbyub/JRSihzfpxMcB2Tu3CB8H1cuHBhF0kGTkwd8iPMhJGTYp5loIhPEBHw3HAkJTgzJbF8xngkcX5GwnGuU0US2ujyiFNeGeahv4Hwo5ux6NFCYjwPlwWsuhT/MhKdjp3f6WYVdfRttV5WdC27YyPBIxn64NHSxvsmu64bIhEJjcBjwP9Iyk5SdoJA/XSf1SbnjR2jcnX1cNLS7UhXY7Uf0f7JSV6rvHV6eUX8zmi/hTVutPxpE7QgNYrOaFf+87I1+2Sm3LbTR0k5qvY1SO/1SKaH14RvXHmm9OXAv3R5ZL4R5RldRYJH0uXTOnXXziO0HOL1ZaM+zpqXjMD6LitjyCNJQuklBm/5ZZq/E52bSH1eiu92+n84QS/MggqY8c+x9ZpuLhSr0+zbIzkHxjtcJVScQSTiw8kIu45KCU5WSygXLQO6vxlNsEabrHBhFMGHNxrCdexLJmOeEiQ5wRln26xJ3yNJwgvQSCihhMoUSELKocsjJv2yLAuV4aS8430WU2HEUkxZ4LUEvaxXNnc2aQoH4Sy8jXKyxr1dJZTXIxnOj1qe8HhkIzI1gPDnZlVZBpsEbS2RGMPaso3an1RPJ9HxmtOkCUa7Hx0tnDKoF3S9ojKWjEtasUMi5pnxVtvmtOLAiWYh40MbwU+kbG/we4wkoF0aednoQx2ad5JkPDZZYCk3Jc7fgGae17wjv0g/5tFCnpOyWbwT5RnttN3rDSnIyZyUqGlTbV+jeZnMkYm854B+zJqRbVcJFVLOifVTI96GylSnZaYRkE00SH1EXkhSqI9ZmZvDmgtN6jFVQkVonay0CaNRaJLNDg0iORfFO1wl1MmGVjoWwh3jzW4ITRdRBK0D0C66aIhOXnb6v1HZPKuHZrs70SBIjbxTaKcsoZh1S+J8isSaxiEdFLUJCT1TlFAQ6zNOyjtefSwehiunA4pETZnMlJynDrer1dNmUbAWtYk2Q4olePQ8o6FX3rDT2bWei+T622uwSYjkwp8UbXSCTzahPIskOo0WViyheJHYPJKkbDBpXojKWK2Cgs5rdcPptMtNUruZBQULRn1GKd+uEoqT0ednh7tnstGkctYhhFEOieoHvHSilhwh3umViqIso3nkpZRwJPqjtu9r+ze9dOCNE9JP2YdRgoQapDdScmnh9UhCSmVCq5F/WtZ4ZtVH/8uqQ1WmJqodEFLqWLF+DUcJZQYzfodbdWh8GPc9GnRaowNds3JOdrhKqDjDSeo/2xThDnHXEMpFNEErYRKU02V9ukhOXqyrCwQ8OSMaJtwMvOuvTi1kWKV4JL3pPze/wEbFCZp49YqkYW0sReltdOhaZYAO3nPWIji89g1XiRhu/fp6IjM+WePAblh7j0cyXKRrP4H3Tfav40VOhtEn4FrqInoNULE2MLr2Q/6lNndaSxQOjVaUUDz+JnglhSl0cVZlLEsJZZW3kbLg19KhXC0y+EZy7cioP9u13uMrcKmNrkF/EKrDQ+Q/9TBMhb9ZfyNvCb1sSyixCum28VIKckUJpbuOp87vVynAOEooBw72jCCkyFUOixjzINNdQPCZwFAx+gLlOp7gdxJ20rI02tfxzGDWPyNVtxEbaNHvXsezD1cJ5cIFwrfwcOHCCmirAuITqlPbRF06KYIS2mhi5J3GOXUdz8gfhxPg+SMSXatEYtEgpmASKUefNlFgY0XDKWVigA6exkfokcW6GBtPBh08mpxeq4ou1q1uwlkbCCOlsRECC3hDWwnVL97VBrtXOCO5/jZSFkRDCWW0OWJdxxMlycqmipdU1Yds8CJEP6WEEryOp0W0llekhxpZSBG+GHVnu3KC1x99fr9CnEp3ZKMe5nU8ZjrWIQCvTLNNtKTOb3I9yQhahSipmzgH18o5LW1iPpP4xDhxdUtESUxSsK/e8dOLROo2+j7Fas2ilS7Ld52VAwQrinOrMKMjUodBZDyz5m+6TlMlrquF4sJVQp1kaK1jIdzviiefULedN8RynhumDYoAJYEoai7E0DE5FIyUVgL06tQGM4b1YEZbMZvckhM8qmghVmCshGI/nzWyl626QnUGKv3lGadw01wTjHAjgnvmjxROK2IJ9cilY3DbeUPCXtS0S9L7XaGbcsFU9nh8/fozTMsOObgNFUhfxxNRRF1tgcfm9PCeOz/ZSOD7UKGf0jTdPXc47rtolJKfhTMGBcLT/2ryANwYjAbHw4xhPZS/zczwCaxOL6wNRM+O7Og/7/2WHb1rVN9OmDq4G66aNEDIkS0Br/uQbxjbvxO/MAbOH9PbUnoe/j5vhPL3ZeP74aJxfZGcGPKFpv1EjyThnxeOUuVj4cVrJ+DG6Sm45PR+wrSIbPRYvlm0Y+KBi0crf//fBSE6WeU+cpk6YtRTV44zpEFb7+sLzsCvJouP+9A3hp49fGkg+qVWSUpo4UFkY20Fz10zHrNH9sKg7u1Vz88Y1BXnDO+Jxy7j00P4YkST3U2jJAUiCBJ0CM75Pn9oI8vzEdavcxskJXi4cwMB4b3KcoVlOUk9khUFGPu7tHIsOcGDKyb2V36Tt4R3NOfIO6tWxkDgW0i+0f06YeaInrp1kNcj4Y5ZQ3Hn7GEAxNbqndsmYv7YPnhjwSTdu3BmpXdumowrJvS35StpwdRBuPnsU7npFCszo2ui5F+hwyp1onlj+uDVX0/UpTNTsIgi3Pn+5rNPVUXY1OKaSQNU0UdpOLXU0JajGKcxr3uH/rZ7QOTCVUKdtKA3xCLgLXRetLkBBgJhiGMFVpRQPcIIBSqKMf34C/0FU1OEy+ndKRm5T16EhzUhT83wfxeMUC2meOgnkMZFAL06hfoNHRI4yevBB7+bggvH9dXlMTIrB4Dt98/FLyxsnmgYWRHwFujtLcoNHrq2T+K+e+qXpwmXM2NYT8P3EwZ2Uf4OWNEYr1YWTEvB/10wMhQFyMY1kl4dk9GLoTCg69ZuKAmmDO5uWj5LFCt+xWQha350M+A/zTMR8HhKnl4+oT+W/DkQBt5soXrZeOO+zLSEYp3KU337jtnDcPOMwcH62eW+feNk5D55EZ688jQ8dMkYQxo++N0UXDdlYLA8dYG8djXC13+arijBCFibSq9HwuLr9JuIc4ezx8CPd87Ap7+fhm7tk3RtdBWlBDaKPkWDyIQ7Zg1jvuehYxu9hacd/GnmUOXvaUO6Y/GvJ5oo1yTccs5gVT4WLjm9Hx66ZIyltQzLSkiXRklLWaxokg/u2QG5T16E3Ccvwm3nhejUOU+W9IrruaP76Mqn4fVIivySAFwwpg+evFJcthLQ3zi8d0fdd/TulIxrJg9k5iU0O33Gd9opXfDWjZN1/hTbJnrx/m/PxIg+HVXP6TlSUaQIbPbt4KpJobH1r6DSzuf3hxRBnP7w1Z/OwqFH53OVzaE8DAsnVjrVU+MG0Pbj2aN6YdHVoVD12v7OkhGiigCvRnlG8iV4PHj3pjMxWrP29UjA3eePwA3BwwHt2oRWltHlvnr9GTjz1G626WThvBG9sOia8eYJoefZI5eNVfohs72CD43GiohPKB5eW3AGJqXo+cEaB3aut4V7Je6+i0cbHk4+9cvTMHc0+0AjbLcVtuQTpUyNoBVYa4erhIozxJpPqHDGnh2nkjyEG4HHyiIpkmanBEYyNRrWbKzNngvnQF9rMxrTZg4tlYgmNvq/ncMbu35hog3Sd9tQkaAkydhBM43IRMdzphzWJkS5jqf8n3045YeMdobLCk/OgojZPS+F6vYRNxGvXGO6eMVo8/HKMRo2bJ8hnHptNo02G235oH3Hc0BONn9mivGWgpbqSE7TIv1ZsYQyuCbKy67zHcVMo/5XC69HYio+RGF81VGsvBBt0Zk3+FaZob9D1jx8muzKf2024jvI55cpayR2PaIbWbbDadYzoeKCdBi/DznZD/xm+oSyWZfZgQ9RyJspD0X7ZLSXMLz25kHk4MvJ5Uk4kQ7DzeMUwl2v0cp65nszSygH97InG2JzNeEiYnBaAIcz+I2celpFNAVgNASOk2Gt47H+1gh6keSUTx7RzT2THhuWUOEi2guVNola/xLRrZ+GU2OKxUPlOp4D7eZUuGNCi0RduTAr2fS9xLDcYSyjef2MN19Z3aTzrmNxnXobbCysVG23ZbR0qcO7q9M2c/y+8UKoxwLYIbQjN9hF+jNPUalKI0gi2/pFvTnXwigqX7RA6I6WtwOejGX5bzGkybayV52RXNPxy6E5lacsF1VCsZKxnynaHWZ9NEzHiqROpxpvyhpElH6NgtWkj4SCAJB08XEQxoLRlkfkOp42rRNgR22Mr/V/pMglxZpdWXSj39lH7K0mXBgi7Cgj9PG0lWwcyRiOsEqMUxPGaFhCGcHKJGx34+vK1MiCvo4XzpgOpy8atXGkHMlGW7mZnKD2zSQqr5xSxKjgUJGsb6B9QoVfvgOFgPJDIllwdGrymiX6WP5O+I7JOUoo42q50Ct32OmsWkLxYNsSSuL/1r7jKcjJJjGWrh6EHMc6NrzE6hWoTOjKnsV+qsprsnqXIIUlDxQdhkkhRjLdoygYoqM44PGEpewxosluF9fmI/Nzs99PtYVEpac2snSkRyUxS8HOIk7/jOUTyi501/EsKP71dLEV4to9BvlNviOhhZVQVtdptGwiYPlzVNIr483gwEIpxxIpxnDoOl5LImIHDgYyUH2QE1/8iiW4SqiTFE4NmXBkldlVo1hFNAS0oYIgfg+CXATR6JQlVBh90ShrvJw28haGZPy01TgIt3qNxA4b+FY41ssSLSdJsYQK/6DCKflGm7jroitxYKaklKF3QK3kpV7wPoFvCWBMly49zzm6nWtPFrKE4zBZ9dtAYdfMmWC0m8KTGSJKIpLGjozWWkywSrCkvLRMgTNrRPLp0Vqz6K1sAv96GcoeQ59QtseZOp+i8PLTUcj09NFpzcC+jmdAC12fUA2MskjdBtY6omXzruOZWUKFFJrsdPHgKsAwap+i4ONDsSFwUOnBqs+OjI8D9pvC2lwc+tudE+0jPrUAJzFibaCHs2GJ13u00biOYMQZK+Htw5mrXOW+s6DZqbKEarExzW/gWJMzVkE22Vr/NaLiKlyLLaPrM+GCaQmVwDpFtwcnr0kDGksogbQi5dEwc0xOw+nForZqXvFGG6RIKxMAfX9UWUJp0vKUUDFpCUUpi6MpsggHjJouHC6JKblIPUZyPHyumCm1DXkgYN3hJEQidYpcnbHbxbXZyFgJXGXVX8ej6WIrp/SEsGgzI5e2SrUDkUME0RZmOd0H9AdfpE6ixHXqBoJtHlgc0SxLQrIcYZXE4wMLTopgtpP52JHx0QDXz5ihX7zQ3/F2fTGW4CqhTjLYPSW/ciI7akE4g8/Ja22RFJoXnaaOWnb5BH00DhrhRs/rmJxg+D1d2jkTbcjoO5y0hGmT6LHEEzoEul2cPVRdRgeHoro5hV6d2ih/szh9+gBrEcrsoCenTeaN6YOLT9dH6gPCV3CEfZ1YW57MbtshPQOhu7NLqlXPxcNIB8sPjzx13cHZtmMb476o7btasD6hc9uATJAhm7bR5JSuhu/nj+2Dqyfxo9QAwIjeHXXPtFEaidJCgsTk++xRvXTPRDZTWtnYp3NgLF06PiTP+FOL+sWUYPQk0YOFcf07B0oJFtNeY2k37pTOzHx0m2j5r+VNIJodu36785yWH/TPIT07qN794jT22L84+HxAt3bcekb1ZUd1TU7wMCNGAsCvJg9gPp89Ut8/ZgWfje3P5rMRulLzJo+NF47rI1TWFcH1UG9KjutgUwEzY1gPfXsx6DWTZR3bJFCWE+w0Q3t1YL+gYEfmk0ivs0cFolnNHxvqU2cNUcs3lizh4TTO+CIgPOlLZMLpAZlA69WJUoNEuGSBbD7HC87D108NlKXlc/cOgSikPllWIgteMCbUxzySpNDBk0Hzx6r7JKvdr56kH0Mk35Ae/DYm63oSLXVk3wCN2ihk2hrpPqF917tTsuKQnQVJkjBzRCii59CeHVX0aqE44Oe025xgH+PJXi20/fnXUwYJ5XMGBkqN4L+Evh4dknTzi5LWwf0OGaMDg3L94tPsRVsm8v0yk/1RPIJtLRZqA1cHZR+xtTNzYQqj9UD39klok+jF0fI6AMCBf12A0Q/8xEwrSRI23jML055YKVTvn2cPw+fbjwAADj06H40+P9okeLAus8QS/TQSGCfuXdsloqy2yXJZVmRA5mPzcaCgEpe8vJ75/qVrJ+CDTYexJacUbyw4A7NH9cZzV4+Hzy+jodmHzm0T8ea6HBRXNTDzb7pnFob+80cAgegotNULC2mPzMO3u47iH1/uxXkjeuKN30zCr/+zWXn/x5lDcMuMwZj4yDIA1kJe8/iS/ug8JAb5f+jR+ZAho7KuGYtTM/HuhlxIwf9pceP0FLy7IVf1bP7YPvhx33HVsy//OB1XvroBALD3oQsgAQpPzPD+b8/Eqff8oPxO8nqErq9ddcYpSh99+8bJGH5fqL77Lx6FS07vjwSvhGGCdDgNMlF9d/tZeGd9rmHar/44XXEEzEP6o/NCZdugp3O7RKQ9Mg8j71+qev7ydRPg9Ui4etIA3Ts7ePaq05FRVI3XVmeprPgeuHg0/vX9gbDL3/nAXCx4azM2ZZcCADYsnIWs4mqkphfrFpw0nxK9kipKIS8dD1r5+qeZQ/DKqixuerJoeckkFPx7vz0TPr+Mnw8cx+0f79S9T2Ys8Mnm38xIctcDc03lxxUT++OaSQPwyGVjMeI+dftvumc2urRLRIJHUo3nQ4/O1x0qNAfHbGKCpLpqkP7oPPj9+quSgPlijqUc79Y+CWmPzENyggdPLU0LFmRcDsEDvxiNob06CCuh3rxhkqr45AQPZgzrgbUZJXjhV+Mxsk8nPHnFOCz8aq8qH7l6fujR+VxLAAD45JapmJTSFXf/dzezftuWUNDWGfh98Wl9VUqlnffPRZd2iXh0yUFdGTdOT8G1Zw5Em0Qv9j50PsY99LMuzf9uP4u5Rtn38AVc2u+ZPwq3njsEM59ZpXr+xm8moaHZpxpj88b2Rdoj89Amkb05M8LWf85R+mybBC/qmnxYcfe5qjQvXzsRTdf4df1ei1vPHYybzkoxTMPyCWMG0j9ySmqYZVlBe5Vynp3/v3+Yxi9AMxjPHd6Tk1CPa88cgCsm9kebRK8yNu/9OjAmztYcMi3589nC5X71x+lCV/u6d0hW6gVC/f3hS8YACK1/Pt6cx8wvSUDGY/OFDy3+dclY3H/xaJVy4NZzh6BLu6ASyi9jcM8OSt/9a3B8SwD+eeEo/N8FI7gy6OXrJqLZH+qTLJruPn+47tk1kwMy3Gis3D5rKG45Z7CSZghFIw3lZp+AYnX9P2aZ9vm3bpisrG8Gdm+naistaMs1et1KcOE4+zIBAO67aBT+Pm+EI2udcCBJanmx6Z7ZXD6Knt8fenS+ah3MAjk86tO5jdIO2Rr5I4Ku1DzcWqB0PdcnVMTgKqFaEXyyjHbUwr5dkr556bnDyrUyeuGclOBRTjrCMc1PZFzHCzgStqGEskBGgtdjeFKTnOBRNlQdkhPg9UjKd5KNk5EVF+3rKtlrroRqk+hVFisJXg8SvR7V9ZpEj6ScVJkhOcGDBpP6AulC/YTwomdHL5ITjftEdwYdHk3faGz2q06drV5f1Ar05EQxJVQi1aba9pUkibnpbQkkJ3hVV15YCzq6zxmVEy5YCzfSf1nv7FgyJSZ4lLFON6NT13ETg2OGoF2SV+GdzgkqxdM2CV40+Zpt19tGw/+2NhfBWpC2510bSfLq6xEdY8kJXtN+5ZEkeDwSkj1eJHgkVV9N9ErMfsGSp41BBV+Cx6OSz070Wy10myb+xTjVr0SvxxI9WrkvI3Tlkyy+2zMs80j/Z/GJLjLRK0XkujdvftR+e5tEL3dBLUmhtufxjOfn0eibvF6JOe94PZJujBEaFZqC/xr5CqNpa5/kRU2jT+G57rpusN+bgfCiodnHTeMx2MDwQPqHyLJKKNy7Sd3a6KE0tKXz2pB9tSjUV8yUA1Z8g5qlpXlC1xsKjBD412zMS5K1dQur3yRR60iicNHygsjaNgZ9LjAf6L9FTS+7L5D6Qv759MpoLU2s9iL5jB1qB/4VaU/t+saoj9B18trNigJK+wlm/LeLkDU1PQgV82B++iAjjfgoqhw12uewYFeR51T+lkIoXpdmfBjkcX1COYPWo7I8SWB0AuHzy6bCiTbPtnKVjpc2nOt4rM1QLPibUEUp4aQRNlEX/BxtefQCyMq2P9L+qljNTfdJ4ShYVup0OF20YMQC2iLIiStfsXASY0RCpKMkaU+lFAeqGu5aZZOx81pNWvLcJEx4uBxgLSyVcW/iG0fk++1E82JBsYTySsLOmU0dkwsyT3QasTrfaKMb0b6IjHhjdGgRjbHLq8Lu+HBymmZvqgXpoRJYGVehdrSQyaAcFtibUJvlMqoRaQOz63giFlZm1MfC3ENg5hNK1J+mU/77lIhunHrNIhyy0BL+Z5TxGPwdVf9rsdO9LMHQnxDjmYeaU0zLjlOexDpE50ltWtcnlH24SqhWBJ9ftiScrCwkrUYbEgFLYWLXJ66TIkBkw+S0bxstaN5Y2bhrrcvCWSCysrLK81PWLaFIJrartY1Ymwh4G08ZMpr9zkTHiyXw+C+BHRbbSX2UR1L/Tcawtg7hPiKQTDsWjPxkBNKTdGIfzkvFVkKFlG7hOsFWhfbWvLMywogFVYLXI+4Q3qEhzJN7WtZY9UtoxD8jh8deg8MBo77iFPQWEJx0wkoo5wj0SPqr3+SX6PxlWZY4dFhi1ObKdTwbck4X5U0gjRF4KY2K0Fq4xAPMDktFY7o4tX4h1iw8Z/92xpGdaItKG4b5XcZ9IjIdJbZWddZB88rwUCv4r5hj8uhwJd55LwquY3IDPquDCpwsnHIerhIqzmAknnx+2dLJrqXTHk5SkUgjPLCu47D8REUbIt8U6ZDDSSollHg+K6btToGeNAnnWvK0zmmwro2KQL+RCG1KzPw9xSqM1kdGokcdJch5aBcEobDPsiZd6G+nKSF18a0OgvUKVsxLx1JCiYQdN6JNnUafSHudRQRNiiWUJ9Q+YdIXriWUNrtVOcWanmSTdgcCV6p5YPHb6Y2/01EhnZS14cwVdi0znCJfRIljR+SJjVPr5UaijFiCmXJV9EDPKesu5Ro6p95osT9cHZTe6le/5nOhBmvKM7KajUVLqPhcpToPVpu4jsmdQcvv+F04BqHreGTBDGumwNwTpjCOjFgKJ9uhcS1KAaPFthO+FkL1WANJTys+LFlCRdgEicUaepHt1DUHpxEOOXavOGqtLGieqJQxrWSm5/uSCX07vRh3JHw4cbGgsoSSFFq0G0B6bIvVL04jSckr1soi0whaPzaqsmFMsd0Nv518ihLKI1mQBybzl2B78OS7zhLKooJZywea30bf6NThgN2uw50fZW26MMuzAY/kzHxhpQjSD8KdLo34EM51vHDrVmAibESuDEXa6ttJ8HhiVfY61bu9Jtfx4sWCgvQTp+YwFwxYUFrHS7+JdxidnUmMdC6sw1VCxRmMhP/QXh10i6ou7dSRkIjz5iE9OzjkE0q4CB1YVyHs+oQioXCdgIgxFgkBb4bR/dghq/VQN+zA7qHyWQsYHpu84Tp7lsk/Mgb30H8ja9FKb+ZJN3FykhSNBqitsX+Xto7Ub7ccIwVt/66hMq0u8nt00Ic6Fwm17QSM5A8vnHCXtkmKdSHdl7szvsMq2iUH6qQjfEkSbXXEJ9hobCYHnX8PNghvrcsTlK2DObKB0NixjVg8EK3sJmBZ5hH+DuvVQcjkXxTDgv1KUS5TJZiJ6u7tA+3bu3MbQ3nQr0sgnHqXdkmmi7l2gsEFeAcs2v5g1ZJXq2SXZVnH767t9HORk/OTHWjbqkPQeXo41y3DxYhgyHqPJOkiKumCU3Ac7PbqFOhjPTsmC0nRocE6h/YO9GuWQhcA+nZuI1CaGexv2LXd0qibEhnIgpm/MqNyU4JrkA6MADc0ePIulsCzjOXBqeULieQ2pCd7HuHVQwLA0P0wnD7ZL5hXNMCNFoROMg5PZawNncbQIM+ctu7v9P/t3Xt8HFX9//HXJ2nSJE1zadK0aZo2vaRN7/c7l7ZcKrRAUUGuchERgSJegC8KWBS/v+9Xf4hfUBFURAF5AD8Bxa8PFUQUFMRaAeUmUgq0BZqW0jZt0+v5/TEzyezu7GY3yTbd9v18POaR7MyZM2d358zls2fO8af86cMAACAASURBVK8n6/uld11X7p+Hw9ds6Yg63weDlzRUJX5+1aWJ33kyUftNsL2yDEbPTiYo5/74ng8k8Z9r8N0NCV1fBvQ4XvfQ6Hg5ygyGV/fh9WZvKM27PzGLptq+XPDjFTHp7v3kbJ54tblt6OqyogJ+fP5MJg0uj7yJuOsTM2mo6sPhX/99zPyOOnyMd/H8EcwZUUV9ZUnC0MuBXvnGbz97BH9ZtZG7nnmTf73XEpPfqdMHc/+KNQD8v4vm8NHvPZ2Qx82nTyHPYPGEWgZfUkJlSQFHfiN2e5csGMF3fp986PR46dyY3Hb2NP742gYuu9cbQv2bp05qG3Y37P+eMol3N7dSVJDPGd9/hi2tqUfjCja9bOFIfvfye7y4bgv5EVGxJ69ayLz/ejxhfnDyOH3mEO599q0uXUz9z+lTOO5bT7L2gx0J5QsLX9gFgZeodLecPoW9+xyX3/dch9u+5fQpLPM/28lDKmLKAPC5Y0bxzUf/BXg3IM1bd7ZdaE8fWgnAQxfPZel3/sS6za0pt/WZoxr5n9+9ljC/qk8hG7ft4uNzG3j13S1MrKugrrKYM3/wl7Y0Pzl/Jh+/41nA2ydWvrmJ2/64Ckjd38w1i8dy9zPe8NAdXRf/ctlh/P6V9QwoK6K0qBfThlayqjl2CN0fnTuDf6zd3FaWjkQNi/3klQtobtmZdJ34IcmbBvbltrOn8db729m1Zx/feuw1Nm3fzH9/ZAJX/cwbjvvWM6cyb2QVr7y7BYgdHW/JxNq27/h/Lzss5obwV5cdzvE3PxlZjk/PH8GtT3j1Obi4/4/jmvjRn1YDweN4qVtC1fTtzT0XzObxV95j114v4BreZ8tLCrjzvBlMrq9g8lcebZtfmB87UuMDF83hlsf/zR//1UxlSSE/OncGU4dUcvKtf0oo99XHNzF7eD9mDa8C4GefnkPz1p1cdPfKyPc5d0QVt545lUdeWMev/vEuAPdcMCu6KX+ecfcnZjGmti9/Xf1+ZH5LJtbGBEZ/cek8Snv3Yt0HrVSUFLDklqcS1rnnglm89M4WLvzJ32AvMVGKJ69ayDtx9TLsnLkN1JT1ZvGEWjZtTz7i6eePGc2kwRUc0VjNb1/03ucXjh3FP9ZuZvuuvdx82hQee/k9+hYVMLKmb9J8AB6+ZB6VJQWhPoVSJk/6o8eDF8/lw9/9c8L84FAcDsYFrRqDlr3zRnrf2+BK78L17U3bmZ/B8PZR5X7yygVAe2fvwbEpymOfO4LW3bF9zoXLe9vZ09i20zsPxQfl9mcn0/d8chavvLOVvDyjoqSQH503g+KCfE67/ZmYYNgPz5lOTd/oG7PTZwyhvLiA48fXcs+z3vF0YVMNFx05IjJ9cJycPLiClW9vShoI//kl8/j1i+8ybWgli29OrBdhP71gFnudSyhjsGt1R4vPVC2WwgGOJ69cwMZtu6goLvC3HayfLN/kbjh5PB8aPzDp8gcvnss/127mpMl1KXJJ7YkvzI+c/5vLj+jWQSzS7RNq1IBSrlsyrtvqQXBcbqqNPm4lu35eNG4g3zljKovGDWib9/NL5vHv9S0x6R68eC7VfTr+MWfZUY2MHljGwqaaDErfLihlRUkhd543gyn1lQlpurt11O0fn8Zzb39AeXHXgyphk+or+N5Z05g/Or1j8twR1dx65lQWjsnss7tuyViOaOzP1CHtn1V9vxJ+eM50Zg7rl5B+wegavnvmVI4ZOyBhWbyo/eaLx49h7ogqpjfE5v3454+kZWdmowAPqijmjnOnJ+R1sEr2Q3DTwDJuP3sahzVWJywL/9CVThDq4UvmtR2XpZ2CULnGryvXLB5LS+sebnrMuxEPKkn8NfWY2jLG1Ja1BaEAjvQviFt3Jw4vfHijt+zDU+t4cOXatvkdNXOO95mjGzscBrcgP49RA/oyakBffvvSe/zrvZaYm4JwC5hkB8MTJw1q+39yfUVkmisWNWUWhEqjNVZFSWHMSezDUwdHBqEGV5a03YycNXso330idTmCE3lBfh6Lxg3kxXVbIls+xLfOKS8uYPOO3RQXJo/cp8WCP0ZZUQHzR/fnnr+8Fb84xt6Ix/GinOB/V/FBqOrSQja0xN5QBd9lsiFmwxcmFcUFfhDKex1clNeUFTFnRDU/W7kmaZkAjhk7IDIIdey4Adz77NsYcMPSCQBs3xV7Mp83sv3ktGjcQOoqituCUFHBQ/C+46KCfBY21fD4K+tTlg1gfF054+vKY+YNKIu94ansU8gRoRvdsbVlvPTOlqR5jhtUnjCvvl9JTIuijtY5c9YQhlb1Yaj/q17wGY4e2N7C6LgJtUD06HjhY0p83mMHlZGfZwl9SC0Y3Z/PHNXIrU+8HnOsCx9r8iz8eFp0n1DHjR9IYa88PjS+Nun7nT868aLz2HED+OUL77S9ntHQjzq/JY/DsSDFRX7vXvkx25s2tB//XLs5aXoz47gJtfz2pffa5oX3t3hRF0phh8ctnzjYq2PDk/xKD15rtcMb+0eOVlZXUZyylWB+nrFkolfnUx1SC3vlcby/nwT5lxUXcNvZ09vSnDK9PnkGIcFxI7jojnp8Lr6MUSbUJdaPcH7hbIPHDoNH+4LvrS2vwdF5ZSKol0EAdOrQSh4N7RdhkYE6v7x9CvNZNG4gD/09+pi4PweUqC7tzWGN7TfQC0bXtB1fw5/vUWOS35TlhfaxQG15UeRNHngtQYLrnwUR9TtQU1bEx+c0dPQWAJibpE6m2Q1aSmVFvdjSuiftH5OSHcOTrZ/qfF1S2Itjxw1sCwzHmzqkMuYGuzMakrS0GD0wdbA5U0HwvaPAVmVJYYfH0Uylyi/5gD/G4omx56aasiJq4s776X7+Bfl5CfllInyujj8vto0U2unco3nX2J0LmnUkVXA1Svh4nq6igvzI7SQ7nplZ23mwM7ztJa6f6vyeysKmjoNhB4v2w0JihTx2XPS+Ehw7C3vlpdWCONn96aFOj+PlKCP6wqKzj7Ml5h+bT7JrlWTbS6eD03BLkeDiIDyvJ0ZYg/T7ucp28YIRVdL5ToMboSI/aNNdvyLG/3IYtR+Ef+lt//W3a9sNmn1n0pF1Z4dVTn4hnjg/fhupO+OOnr8/+tjIRoOGPIu90Mykw92o0fE6EtWCIJ1fqL0+obz/4wchbL8ZSbsYCXkn6vyHnSvNuC3ub8brp3mj0l1Do7d1pB43P36XSn7+ihbVSjY4Tid7tKs77fEj/smC88mkPzphz+6P3fX9Hwi6MjpeWx5Z/j5y5PDTZW2P4x1gA4L0dH1LV26UUmT/CdeJHKnGByQFoXJM+AY26sIyk5Naqhug+EVJh19P+ktOx9sP3wAEwYZwAKinTtDp9hOSjfKFswwevUhnGPEgCFXs95kSfJ5dL2LHF22xgYXObDBxncLOBKHafnnO7EIzk88osa+OuJYWoU3Hd7yf7rDx3SEbeSa0KslgI0G9zuT7TNYZZHurnOTHpKiWV+F1OhukTbWvdCbLdALMmY4omI3vvqu/dmd6HOrqe0g3mJFsNNZ0z3fOhVtCZf9yKthW7x7u6DxbDqaL+e58HK+zOjoX5koQpKvSfRxvf+upH1ozdYjsJjlDX0fPix0dT99IZykIlaPMonf8TE5qqdLGL8u0T6h0ihEuf9BiIT+mYqfedrak25osSNUd5Yu6Tm3raySNG47d/i/kQZ9Qe7rpaiu+XFE3d+HWJm0X3l3cbke/9Ee2kgnKk+HGMw2wphITJO7gawuyz8Z1cTbyNIv93BNayaVYN78TwZ9kSTsKMFioJVR8Hu0d1KZdjLj1I1rHdWGfT+fYkWnALCvffZB3JzNPtyqFO/zuiqjHByHi8cwkdTT5jyvegva669qOven8WNBVbUGogs5durWXv+utdLLpYGgR1R2PKYVHM+7c+n5ZDoLPsyu6+uNDtuTKzWvKUSD3YzlEsqkrP0hL5ygIlWM6Oodm8jheypZQCY8dRadN+jhDhjU0GLo9piVU3LCw+0u6mwvSFaT5q3Q6lz/hz33Pvsxvbor8IFTQ7LyrF5+JLUlSp2kfwjfzFkxhnXm0JVngoasyyS687Y5a1HXXDXd0Obo/z47610m17WAXzrRVTzwv+O7/nyJd8j6hMt8/Y7afal64v6s080vn0d+Mg1BZjUJlIe8sbibh+BeXcbKWUB2dv8LHmqDFarrnga7YFTyOl+G2EkbB66YfC7LmILrA70p97Gh0u3S3fajfMKXbKu2ArQ854ACL74mkrUu7rvb7LlEQKkeFH0sJyyRgkyppR6042reXJO+0S+EJOt8O5xfcQKbbR1N3STeQl+2+QIKbm0wCi8HnuLebrgjitx1/MZxnsa2WuqtPqM482mIRgYd0Hs1Llqare12y/TZVJ4jdJRsXhPl5FvNJJVzQpzigBJ9FVwPK6d7oJ2vx1N5aLv0PKGhdCNE3g11rCdXx59HVwF13SBbUy1RHN4Ftdbj7olCx249b3NlTS3g/CM4DUQNIdLegJVSmfUIFciUWkSvlTKWtX7puGR1PuqK9JW4PF+QgdKgHOOXgkcmurP2+e+QvX768p8uw39x+++3LL7zwwp4uRpfs3rePp1/fyGGN1SyZOIjv/eF1zpo9pG0kg2lDK3lw5RpOnV4fMxzrvn2O8+Y1xIycY2ZsaNnJNYvHMqa2jIaqkrbRnSbWlfPW+9s5d24D5cUFLJ5YS2GvPE6dXk9TbfvoVyWFvfjBU6uoqyzm8MZqLjhsOP3LenPUmJq2C/XRA/oysqaUo8cM4IRJg/j3+hbqK0s4Y9aQtnyG9OtDc8tOvnbyBF56ZwuDK4tZfsI4trTu4bolYxlUUczk+goefm4d1584jtED+nLpgpEMixhdpbggn49Mq6O6tDfXLhlLXWUxlSWFPPFqM3eeN4OG6j5UlhRy8+9e48RJg5hUX44ZXLFoNKW9e3HajHpmNPSjuWUn581rSPpreWF+Huu3tnLNkrHUlhdTUpjPqAF9OXJUf6YP7UdZcUHM6B5ja8vaRk47qqmG02bUs3RKHWMHlVHfr4TXm1u4ZskYyvyR3yYMLuft97e3jTRYkJ/Hx2bU0+SPPjZpcAWvvruVpZPruGJRE32LevGZoxp5+/0dfGnxGN7b0sq1S8a2DR1d2rsXzS27uGLRqITR1gLjBpWzeuM2PnfsaIoK8hlUXsyaTTsYXt2H1Ru387Wl46mvLOEP/2qmoaqEq48fw5QhFdy/Yg2fOmI4X1g0mp179vGRqYPZ0LKT604YmzCCVm15EbOHV3HS5DqmDKlk2cJGdu3Zx7oPdjB7eBUnT/GGfN68YzdXfmg0J0waxB1PvcGw6j5cfVwTeWZcfswoxvojPy47aiRbW/dw9XFNvNG8jS8uHtM2suKkwRW89f52LjuqsS2A0SvP+NiMetZvaaUgP4/PHj2KzTt2s2ef46ZTJ/PR6YMpLy7k0oUjeXPjdi4/elTbTV9+nrF+aytHNPZnRkM/5o2sZvOO3Xzh2NEMqSqhurSQB/62hpade7j3wtlUl/bmmVXvc8zYAXx16Xi2tO7mzFlDyMszpgyp4N3NrXzqyBFtQbch/UqYXF/JjE4OjdtY05emgX1ZtrCRh1au4ROHDWfcoDKuOq6JyUMqKSnIZ97I6pj9clh1H8YPKmPmsKqE/MYPKucXz69jfF0ZV32oiZqy3jz7xvsUFeTztQ9PiBmVbkZDPzZt383Zc4Yyc1g/KksKOdIfQbKqT29eW9/CxQtGxAxlXtq7FydO9upAvHkjqynMz+P8wxoYXFlCWVEByxY2UlteRHPLTq71jwuBmcP6Udq7FwuaaqgoKWTN+9u5ZslY5oyoYmyt9/6GVfdhlb+PlPZOb3DYkTV92NK6h8aavly7ZCxHju4f897G15Xz5sbtMSOCzhrej7uf8UaV/NLxYzh+Qm3kiGtlRQWs3bSDBaP7c3hjf2YNT/wOpgyp4I4/rea2s6cxsiZ2pBvnvPc9d0T7CExD/GNJQ1UJG1p28pPzZ7K1dTfLFjamDF5Mrq+gpqyobYTUsLkjqtjnHEsmDupUq4zC/DzveHSCd6xMZtygct7cuI3PHjMq5eiqLa17+MzRo2ioih5dK6in1y0ZF7OPDCwrYtWGbXzysOEMLC9iYYqR197ftov/OG4MS6cM4uV3tnDS5Lq2kdXG1paxeoN3nDxiVH9ad+/jlOn1GQVZZw6rYseuvZwyvZ4Zw/rx/rZdnDO3gfw8Y3J9BWs37eDjc4cyd0Q104Z6o2A1DujL6g3buOpDTWzbuSfy+BqlMD+Pdze3co3/+Q/t14fX17fwpcVjKCsqYNawKvr0zo8Z3fHs2Q2MGpB8lLLtu/aybOHIpCOcdUbwvV0b972lY0T/Ul5vbuHq48bQJ826nY59Ds4/bFhC3euId1zw9uVxg8oYPaCM2X79nji4nNry4qQjp5X27sWaTTv46knj2bfPxdSbyfUV9O/bO2Y01MGVJUxvqGTa0Njzxoxh/WLOOYHZw6soKcxPOZpnoK6ymFUbWrhm8diYUWkzVVdRzAy/jMP7lzK2tizyeNcZUZ9JWNPAMt7Y0MKVi5ra+s0EbzS8o8cM4MhR/Xni1Wbu+sRMKvsUdqksTQP7sqp5G1csGh2zrbCo+tZdpjdUxhxLumL28CqK/ZF8k25vaCUb09heY01fRg0obasDB5o8M86cPSTlMa+nzRlRxZ69jhMmde48HNa3qIDjxw9Mei+QC4ZWlTCpvqLT18uBEf1L+ff6Fr54fPrnjnwz1m/dyXUnjGFQRTEfbN/NFYtGd35k8oPM9ddf/87y5ctv7yid9WSnifvb9OnT3YoVK3q6GCIiIiIiIiIiBw0z+5tzbnpH6fQ4noiIiIiIiIiIZJ2CUCIiIiIiIiIiknUKQomIiIiIiIiISNYpCCUiIiIiIiIiIlmnIJSIiIiIiIiIiGSdglAiIiIiIiIiIpJ1CkKJiIiIiIiIiEjW5VQQyszyzOyzZvaKmbWa2dtmdqOZ9enpsomIiIiIiIiISHI5FYQCbgK+CbwELAMeAC4DHjGzXHsvIiIiIiIiIiKHjF49XYB0mdk4vMDTg865j4TmvwHcDJwG/LSHiiciIiIiIiIiIinkUuuh0wEDvhU3//vAduCs/V4iERERERERERFJSy4FoWYA+4BnwzOdc63Ac/5yERERERERERE5AOVSEGoQsME5tzNi2Vqg2swK93OZREREREREREQkDbkUhCoBogJQAK2hNDHM7EIzW2FmK5qbm7NWOBERERERERERSS6XglDbgd5JlhWF0sRwzt3unJvunJvev3//rBVORERERERERESSy6Ug1Dq8R+6iAlF1eI/q7drPZRIRERERERERkTTkUhDqr3jlnRmeaWZFwGRgRU8USkREREREREREOpZLQaj7AAdcHjf/k3h9Qd2z30skIiIiIiIiIiJpMedcT5chbWZ2C3Ap8BDwK2AMcBnwJ2Chc25fB+s3A29mu5z7STWwoacLIZIDVFdE0qO6IpIe1RWRjqmeiKTnYKorQ51zHXbEnWtBqHy8llAXAg14X9Z9wHXOuZYeLNp+Z2YrnHPTe7ocIgc61RWR9KiuiKRHdUWkY6onIuk5FOtKr54uQCacc3uBG/1JRERERERERERyRC71CSUiIiIiIiIiIjlKQajcdXtPF0AkR6iuiKRHdUUkPaorIh1TPRFJzyFXV3KqTygREREREREREclNagklIiIiIiIiIiJZpyCUiIiIiIiIiIhknYJQOcLM8szss2b2ipm1mtnbZnajmfXp6bKJdBczu9rMHjCzVWbmzGx1B+lnmdljZrbVzLaY2a/NbHKStIPM7Cdm1mxmO8xshZmdkiRtbzP7ipm9YWY7zex1M7vGzAq64W2KdImZjfL3z2f8/XmrmT1nZl+KOieY2Wgze9jMNpnZNjN70swWJsm73MxuMbO1/rnmRTP7tJlZRFqdl+SA5u/795jZy2a22cy2+/vrN82sNkl61RURwMxK/OsgZ2bfjliu+iKHJL9ORE0tEWlVTyL06ukCSNpuAi4DHgJuBMb4r6eY2dHOuX09WTiRbvKfwPvASqAiVUIzmw08AawFrvNnXwo8aWZznXP/CKXtBzwF1ADfBNYAZwD3m9n5zrkfxWV/H3AScAfwNDAH+CowEji3829PpFucD1wC/AK4B9gNLABuAE41s9nOuR0AZjYC+DOwB/g6sBn4JPAbMzvOOfdYkKmZFQKPAlOAW4CXgeOA7wIDgOVx5dB5SQ50g4FavH10DV49mABcCJxmZpOdc+tBdUUkwleA6qgFqi8iPElih+K7wy9UT1Jwzmk6wCdgHLAP+Fnc/GWAA87o6TJq0tQdEzA89P8/gdUp0j4LbAHqQvPq/Hm/jUv7db+unBCal+/nsREoDc0/3k97Y1weN/rz5/b056Tp0J6A6UB5xPwb/H300tC8+4G9wOTQvFLgTeBV/AFK/PkX++svi8v3Z8AuYGhons5LmnJ2Ak7x99MrQ/NUVzRp8idgKt6N8+f8/fTbcctVXzQdspO/L96ZRjrVkySTHsfLDacDBnwrbv73ge3AWfu9RCJZ4JxblU46MxsJzAAecM6tDa2/FngAONrMBoZWOQN43Tn3SCjtXrxfGvrhBZ7CaSGxvgWvVd+kRznnVjjnNkcsus//Ox7Ab5J9IvCEc+650PotwA+AUXj1KHAG3jnl+3H5fgsoAD4WmqfzkuSyN/2/laC6IhJmZvl4++evgQcjlqu+iOC1XjKz0iTLVE9SUBAqN8zAi3Y+G57pnGsFniN2BxY5FAT7/NMRy57BOzBPA/D7/ajz50elDecX/L/WOfd2OKH/eh2qb3LgGuz/fc//OxHoTfJ6Av7+bGZ5eL98/90/t4Q9i3cOiq8nOi9JTjCzIjOrNrPBZnYscJu/6Ff+X9UVkXafBZrwujiIovoiAh/FC/hsNbP1fl9O5aHlqicpKAiVGwYBG5xzOyOWrQWq/edIRQ4Vg/y/ayOWBfPqOpE2SB+VNkhfl2SZSI/xf7m+Du/xiZ/6szPZ9yuB4qi0/rlnI4n1ROclyRUXAM3A28Bv8PocPMs596S/XHVFBDCzYcD1wFecc6uTJFN9kUPds3h9NH0UOAd4nPZ+aYOWUaonKahj8txQAkTtZACtoTS79k9xRHpcif83ql60xqXJJG3wf6r6VpJkmUhP+hYwG/iic+5Vf1531ZMgfSb1JEij85IcCB4GXsHri2MK3iMS/UPLVVdEPLcCb+AN4pKM6osc0pxzs+Jm/cTMXgC+BnzG/6t6koJaQuWG7XjN+aIUhdKIHCqC/T2qXsTXiUzSBv+nqm+qa3JAMbOv4v0Cd7tz7v+EFnVXPQnSZ1JPwnmK9Cjn3Brn3GPOuYedc1/G++X6v83saj+J6ooc8szsLOBY4CLn3O4USVVfRBJ9Ay/os9h/rXqSgoJQuWEdXrO6qJ2tDq853gEb6RTJgnX+36hH44J5azuRNkif7JG7OpI/qiey35nZcuAa4EfARXGLM9n3NwE7otL6554qEuuJzkuSk5xzLwB/xxuNCFRX5BDn75/fxOsn7V0zG+kPAjPUT1Luz6tA9UUkgR+4XQdU+7NUT1JQECo3/BXvu5oZnmlmRcBkYEVPFEqkB/3V/zsnYtlsvOFJ/wbgnHsH78A9O0laiK1DfwXqzKw+nNB/PQjVNzlAmNmXgS8DPwEucP74vCH/wGuunayegL8/O+f2ASuBKREXNTPxzkHx9UTnJcllxXijo4Lqikgx3iOqi4HXQtMT/vKz/NcXoPoiksDfRwfTPjiM6kkKCkLlhvvwbqovj5v/SbznPe/Z7yUS6UHOuX/jHWBPMbOg4z/8/08BHnfOvRta5V5ghJmdEEqbDywDPqB9hKQgLSTWt+C16pv0ODO7Dq9TzLuA8/wLmBj+MMCPAPPNbFJo3VK8G4nXiB1Z5V68c8qFcVldjtfh+f2heTovyQHPzAYmmb8AGI8/QpHqigjb8K6f4qegteCv/de/UH2RQ5mZVSVZ9FW8/rYfAZ1XOmKJP5zKgcjMbsHr8+MhvBvmMcBlwJ+AhVE3ICK5xszOpr3p9zKgELjRf/2mc+6uUNq5wO+BNcAtoXUGAPOcc8+H0lbhtYyqwmtuvhY4HZiP14Lkh3HleARYAvwQb2jVOcAngLudc2d309sV6RQzuwT4NvAWcC3eML1h7znnHvXTjsS7yNkN3ARswbtImQAsds79JpRvIfBnYBJwM/AycDxwMnCDc+7auHLovCQHNDN7CKjFG7noTby+MqYBp+H1lzHfOfecn1Z1RSSOmTXgdVT+HefcpaH5qi9ySDKzm/BaMv0e7zqsFG9/XgD8BVjgnNvhp1U9ScY5pykHJiAf+DzwKl7TvrV4N9OlPV02TZq6a8Jr9u2STE9EpJ8D/A5oAbbiDb09NUnedXitRjbgjRyxEvhYkrRFwA3Aar++rcK72S/o6c9IkybgzhT1JKGu4F2Y/Byv1d924Cng6CR5V+AFuNb5+/5LeBc5FpFW5yVNB/QEnAr8L/C2f9zfgTdK3i3AkIj0qiuaNIUmoME/r3w7Ypnqi6ZDbgJO8u831vrnlW3Ac8AXgaKI9KonEZNaQomIiIiIiIiISNapTygREREREREREck6BaFERERERERERCTrFIQSEREREREREZGsUxBKRERERERERESyTkEoERERERERERHJOgWhREREREREREQk6xSEEhERERERERGRrFMQSkRERCQLzGy5mTkza+ih7d9pZq4nti0iIiISRUEoEREROaSZ2Xw/WJRs2tPTZRQRERE5GPTq6QKIiIiIHCDuBX4VMX9fJ/O7AfgvYGenSyQiIiJyEFEQSkRERMSz0jl3d3dl5pzbA6gVlYiIiIhPj+OJiIiIpMHMGvzH85ab2elm9oKZtZrZW/68XnHpE/qEMrN+ZnaTmb3ur7vRzP5msP43PgAABPlJREFUZlfErdvLzK4ys5dC6R4yswkR5Soys2+Y2Toz22Fmz5rZsSneR6OZ3WVm75jZLjNb7a/fJy5dvZndYWZvmtlOM1tvZn82s3M6+xmKiIjIoU0toUREREQ8JWZWHTF/l3NuS+j1CcDlwHeAd4ETgS8DQ4HzOtjGA8ARwG3A80AJ0ATMB74RSncPcCrwKHArMBC4BHjazA53zv09lPZeYCnwCPAbYATwIPBG/MbNbBrwOPCBX4a1wCTgMmCemR3pnNvtB9QeBeqA7wL/AsqBicDhwI87eJ8iIiIiCcw5DZoiIiIihy4zmw/8PkWS/3XOLfFbNL2B10fUDOfcSn99wwv6LAXmOOee8ecvxwtODXPOrTazcrzgz63OuYtTlOcY4LfA/cBpzr9YM7OJwErgaefc4f68Y/ECTz92zp0bymMp8BCAc85C858Hevvl3xqaf7L/Hs5zzt3pb+t54Crn3NdTfDYiIiIiadPjeCIiIiKe24FjIqYvxaV7NAhAAfhBoiBQc3KK/HfgdVI+K/yIXoQgj6+50K+FzrkXgF8Ch5lZf3/2Uv9vuBUVzrmHgVfD8/xH+SYCPwV6m1l1MAFPAduA4DG+zf7fBWZWk6KsIiIiImlTEEpERETE85pz7rGI6fm4dC9HrPuS/3d4ssydc7vwHuMbD7xhZi+a2S1mdlRc0mF4ra2itvPPUJpge/vwHpeLF7/+GP/v9UBz3LQe6AMM8Mv6JvA1vKDUO36/VV83sxnJ3p+IiIhIR9QnlIiIiEhmOt2XgXPue2b2c2AxcCTwUeBSM7vPOXean8ySZpAoVdr4ZcHrG4FfJ1lnU6is15jZHX5ZDwcuAK4ws687567KoIwiIiIigFpCiYiIiGRqbIp5qzpa2Tn3jnPuB865s4HBeB2LfyzUyuh1vGu0MRGrB9t5Iy7tqIi0TXGvX/P/7k3S4usx59zf4sq6yjl3i3PuVGAQ8EfgSj2iJyIiIp2hIJSIiIhIZo4xs6nBC79j8iv9lw8nW8nMSsysJDzPObcXeMF/2S8uj6v9vIP1x+ONxPeUc67Zn/1z/+8VcdtaCoyOK8Lf8R7nu8jMEh4bNLNeZtbP/7/czAriytpK+yN+lcnep4iIiEgyehxPRERExDPVzM5KsiwcXHoeeNzMvgO8A5wEHA3c5Zx7OkX+o4A/mNlDeMGgTXitnT6N17LpSQDn3KNmdj9wGlBpZr8EBgKXAK3AZUGGzrnfmNkjwDl+AOnXwAjgU/42xofSOjM7G3gceMF/1O5FoAQYCXwYuBq4E1gA3G5mP8Pr4LwFmIb3SN5fnHMxnZ6LiIiIpENBKBERERHP6f4UpRHY4///C7zAzNV4rY3WA1/1p1TeBu7AC/AsBXoDa4HvA//tnNseSnsmsBI4F68Pp23AH4BrnXP/iMv3Y8AN/jrH4AWfPuK/l/HhhM6558xsil/2E4GLgK3Aarzg0+/8pM8DDwLz/XzzgbeA//TLIyIiIpIxC438KyIiIiJJmFkDXoul651zy3u0MCIiIiI5SH1CiYiIiIiIiIhI1ikIJSIiIiIiIiIiWacglIiIiIiIiIiIZJ36hBIRERERERERkaxTSygREREREREREck6BaFERERERERERCTrFIQSEREREREREZGsUxBKRERERERERESyTkEoERERERERERHJOgWhREREREREREQk6/4/GensQIaM9DQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 1440x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot number steps per episode\n",
    "plt.figure(figsize=(20, 8))\n",
    "plt.plot(steps_all_episodes)\n",
    "plt.title('Number of Steps per Episode')\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Steps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1332x756 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot average step vs reward for Q-learning agent\n",
    "\n",
    "step_count = 0\n",
    "step_graph = []\n",
    "for s in steps_per_hundred_episodes:\n",
    "    step_graph.append(sum(s/100))\n",
    "    step_count +=100\n",
    "    \n",
    "reward_count = 0\n",
    "reward_graph = []\n",
    "for r in rewards_per_hundred_episodes:\n",
    "    reward_graph.append(sum(r/100))\n",
    "    reward_count +=100\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "color = 'tab:orange'\n",
    "ax1.plot(step_graph, color=color)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "ax1.set_ylabel('Average steps per 100 episodes')\n",
    "ax1.set_xlabel('Episodes (in hundreds)')\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "color = 'tab:blue'\n",
    "ax2.plot(reward_graph, color=color)\n",
    "ax2.set_ylabel('Average reward per 100 episodes')\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.set_size_inches(18.5, 10.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 1, 4)              64        \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 4)                 0         \n",
      "=================================================================\n",
      "Total params: 64\n",
      "Trainable params: 64\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Training for 10000 steps ...\n",
      "  100/10000: episode: 1, duration: 0.116s, episode steps: 100, steps per second: 862, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.290 [0.000, 3.000], mean observation: 0.210 [0.000, 4.000], loss: --, mean_q: --\n",
      "  200/10000: episode: 2, duration: 0.031s, episode steps: 100, steps per second: 3251, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.150 [0.000, 3.000], mean observation: 0.050 [0.000, 4.000], loss: --, mean_q: --\n",
      "  300/10000: episode: 3, duration: 0.029s, episode steps: 100, steps per second: 3489, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.350 [0.000, 3.000], mean observation: 0.230 [0.000, 4.000], loss: --, mean_q: --\n",
      "  400/10000: episode: 4, duration: 0.029s, episode steps: 100, steps per second: 3474, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.320 [0.000, 3.000], mean observation: 0.230 [0.000, 4.000], loss: --, mean_q: --\n",
      "  500/10000: episode: 5, duration: 0.031s, episode steps: 100, steps per second: 3226, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.120 [0.000, 2.000], mean observation: 0.080 [0.000, 2.000], loss: --, mean_q: --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eijaz/anaconda3/lib/python3.6/site-packages/rl/memory.py:39: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  548/10000: episode: 6, duration: 0.926s, episode steps: 48, steps per second: 52, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.854 [0.000, 3.000], mean observation: 0.854 [0.000, 5.000], loss: 0.000053, mean_q: 0.020249\n",
      "  582/10000: episode: 7, duration: 0.268s, episode steps: 34, steps per second: 127, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 2.382 [0.000, 3.000], mean observation: 2.206 [0.000, 7.000], loss: 0.000004, mean_q: 0.034039\n",
      "  617/10000: episode: 8, duration: 0.280s, episode steps: 35, steps per second: 125, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.914 [0.000, 3.000], mean observation: 2.200 [0.000, 5.000], loss: 0.000002, mean_q: 0.031798\n",
      "  676/10000: episode: 9, duration: 0.459s, episode steps: 59, steps per second: 129, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.458 [0.000, 2.000], mean observation: 6.847 [4.000, 12.000], loss: 0.000002, mean_q: 0.027068\n",
      "  700/10000: episode: 10, duration: 0.190s, episode steps: 24, steps per second: 126, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.125 [0.000, 1.000], mean observation: 8.000 [4.000, 12.000], loss: 0.000002, mean_q: 0.026691\n",
      "  714/10000: episode: 11, duration: 0.116s, episode steps: 14, steps per second: 121, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.214 [0.000, 1.000], mean observation: 8.000 [4.000, 12.000], loss: 0.000002, mean_q: 0.025796\n",
      "  814/10000: episode: 12, duration: 0.771s, episode steps: 100, steps per second: 130, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.120 [0.000, 3.000], mean observation: 12.310 [0.000, 14.000], loss: 0.000002, mean_q: 0.027878\n",
      "  849/10000: episode: 13, duration: 0.278s, episode steps: 35, steps per second: 126, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.400 [0.000, 3.000], mean observation: 10.943 [1.000, 14.000], loss: 0.000002, mean_q: 0.030065\n",
      "  868/10000: episode: 14, duration: 0.152s, episode steps: 19, steps per second: 125, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 2.263 [0.000, 3.000], mean observation: 3.789 [1.000, 12.000], loss: 0.000002, mean_q: 0.030686\n",
      "  894/10000: episode: 15, duration: 0.303s, episode steps: 26, steps per second: 86, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.962 [0.000, 3.000], mean observation: 6.308 [1.000, 12.000], loss: 0.000002, mean_q: 0.031039\n",
      "  935/10000: episode: 16, duration: 0.328s, episode steps: 41, steps per second: 125, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 2.854 [1.000, 3.000], mean observation: 2.756 [1.000, 7.000], loss: 0.000003, mean_q: 0.031112\n",
      "  960/10000: episode: 17, duration: 0.199s, episode steps: 25, steps per second: 125, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.280 [0.000, 3.000], mean observation: 7.840 [4.000, 12.000], loss: 0.000004, mean_q: 0.031075\n",
      " 1029/10000: episode: 18, duration: 0.591s, episode steps: 69, steps per second: 117, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 2.739 [0.000, 3.000], mean observation: 2.710 [0.000, 10.000], loss: 0.000003, mean_q: 0.030955\n",
      " 1101/10000: episode: 19, duration: 0.830s, episode steps: 72, steps per second: 87, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 2.486 [0.000, 3.000], mean observation: 2.653 [1.000, 7.000], loss: 0.000003, mean_q: 0.030576\n",
      " 1123/10000: episode: 20, duration: 0.313s, episode steps: 22, steps per second: 70, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 2.227 [0.000, 3.000], mean observation: 3.682 [0.000, 13.000], loss: 0.000003, mean_q: 0.030240\n",
      " 1223/10000: episode: 21, duration: 0.860s, episode steps: 100, steps per second: 116, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.740 [0.000, 3.000], mean observation: 5.370 [1.000, 10.000], loss: 0.000002, mean_q: 0.030175\n",
      " 1298/10000: episode: 22, duration: 0.585s, episode steps: 75, steps per second: 128, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 2.293 [0.000, 3.000], mean observation: 2.760 [0.000, 10.000], loss: 0.000003, mean_q: 0.029977\n",
      " 1326/10000: episode: 23, duration: 0.234s, episode steps: 28, steps per second: 120, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 2.643 [1.000, 3.000], mean observation: 2.464 [1.000, 7.000], loss: 0.000002, mean_q: 0.029774\n",
      " 1414/10000: episode: 24, duration: 0.683s, episode steps: 88, steps per second: 129, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.477 [0.000, 3.000], mean observation: 2.898 [0.000, 10.000], loss: 0.000002, mean_q: 0.030216\n",
      " 1444/10000: episode: 25, duration: 0.249s, episode steps: 30, steps per second: 121, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.300 [0.000, 3.000], mean observation: 1.567 [1.000, 5.000], loss: 0.000002, mean_q: 0.030169\n",
      " 1544/10000: episode: 26, duration: 0.875s, episode steps: 100, steps per second: 114, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 2.150 [0.000, 3.000], mean observation: 0.580 [0.000, 4.000], loss: 0.000002, mean_q: 0.029972\n",
      " 1644/10000: episode: 27, duration: 0.784s, episode steps: 100, steps per second: 127, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.210 [0.000, 3.000], mean observation: 0.470 [0.000, 4.000], loss: 0.000002, mean_q: 0.029709\n",
      " 1666/10000: episode: 28, duration: 0.175s, episode steps: 22, steps per second: 126, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.591 [0.000, 3.000], mean observation: 2.273 [0.000, 5.000], loss: 0.000002, mean_q: 0.029505\n",
      " 1724/10000: episode: 29, duration: 0.521s, episode steps: 58, steps per second: 111, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.431 [0.000, 3.000], mean observation: 4.431 [0.000, 12.000], loss: 0.000002, mean_q: 0.029372\n",
      " 1761/10000: episode: 30, duration: 0.293s, episode steps: 37, steps per second: 126, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.595 [0.000, 3.000], mean observation: 2.405 [0.000, 8.000], loss: 0.000002, mean_q: 0.029242\n",
      " 1861/10000: episode: 31, duration: 0.775s, episode steps: 100, steps per second: 129, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 2.040 [0.000, 3.000], mean observation: 1.890 [0.000, 9.000], loss: 0.000002, mean_q: 0.029044\n",
      " 1865/10000: episode: 32, duration: 0.038s, episode steps: 4, steps per second: 106, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.750 [0.000, 2.000], mean observation: 4.250 [4.000, 5.000], loss: 0.000003, mean_q: 0.028873\n",
      " 1965/10000: episode: 33, duration: 0.769s, episode steps: 100, steps per second: 130, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.650 [0.000, 3.000], mean observation: 4.450 [0.000, 9.000], loss: 0.000002, mean_q: 0.028741\n",
      " 2065/10000: episode: 34, duration: 0.826s, episode steps: 100, steps per second: 121, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.270 [0.000, 3.000], mean observation: 4.970 [0.000, 14.000], loss: 0.000002, mean_q: 0.028479\n",
      " 2165/10000: episode: 35, duration: 0.773s, episode steps: 100, steps per second: 129, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.600 [0.000, 3.000], mean observation: 6.170 [0.000, 9.000], loss: 0.000002, mean_q: 0.028186\n",
      " 2180/10000: episode: 36, duration: 0.124s, episode steps: 15, steps per second: 121, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.667 [0.000, 2.000], mean observation: 2.933 [1.000, 7.000], loss: 0.000003, mean_q: 0.027999\n",
      " 2262/10000: episode: 37, duration: 0.695s, episode steps: 82, steps per second: 118, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.317 [0.000, 3.000], mean observation: 6.037 [0.000, 14.000], loss: 0.000003, mean_q: 0.027833\n",
      " 2280/10000: episode: 38, duration: 0.144s, episode steps: 18, steps per second: 125, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.833 [0.000, 2.000], mean observation: 2.889 [0.000, 7.000], loss: 0.000003, mean_q: 0.027619\n",
      " 2369/10000: episode: 39, duration: 0.721s, episode steps: 89, steps per second: 123, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.809 [0.000, 3.000], mean observation: 2.809 [1.000, 7.000], loss: 0.000002, mean_q: 0.027549\n",
      " 2384/10000: episode: 40, duration: 0.122s, episode steps: 15, steps per second: 123, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 2.200 [0.000, 3.000], mean observation: 2.200 [0.000, 7.000], loss: 0.000001, mean_q: 0.027372\n",
      " 2445/10000: episode: 41, duration: 0.479s, episode steps: 61, steps per second: 127, episode reward: 1.000, mean reward: 0.016 [0.000, 1.000], mean action: 1.148 [0.000, 3.000], mean observation: 5.393 [0.000, 15.000], loss: 0.000002, mean_q: 0.027262\n",
      " 2467/10000: episode: 42, duration: 0.181s, episode steps: 22, steps per second: 122, episode reward: 1.000, mean reward: 0.045 [0.000, 1.000], mean action: 1.455 [0.000, 3.000], mean observation: 4.318 [0.000, 15.000], loss: 0.000243, mean_q: 0.027789\n",
      " 2515/10000: episode: 43, duration: 0.375s, episode steps: 48, steps per second: 128, episode reward: 1.000, mean reward: 0.021 [0.000, 1.000], mean action: 1.458 [0.000, 3.000], mean observation: 2.917 [1.000, 15.000], loss: 0.000305, mean_q: 0.031158\n",
      " 2525/10000: episode: 44, duration: 0.084s, episode steps: 10, steps per second: 119, episode reward: 1.000, mean reward: 0.100 [0.000, 1.000], mean action: 1.500 [0.000, 3.000], mean observation: 6.900 [1.000, 15.000], loss: 0.000395, mean_q: 0.034306\n",
      " 2531/10000: episode: 45, duration: 0.053s, episode steps: 6, steps per second: 113, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 10.500 [4.000, 15.000], loss: 0.000268, mean_q: 0.036369\n",
      " 2548/10000: episode: 46, duration: 0.143s, episode steps: 17, steps per second: 119, episode reward: 1.000, mean reward: 0.059 [0.000, 1.000], mean action: 1.412 [0.000, 3.000], mean observation: 2.824 [0.000, 15.000], loss: 0.000573, mean_q: 0.037730\n",
      " 2554/10000: episode: 47, duration: 0.055s, episode steps: 6, steps per second: 109, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000861, mean_q: 0.042103\n",
      " 2560/10000: episode: 48, duration: 0.056s, episode steps: 6, steps per second: 107, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000657, mean_q: 0.044036\n",
      " 2566/10000: episode: 49, duration: 0.053s, episode steps: 6, steps per second: 112, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000666, mean_q: 0.047346\n",
      " 2572/10000: episode: 50, duration: 0.055s, episode steps: 6, steps per second: 109, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000775, mean_q: 0.050766\n",
      " 2578/10000: episode: 51, duration: 0.054s, episode steps: 6, steps per second: 112, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000657, mean_q: 0.052428\n",
      " 2585/10000: episode: 52, duration: 0.062s, episode steps: 7, steps per second: 114, episode reward: 1.000, mean reward: 0.143 [0.000, 1.000], mean action: 1.714 [1.000, 3.000], mean observation: 7.000 [1.000, 15.000], loss: 0.000513, mean_q: 0.052412\n",
      " 2592/10000: episode: 53, duration: 0.061s, episode steps: 7, steps per second: 115, episode reward: 1.000, mean reward: 0.143 [0.000, 1.000], mean action: 1.286 [0.000, 2.000], mean observation: 6.857 [0.000, 15.000], loss: 0.000467, mean_q: 0.057392\n",
      " 2598/10000: episode: 54, duration: 0.055s, episode steps: 6, steps per second: 110, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000738, mean_q: 0.062082\n",
      " 2605/10000: episode: 55, duration: 0.061s, episode steps: 7, steps per second: 114, episode reward: 1.000, mean reward: 0.143 [0.000, 1.000], mean action: 1.429 [1.000, 2.000], mean observation: 11.000 [4.000, 15.000], loss: 0.000515, mean_q: 0.064665\n",
      " 2613/10000: episode: 56, duration: 0.069s, episode steps: 8, steps per second: 116, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.375 [0.000, 2.000], mean observation: 9.375 [1.000, 15.000], loss: 0.000549, mean_q: 0.071210\n",
      " 2619/10000: episode: 57, duration: 0.053s, episode steps: 6, steps per second: 112, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000545, mean_q: 0.075247\n",
      " 2627/10000: episode: 58, duration: 0.129s, episode steps: 8, steps per second: 62, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.375 [0.000, 2.000], mean observation: 11.250 [4.000, 15.000], loss: 0.000486, mean_q: 0.080095\n",
      " 2634/10000: episode: 59, duration: 0.062s, episode steps: 7, steps per second: 113, episode reward: 1.000, mean reward: 0.143 [0.000, 1.000], mean action: 1.286 [0.000, 2.000], mean observation: 10.143 [4.000, 15.000], loss: 0.000559, mean_q: 0.087874\n",
      " 2638/10000: episode: 60, duration: 0.038s, episode steps: 4, steps per second: 104, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.750 [1.000, 3.000], mean observation: 6.500 [4.000, 9.000], loss: 0.000353, mean_q: 0.085352\n",
      " 2646/10000: episode: 61, duration: 0.070s, episode steps: 8, steps per second: 115, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.625 [1.000, 3.000], mean observation: 7.000 [1.000, 15.000], loss: 0.000453, mean_q: 0.096346\n",
      " 2652/10000: episode: 62, duration: 0.056s, episode steps: 6, steps per second: 108, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000417, mean_q: 0.100005\n",
      " 2660/10000: episode: 63, duration: 0.070s, episode steps: 8, steps per second: 114, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.375 [0.000, 2.000], mean observation: 11.250 [4.000, 15.000], loss: 0.000445, mean_q: 0.108189\n",
      " 2666/10000: episode: 64, duration: 0.053s, episode steps: 6, steps per second: 112, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 10.500 [4.000, 15.000], loss: 0.000273, mean_q: 0.116120\n",
      " 2672/10000: episode: 65, duration: 0.055s, episode steps: 6, steps per second: 108, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 10.500 [4.000, 15.000], loss: 0.000296, mean_q: 0.123202\n",
      " 2678/10000: episode: 66, duration: 0.056s, episode steps: 6, steps per second: 108, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000294, mean_q: 0.126472\n",
      " 2684/10000: episode: 67, duration: 0.054s, episode steps: 6, steps per second: 111, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 10.500 [4.000, 15.000], loss: 0.000188, mean_q: 0.128594\n",
      " 2691/10000: episode: 68, duration: 0.061s, episode steps: 7, steps per second: 114, episode reward: 1.000, mean reward: 0.143 [0.000, 1.000], mean action: 1.286 [0.000, 2.000], mean observation: 9.571 [4.000, 15.000], loss: 0.000277, mean_q: 0.144430\n",
      " 2697/10000: episode: 69, duration: 0.054s, episode steps: 6, steps per second: 112, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 10.500 [4.000, 15.000], loss: 0.000260, mean_q: 0.146660\n",
      " 2703/10000: episode: 70, duration: 0.055s, episode steps: 6, steps per second: 109, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000172, mean_q: 0.154149\n",
      " 2709/10000: episode: 71, duration: 0.054s, episode steps: 6, steps per second: 111, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000241, mean_q: 0.163804\n",
      " 2711/10000: episode: 72, duration: 0.023s, episode steps: 2, steps per second: 86, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.500 [1.000, 2.000], mean observation: 3.000 [1.000, 5.000], loss: 0.000118, mean_q: 0.170932\n",
      " 2718/10000: episode: 73, duration: 0.061s, episode steps: 7, steps per second: 115, episode reward: 1.000, mean reward: 0.143 [0.000, 1.000], mean action: 1.714 [1.000, 3.000], mean observation: 7.143 [1.000, 15.000], loss: 0.000218, mean_q: 0.172963\n",
      " 2726/10000: episode: 74, duration: 0.070s, episode steps: 8, steps per second: 115, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.375 [0.000, 2.000], mean observation: 6.625 [1.000, 15.000], loss: 0.000159, mean_q: 0.183428\n",
      " 2732/10000: episode: 75, duration: 0.054s, episode steps: 6, steps per second: 111, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.833 [1.000, 3.000], mean observation: 5.000 [0.000, 11.000], loss: 0.000186, mean_q: 0.195676\n",
      " 2738/10000: episode: 76, duration: 0.053s, episode steps: 6, steps per second: 114, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000152, mean_q: 0.200532\n",
      " 2744/10000: episode: 77, duration: 0.053s, episode steps: 6, steps per second: 113, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000168, mean_q: 0.216839\n",
      " 2750/10000: episode: 78, duration: 0.055s, episode steps: 6, steps per second: 109, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000122, mean_q: 0.218005\n",
      " 2756/10000: episode: 79, duration: 0.055s, episode steps: 6, steps per second: 108, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000155, mean_q: 0.230616\n",
      " 2762/10000: episode: 80, duration: 0.054s, episode steps: 6, steps per second: 111, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000104, mean_q: 0.236570\n",
      " 2768/10000: episode: 81, duration: 0.055s, episode steps: 6, steps per second: 110, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000089, mean_q: 0.246964\n",
      " 2774/10000: episode: 82, duration: 0.055s, episode steps: 6, steps per second: 109, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000062, mean_q: 0.257005\n",
      " 2780/10000: episode: 83, duration: 0.055s, episode steps: 6, steps per second: 110, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000135, mean_q: 0.263138\n",
      " 2786/10000: episode: 84, duration: 0.054s, episode steps: 6, steps per second: 110, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000123, mean_q: 0.272412\n",
      " 2792/10000: episode: 85, duration: 0.054s, episode steps: 6, steps per second: 111, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000151, mean_q: 0.286602\n",
      " 2798/10000: episode: 86, duration: 0.112s, episode steps: 6, steps per second: 53, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000104, mean_q: 0.297649\n",
      " 2806/10000: episode: 87, duration: 0.073s, episode steps: 8, steps per second: 110, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.625 [1.000, 3.000], mean observation: 7.875 [1.000, 15.000], loss: 0.000093, mean_q: 0.307205\n",
      " 2812/10000: episode: 88, duration: 0.053s, episode steps: 6, steps per second: 113, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000182, mean_q: 0.321008\n",
      " 2818/10000: episode: 89, duration: 0.054s, episode steps: 6, steps per second: 112, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000146, mean_q: 0.337729\n",
      " 2824/10000: episode: 90, duration: 0.054s, episode steps: 6, steps per second: 110, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000114, mean_q: 0.335384\n",
      " 2833/10000: episode: 91, duration: 0.077s, episode steps: 9, steps per second: 117, episode reward: 1.000, mean reward: 0.111 [0.000, 1.000], mean action: 1.556 [1.000, 3.000], mean observation: 9.556 [1.000, 15.000], loss: 0.000159, mean_q: 0.353010\n",
      " 2839/10000: episode: 92, duration: 0.054s, episode steps: 6, steps per second: 112, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000129, mean_q: 0.370507\n",
      " 2845/10000: episode: 93, duration: 0.055s, episode steps: 6, steps per second: 108, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000223, mean_q: 0.385204\n",
      " 2851/10000: episode: 94, duration: 0.055s, episode steps: 6, steps per second: 109, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000102, mean_q: 0.381926\n",
      " 2853/10000: episode: 95, duration: 0.023s, episode steps: 2, steps per second: 86, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.500 [1.000, 2.000], mean observation: 3.000 [1.000, 5.000], loss: 0.000216, mean_q: 0.389084\n",
      " 2859/10000: episode: 96, duration: 0.058s, episode steps: 6, steps per second: 103, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000152, mean_q: 0.400703\n",
      " 2861/10000: episode: 97, duration: 0.024s, episode steps: 2, steps per second: 83, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.500 [1.000, 2.000], mean observation: 3.000 [1.000, 5.000], loss: 0.000171, mean_q: 0.416869\n",
      " 2868/10000: episode: 98, duration: 0.062s, episode steps: 7, steps per second: 113, episode reward: 1.000, mean reward: 0.143 [0.000, 1.000], mean action: 1.429 [1.000, 2.000], mean observation: 8.857 [1.000, 15.000], loss: 0.000213, mean_q: 0.422710\n",
      " 2874/10000: episode: 99, duration: 0.055s, episode steps: 6, steps per second: 109, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000153, mean_q: 0.428778\n",
      " 2880/10000: episode: 100, duration: 0.055s, episode steps: 6, steps per second: 109, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000072, mean_q: 0.444302\n",
      " 2886/10000: episode: 101, duration: 0.054s, episode steps: 6, steps per second: 110, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000137, mean_q: 0.447562\n",
      " 2894/10000: episode: 102, duration: 0.069s, episode steps: 8, steps per second: 115, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.375 [0.000, 2.000], mean observation: 6.625 [1.000, 15.000], loss: 0.000188, mean_q: 0.468377\n",
      " 2900/10000: episode: 103, duration: 0.055s, episode steps: 6, steps per second: 109, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000158, mean_q: 0.472598\n",
      " 2904/10000: episode: 104, duration: 0.039s, episode steps: 4, steps per second: 103, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.250 [0.000, 2.000], mean observation: 3.500 [1.000, 6.000], loss: 0.000199, mean_q: 0.480826\n",
      " 2912/10000: episode: 105, duration: 0.069s, episode steps: 8, steps per second: 116, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.375 [0.000, 2.000], mean observation: 8.750 [1.000, 15.000], loss: 0.000157, mean_q: 0.491501\n",
      " 2918/10000: episode: 106, duration: 0.054s, episode steps: 6, steps per second: 112, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000239, mean_q: 0.504952\n",
      " 2924/10000: episode: 107, duration: 0.055s, episode steps: 6, steps per second: 110, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000172, mean_q: 0.519553\n",
      " 2930/10000: episode: 108, duration: 0.054s, episode steps: 6, steps per second: 111, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000351, mean_q: 0.525390\n",
      " 2934/10000: episode: 109, duration: 0.039s, episode steps: 4, steps per second: 103, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.250 [0.000, 2.000], mean observation: 3.500 [1.000, 6.000], loss: 0.000151, mean_q: 0.539867\n",
      " 2940/10000: episode: 110, duration: 0.054s, episode steps: 6, steps per second: 111, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000267, mean_q: 0.543545\n",
      " 2945/10000: episode: 111, duration: 0.046s, episode steps: 5, steps per second: 109, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.600 [1.000, 2.000], mean observation: 6.000 [1.000, 11.000], loss: 0.000166, mean_q: 0.554866\n",
      " 2953/10000: episode: 112, duration: 0.071s, episode steps: 8, steps per second: 112, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.625 [1.000, 3.000], mean observation: 7.000 [1.000, 15.000], loss: 0.000368, mean_q: 0.567357\n",
      " 2959/10000: episode: 113, duration: 0.056s, episode steps: 6, steps per second: 107, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000712, mean_q: 0.575422\n",
      " 2965/10000: episode: 114, duration: 0.055s, episode steps: 6, steps per second: 109, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000430, mean_q: 0.583147\n",
      " 2970/10000: episode: 115, duration: 0.046s, episode steps: 5, steps per second: 108, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 2.000 [1.000, 3.000], mean observation: 3.200 [0.000, 7.000], loss: 0.000453, mean_q: 0.590891\n",
      " 2976/10000: episode: 116, duration: 0.055s, episode steps: 6, steps per second: 109, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000370, mean_q: 0.598508\n",
      " 2983/10000: episode: 117, duration: 0.063s, episode steps: 7, steps per second: 111, episode reward: 1.000, mean reward: 0.143 [0.000, 1.000], mean action: 1.714 [1.000, 3.000], mean observation: 7.000 [1.000, 15.000], loss: 0.000413, mean_q: 0.611501\n",
      " 2989/10000: episode: 118, duration: 0.055s, episode steps: 6, steps per second: 109, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000586, mean_q: 0.616207\n",
      " 2995/10000: episode: 119, duration: 0.054s, episode steps: 6, steps per second: 110, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000314, mean_q: 0.633957\n",
      " 3001/10000: episode: 120, duration: 0.055s, episode steps: 6, steps per second: 110, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000373, mean_q: 0.637864\n",
      " 3007/10000: episode: 121, duration: 0.055s, episode steps: 6, steps per second: 109, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000781, mean_q: 0.647514\n",
      " 3013/10000: episode: 122, duration: 0.054s, episode steps: 6, steps per second: 111, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 10.500 [4.000, 15.000], loss: 0.000656, mean_q: 0.654603\n",
      " 3019/10000: episode: 123, duration: 0.054s, episode steps: 6, steps per second: 112, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000473, mean_q: 0.662048\n",
      " 3025/10000: episode: 124, duration: 0.056s, episode steps: 6, steps per second: 108, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000408, mean_q: 0.674508\n",
      " 3031/10000: episode: 125, duration: 0.054s, episode steps: 6, steps per second: 112, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000782, mean_q: 0.678057\n",
      " 3035/10000: episode: 126, duration: 0.039s, episode steps: 4, steps per second: 103, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.250 [0.000, 2.000], mean observation: 3.500 [1.000, 6.000], loss: 0.000340, mean_q: 0.685050\n",
      " 3043/10000: episode: 127, duration: 0.069s, episode steps: 8, steps per second: 116, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.625 [1.000, 3.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000749, mean_q: 0.695227\n",
      " 3045/10000: episode: 128, duration: 0.023s, episode steps: 2, steps per second: 87, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.500 [1.000, 2.000], mean observation: 4.500 [4.000, 5.000], loss: 0.000192, mean_q: 0.702047\n",
      " 3051/10000: episode: 129, duration: 0.055s, episode steps: 6, steps per second: 109, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000990, mean_q: 0.707803\n",
      " 3057/10000: episode: 130, duration: 0.054s, episode steps: 6, steps per second: 111, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000583, mean_q: 0.709554\n",
      " 3063/10000: episode: 131, duration: 0.053s, episode steps: 6, steps per second: 112, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000403, mean_q: 0.721087\n",
      " 3069/10000: episode: 132, duration: 0.053s, episode steps: 6, steps per second: 112, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000531, mean_q: 0.726543\n",
      " 3075/10000: episode: 133, duration: 0.057s, episode steps: 6, steps per second: 105, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000606, mean_q: 0.740473\n",
      " 3081/10000: episode: 134, duration: 0.056s, episode steps: 6, steps per second: 107, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001095, mean_q: 0.741656\n",
      " 3087/10000: episode: 135, duration: 0.054s, episode steps: 6, steps per second: 111, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000621, mean_q: 0.748623\n",
      " 3092/10000: episode: 136, duration: 0.046s, episode steps: 5, steps per second: 108, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.600 [1.000, 2.000], mean observation: 6.000 [1.000, 11.000], loss: 0.000553, mean_q: 0.759435\n",
      " 3098/10000: episode: 137, duration: 0.054s, episode steps: 6, steps per second: 110, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000779, mean_q: 0.764081\n",
      " 3104/10000: episode: 138, duration: 0.056s, episode steps: 6, steps per second: 108, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000700, mean_q: 0.770163\n",
      " 3110/10000: episode: 139, duration: 0.054s, episode steps: 6, steps per second: 110, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001215, mean_q: 0.775801\n",
      " 3116/10000: episode: 140, duration: 0.054s, episode steps: 6, steps per second: 112, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000799, mean_q: 0.781780\n",
      " 3118/10000: episode: 141, duration: 0.023s, episode steps: 2, steps per second: 87, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.500 [1.000, 2.000], mean observation: 3.000 [1.000, 5.000], loss: 0.001240, mean_q: 0.781539\n",
      " 3124/10000: episode: 142, duration: 0.107s, episode steps: 6, steps per second: 56, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000624, mean_q: 0.787322\n",
      " 3130/10000: episode: 143, duration: 0.062s, episode steps: 6, steps per second: 97, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000970, mean_q: 0.794351\n",
      " 3136/10000: episode: 144, duration: 0.054s, episode steps: 6, steps per second: 111, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000808, mean_q: 0.797305\n",
      " 3142/10000: episode: 145, duration: 0.054s, episode steps: 6, steps per second: 112, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000717, mean_q: 0.803006\n",
      " 3148/10000: episode: 146, duration: 0.055s, episode steps: 6, steps per second: 109, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000756, mean_q: 0.811694\n",
      " 3154/10000: episode: 147, duration: 0.055s, episode steps: 6, steps per second: 110, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000907, mean_q: 0.816748\n",
      " 3162/10000: episode: 148, duration: 0.069s, episode steps: 8, steps per second: 116, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.375 [0.000, 2.000], mean observation: 6.625 [1.000, 15.000], loss: 0.001252, mean_q: 0.819780\n",
      " 3168/10000: episode: 149, duration: 0.054s, episode steps: 6, steps per second: 112, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001012, mean_q: 0.821674\n",
      " 3176/10000: episode: 150, duration: 0.071s, episode steps: 8, steps per second: 112, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.375 [0.000, 2.000], mean observation: 9.375 [1.000, 15.000], loss: 0.001139, mean_q: 0.831203\n",
      " 3182/10000: episode: 151, duration: 0.056s, episode steps: 6, steps per second: 108, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001356, mean_q: 0.838465\n",
      " 3186/10000: episode: 152, duration: 0.038s, episode steps: 4, steps per second: 106, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.250 [0.000, 2.000], mean observation: 3.500 [1.000, 6.000], loss: 0.001058, mean_q: 0.839245\n",
      " 3192/10000: episode: 153, duration: 0.054s, episode steps: 6, steps per second: 111, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001003, mean_q: 0.845588\n",
      " 3198/10000: episode: 154, duration: 0.057s, episode steps: 6, steps per second: 106, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000714, mean_q: 0.847374\n",
      " 3204/10000: episode: 155, duration: 0.055s, episode steps: 6, steps per second: 109, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001013, mean_q: 0.851781\n",
      " 3210/10000: episode: 156, duration: 0.054s, episode steps: 6, steps per second: 111, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001578, mean_q: 0.855357\n",
      " 3218/10000: episode: 157, duration: 0.071s, episode steps: 8, steps per second: 112, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.375 [0.000, 2.000], mean observation: 6.125 [0.000, 15.000], loss: 0.001315, mean_q: 0.857877\n",
      " 3220/10000: episode: 158, duration: 0.023s, episode steps: 2, steps per second: 85, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.500 [1.000, 2.000], mean observation: 3.000 [1.000, 5.000], loss: 0.000243, mean_q: 0.865156\n",
      " 3226/10000: episode: 159, duration: 0.056s, episode steps: 6, steps per second: 108, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000937, mean_q: 0.867255\n",
      " 3232/10000: episode: 160, duration: 0.053s, episode steps: 6, steps per second: 112, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001430, mean_q: 0.869520\n",
      " 3240/10000: episode: 161, duration: 0.069s, episode steps: 8, steps per second: 115, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.500 [0.000, 3.000], mean observation: 6.125 [0.000, 15.000], loss: 0.001599, mean_q: 0.872804\n",
      " 3248/10000: episode: 162, duration: 0.072s, episode steps: 8, steps per second: 111, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.625 [1.000, 3.000], mean observation: 9.000 [1.000, 15.000], loss: 0.001602, mean_q: 0.875074\n",
      " 3254/10000: episode: 163, duration: 0.054s, episode steps: 6, steps per second: 111, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000739, mean_q: 0.879168\n",
      " 3260/10000: episode: 164, duration: 0.054s, episode steps: 6, steps per second: 112, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001059, mean_q: 0.880707\n",
      " 3266/10000: episode: 165, duration: 0.054s, episode steps: 6, steps per second: 112, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001146, mean_q: 0.887699\n",
      " 3272/10000: episode: 166, duration: 0.054s, episode steps: 6, steps per second: 110, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001492, mean_q: 0.889661\n",
      " 3278/10000: episode: 167, duration: 0.055s, episode steps: 6, steps per second: 109, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001085, mean_q: 0.890170\n",
      " 3284/10000: episode: 168, duration: 0.054s, episode steps: 6, steps per second: 110, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001654, mean_q: 0.895439\n",
      " 3290/10000: episode: 169, duration: 0.056s, episode steps: 6, steps per second: 108, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001414, mean_q: 0.898695\n",
      " 3296/10000: episode: 170, duration: 0.055s, episode steps: 6, steps per second: 108, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001213, mean_q: 0.898494\n",
      " 3302/10000: episode: 171, duration: 0.112s, episode steps: 6, steps per second: 54, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001773, mean_q: 0.901697\n",
      " 3308/10000: episode: 172, duration: 0.058s, episode steps: 6, steps per second: 103, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001274, mean_q: 0.904392\n",
      " 3317/10000: episode: 173, duration: 0.081s, episode steps: 9, steps per second: 112, episode reward: 1.000, mean reward: 0.111 [0.000, 1.000], mean action: 1.333 [0.000, 2.000], mean observation: 7.444 [1.000, 15.000], loss: 0.001123, mean_q: 0.907424\n",
      " 3323/10000: episode: 174, duration: 0.055s, episode steps: 6, steps per second: 110, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001706, mean_q: 0.910641\n",
      " 3329/10000: episode: 175, duration: 0.054s, episode steps: 6, steps per second: 112, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001101, mean_q: 0.913929\n",
      " 3335/10000: episode: 176, duration: 0.054s, episode steps: 6, steps per second: 111, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.833 [1.000, 3.000], mean observation: 5.333 [1.000, 11.000], loss: 0.001618, mean_q: 0.913792\n",
      " 3341/10000: episode: 177, duration: 0.055s, episode steps: 6, steps per second: 110, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001364, mean_q: 0.915336\n",
      " 3345/10000: episode: 178, duration: 0.039s, episode steps: 4, steps per second: 104, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.750 [1.000, 2.000], mean observation: 4.000 [1.000, 7.000], loss: 0.001876, mean_q: 0.919218\n",
      " 3351/10000: episode: 179, duration: 0.054s, episode steps: 6, steps per second: 112, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000721, mean_q: 0.919514\n",
      " 3359/10000: episode: 180, duration: 0.070s, episode steps: 8, steps per second: 114, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.375 [0.000, 2.000], mean observation: 6.375 [1.000, 15.000], loss: 0.000832, mean_q: 0.922542\n",
      " 3365/10000: episode: 181, duration: 0.055s, episode steps: 6, steps per second: 110, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001486, mean_q: 0.925728\n",
      " 3371/10000: episode: 182, duration: 0.054s, episode steps: 6, steps per second: 112, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000976, mean_q: 0.924692\n",
      " 3378/10000: episode: 183, duration: 0.062s, episode steps: 7, steps per second: 113, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.714 [1.000, 3.000], mean observation: 6.571 [1.000, 11.000], loss: 0.002385, mean_q: 0.924716\n",
      " 3384/10000: episode: 184, duration: 0.054s, episode steps: 6, steps per second: 112, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.002498, mean_q: 0.927071\n",
      " 3394/10000: episode: 185, duration: 0.089s, episode steps: 10, steps per second: 113, episode reward: 1.000, mean reward: 0.100 [0.000, 1.000], mean action: 1.700 [1.000, 3.000], mean observation: 9.600 [1.000, 15.000], loss: 0.001123, mean_q: 0.929200\n",
      " 3399/10000: episode: 186, duration: 0.047s, episode steps: 5, steps per second: 107, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.600 [1.000, 2.000], mean observation: 6.000 [1.000, 11.000], loss: 0.001309, mean_q: 0.932442\n",
      " 3405/10000: episode: 187, duration: 0.054s, episode steps: 6, steps per second: 111, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001266, mean_q: 0.932942\n",
      " 3411/10000: episode: 188, duration: 0.055s, episode steps: 6, steps per second: 109, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001549, mean_q: 0.934293\n",
      " 3415/10000: episode: 189, duration: 0.039s, episode steps: 4, steps per second: 103, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.750 [1.000, 2.000], mean observation: 4.000 [1.000, 7.000], loss: 0.000946, mean_q: 0.937124\n",
      " 3421/10000: episode: 190, duration: 0.057s, episode steps: 6, steps per second: 106, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 10.500 [4.000, 15.000], loss: 0.001729, mean_q: 0.934298\n",
      " 3427/10000: episode: 191, duration: 0.054s, episode steps: 6, steps per second: 111, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001211, mean_q: 0.935347\n",
      " 3433/10000: episode: 192, duration: 0.054s, episode steps: 6, steps per second: 111, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000982, mean_q: 0.936919\n",
      " 3439/10000: episode: 193, duration: 0.054s, episode steps: 6, steps per second: 111, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001598, mean_q: 0.939103\n",
      " 3445/10000: episode: 194, duration: 0.055s, episode steps: 6, steps per second: 109, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001345, mean_q: 0.940671\n",
      " 3451/10000: episode: 195, duration: 0.055s, episode steps: 6, steps per second: 110, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.002161, mean_q: 0.941881\n",
      " 3457/10000: episode: 196, duration: 0.054s, episode steps: 6, steps per second: 111, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000651, mean_q: 0.942572\n",
      " 3463/10000: episode: 197, duration: 0.054s, episode steps: 6, steps per second: 110, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001521, mean_q: 0.942013\n",
      " 3471/10000: episode: 198, duration: 0.072s, episode steps: 8, steps per second: 112, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.375 [0.000, 2.000], mean observation: 8.750 [1.000, 15.000], loss: 0.001535, mean_q: 0.942272\n",
      " 3482/10000: episode: 199, duration: 0.094s, episode steps: 11, steps per second: 117, episode reward: 1.000, mean reward: 0.091 [0.000, 1.000], mean action: 1.636 [1.000, 3.000], mean observation: 8.545 [1.000, 15.000], loss: 0.001797, mean_q: 0.944122\n",
      " 3488/10000: episode: 200, duration: 0.056s, episode steps: 6, steps per second: 107, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001574, mean_q: 0.943770\n",
      " 3494/10000: episode: 201, duration: 0.055s, episode steps: 6, steps per second: 108, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.002304, mean_q: 0.945290\n",
      " 3500/10000: episode: 202, duration: 0.056s, episode steps: 6, steps per second: 108, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001699, mean_q: 0.946696\n",
      " 3506/10000: episode: 203, duration: 0.054s, episode steps: 6, steps per second: 111, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000936, mean_q: 0.947609\n",
      " 3512/10000: episode: 204, duration: 0.055s, episode steps: 6, steps per second: 110, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000944, mean_q: 0.947140\n",
      " 3518/10000: episode: 205, duration: 0.056s, episode steps: 6, steps per second: 107, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001345, mean_q: 0.946691\n",
      " 3526/10000: episode: 206, duration: 0.071s, episode steps: 8, steps per second: 113, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.375 [0.000, 2.000], mean observation: 6.125 [0.000, 15.000], loss: 0.001381, mean_q: 0.947802\n",
      " 3532/10000: episode: 207, duration: 0.054s, episode steps: 6, steps per second: 111, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001702, mean_q: 0.950826\n",
      " 3538/10000: episode: 208, duration: 0.054s, episode steps: 6, steps per second: 111, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001676, mean_q: 0.951185\n",
      " 3544/10000: episode: 209, duration: 0.056s, episode steps: 6, steps per second: 107, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001044, mean_q: 0.951745\n",
      " 3552/10000: episode: 210, duration: 0.069s, episode steps: 8, steps per second: 115, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.375 [0.000, 2.000], mean observation: 6.375 [1.000, 15.000], loss: 0.001305, mean_q: 0.952540\n",
      " 3558/10000: episode: 211, duration: 0.054s, episode steps: 6, steps per second: 111, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001459, mean_q: 0.954166\n",
      " 3566/10000: episode: 212, duration: 0.069s, episode steps: 8, steps per second: 116, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.625 [1.000, 3.000], mean observation: 9.000 [1.000, 15.000], loss: 0.001899, mean_q: 0.953598\n",
      " 3572/10000: episode: 213, duration: 0.056s, episode steps: 6, steps per second: 106, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001445, mean_q: 0.953712\n",
      " 3578/10000: episode: 214, duration: 0.054s, episode steps: 6, steps per second: 110, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001123, mean_q: 0.954013\n",
      " 3584/10000: episode: 215, duration: 0.055s, episode steps: 6, steps per second: 110, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001863, mean_q: 0.953988\n",
      " 3590/10000: episode: 216, duration: 0.054s, episode steps: 6, steps per second: 111, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001507, mean_q: 0.954960\n",
      " 3595/10000: episode: 217, duration: 0.046s, episode steps: 5, steps per second: 108, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.600 [1.000, 2.000], mean observation: 6.000 [1.000, 11.000], loss: 0.000825, mean_q: 0.954955\n",
      " 3601/10000: episode: 218, duration: 0.054s, episode steps: 6, steps per second: 111, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000980, mean_q: 0.954606\n",
      " 3607/10000: episode: 219, duration: 0.055s, episode steps: 6, steps per second: 109, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.002033, mean_q: 0.955112\n",
      " 3615/10000: episode: 220, duration: 0.070s, episode steps: 8, steps per second: 115, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.375 [0.000, 2.000], mean observation: 6.625 [1.000, 15.000], loss: 0.001521, mean_q: 0.954320\n",
      " 3621/10000: episode: 221, duration: 0.056s, episode steps: 6, steps per second: 107, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000839, mean_q: 0.955484\n",
      " 3627/10000: episode: 222, duration: 0.055s, episode steps: 6, steps per second: 109, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000813, mean_q: 0.957103\n",
      " 3633/10000: episode: 223, duration: 0.110s, episode steps: 6, steps per second: 54, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001340, mean_q: 0.957417\n",
      " 3639/10000: episode: 224, duration: 0.061s, episode steps: 6, steps per second: 98, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001803, mean_q: 0.957605\n",
      " 3646/10000: episode: 225, duration: 0.062s, episode steps: 7, steps per second: 113, episode reward: 1.000, mean reward: 0.143 [0.000, 1.000], mean action: 1.714 [1.000, 3.000], mean observation: 7.000 [1.000, 15.000], loss: 0.001661, mean_q: 0.957267\n",
      " 3652/10000: episode: 226, duration: 0.054s, episode steps: 6, steps per second: 111, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001384, mean_q: 0.956742\n",
      " 3660/10000: episode: 227, duration: 0.069s, episode steps: 8, steps per second: 115, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.375 [0.000, 2.000], mean observation: 9.375 [1.000, 15.000], loss: 0.001965, mean_q: 0.956593\n",
      " 3666/10000: episode: 228, duration: 0.055s, episode steps: 6, steps per second: 109, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000969, mean_q: 0.956368\n",
      " 3672/10000: episode: 229, duration: 0.055s, episode steps: 6, steps per second: 110, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001101, mean_q: 0.957840\n",
      " 3680/10000: episode: 230, duration: 0.070s, episode steps: 8, steps per second: 115, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.375 [0.000, 2.000], mean observation: 9.375 [1.000, 15.000], loss: 0.001243, mean_q: 0.959951\n",
      " 3686/10000: episode: 231, duration: 0.054s, episode steps: 6, steps per second: 110, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.002060, mean_q: 0.960064\n",
      " 3692/10000: episode: 232, duration: 0.057s, episode steps: 6, steps per second: 105, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001395, mean_q: 0.959951\n",
      " 3694/10000: episode: 233, duration: 0.024s, episode steps: 2, steps per second: 84, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.500 [1.000, 2.000], mean observation: 3.000 [1.000, 5.000], loss: 0.002472, mean_q: 0.960119\n",
      " 3702/10000: episode: 234, duration: 0.069s, episode steps: 8, steps per second: 115, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.125 [0.000, 2.000], mean observation: 6.000 [0.000, 15.000], loss: 0.001151, mean_q: 0.960070\n",
      " 3708/10000: episode: 235, duration: 0.053s, episode steps: 6, steps per second: 112, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001837, mean_q: 0.959794\n",
      " 3714/10000: episode: 236, duration: 0.057s, episode steps: 6, steps per second: 105, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001692, mean_q: 0.959531\n",
      " 3720/10000: episode: 237, duration: 0.055s, episode steps: 6, steps per second: 109, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001554, mean_q: 0.959523\n",
      " 3726/10000: episode: 238, duration: 0.054s, episode steps: 6, steps per second: 111, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001675, mean_q: 0.959100\n",
      " 3732/10000: episode: 239, duration: 0.056s, episode steps: 6, steps per second: 108, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.002490, mean_q: 0.960448\n",
      " 3738/10000: episode: 240, duration: 0.056s, episode steps: 6, steps per second: 107, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001541, mean_q: 0.961194\n",
      " 3746/10000: episode: 241, duration: 0.071s, episode steps: 8, steps per second: 113, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.375 [0.000, 2.000], mean observation: 6.375 [1.000, 15.000], loss: 0.001582, mean_q: 0.961642\n",
      " 3752/10000: episode: 242, duration: 0.055s, episode steps: 6, steps per second: 108, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001703, mean_q: 0.960427\n",
      " 3758/10000: episode: 243, duration: 0.054s, episode steps: 6, steps per second: 110, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001804, mean_q: 0.960058\n",
      " 3764/10000: episode: 244, duration: 0.056s, episode steps: 6, steps per second: 108, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001396, mean_q: 0.960751\n",
      " 3770/10000: episode: 245, duration: 0.055s, episode steps: 6, steps per second: 110, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001557, mean_q: 0.960933\n",
      " 3777/10000: episode: 246, duration: 0.062s, episode steps: 7, steps per second: 113, episode reward: 1.000, mean reward: 0.143 [0.000, 1.000], mean action: 1.714 [1.000, 3.000], mean observation: 7.143 [1.000, 15.000], loss: 0.001109, mean_q: 0.959874\n",
      " 3783/10000: episode: 247, duration: 0.054s, episode steps: 6, steps per second: 111, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001820, mean_q: 0.960076\n",
      " 3790/10000: episode: 248, duration: 0.063s, episode steps: 7, steps per second: 111, episode reward: 1.000, mean reward: 0.143 [0.000, 1.000], mean action: 1.429 [1.000, 2.000], mean observation: 8.857 [1.000, 15.000], loss: 0.001201, mean_q: 0.961050\n",
      " 3798/10000: episode: 249, duration: 0.072s, episode steps: 8, steps per second: 111, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.625 [1.000, 3.000], mean observation: 9.000 [1.000, 15.000], loss: 0.001057, mean_q: 0.961778\n",
      " 3805/10000: episode: 250, duration: 0.063s, episode steps: 7, steps per second: 112, episode reward: 1.000, mean reward: 0.143 [0.000, 1.000], mean action: 1.714 [1.000, 3.000], mean observation: 7.143 [1.000, 15.000], loss: 0.001600, mean_q: 0.962108\n",
      " 3813/10000: episode: 251, duration: 0.133s, episode steps: 8, steps per second: 60, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.625 [1.000, 3.000], mean observation: 9.000 [1.000, 15.000], loss: 0.001598, mean_q: 0.961289\n",
      " 3819/10000: episode: 252, duration: 0.055s, episode steps: 6, steps per second: 109, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001144, mean_q: 0.960965\n",
      " 3825/10000: episode: 253, duration: 0.054s, episode steps: 6, steps per second: 112, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001530, mean_q: 0.961535\n",
      " 3833/10000: episode: 254, duration: 0.070s, episode steps: 8, steps per second: 114, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.625 [1.000, 3.000], mean observation: 9.000 [1.000, 15.000], loss: 0.001069, mean_q: 0.961070\n",
      " 3839/10000: episode: 255, duration: 0.055s, episode steps: 6, steps per second: 108, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001292, mean_q: 0.961880\n",
      " 3845/10000: episode: 256, duration: 0.055s, episode steps: 6, steps per second: 109, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001132, mean_q: 0.962247\n",
      " 3851/10000: episode: 257, duration: 0.055s, episode steps: 6, steps per second: 110, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.002136, mean_q: 0.961713\n",
      " 3858/10000: episode: 258, duration: 0.062s, episode steps: 7, steps per second: 112, episode reward: 1.000, mean reward: 0.143 [0.000, 1.000], mean action: 1.429 [1.000, 2.000], mean observation: 8.857 [1.000, 15.000], loss: 0.000883, mean_q: 0.962400\n",
      " 3866/10000: episode: 259, duration: 0.072s, episode steps: 8, steps per second: 111, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.375 [0.000, 2.000], mean observation: 9.375 [1.000, 15.000], loss: 0.001476, mean_q: 0.962357\n",
      " 3873/10000: episode: 260, duration: 0.062s, episode steps: 7, steps per second: 112, episode reward: 1.000, mean reward: 0.143 [0.000, 1.000], mean action: 1.429 [1.000, 2.000], mean observation: 8.857 [1.000, 15.000], loss: 0.001451, mean_q: 0.961725\n",
      " 3879/10000: episode: 261, duration: 0.054s, episode steps: 6, steps per second: 112, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.002102, mean_q: 0.962088\n",
      " 3881/10000: episode: 262, duration: 0.023s, episode steps: 2, steps per second: 86, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.500 [1.000, 2.000], mean observation: 3.000 [1.000, 5.000], loss: 0.000442, mean_q: 0.961504\n",
      " 3887/10000: episode: 263, duration: 0.054s, episode steps: 6, steps per second: 111, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.002248, mean_q: 0.961835\n",
      " 3893/10000: episode: 264, duration: 0.055s, episode steps: 6, steps per second: 108, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.002007, mean_q: 0.961097\n",
      " 3899/10000: episode: 265, duration: 0.054s, episode steps: 6, steps per second: 110, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001014, mean_q: 0.961111\n",
      " 3905/10000: episode: 266, duration: 0.055s, episode steps: 6, steps per second: 108, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001560, mean_q: 0.961284\n",
      " 3911/10000: episode: 267, duration: 0.056s, episode steps: 6, steps per second: 108, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.002136, mean_q: 0.962195\n",
      " 3915/10000: episode: 268, duration: 0.038s, episode steps: 4, steps per second: 104, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.750 [1.000, 2.000], mean observation: 4.000 [1.000, 7.000], loss: 0.001905, mean_q: 0.961895\n",
      " 3921/10000: episode: 269, duration: 0.057s, episode steps: 6, steps per second: 106, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000719, mean_q: 0.961668\n",
      " 3929/10000: episode: 270, duration: 0.072s, episode steps: 8, steps per second: 111, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.625 [1.000, 3.000], mean observation: 7.000 [1.000, 15.000], loss: 0.001610, mean_q: 0.962416\n",
      " 3935/10000: episode: 271, duration: 0.055s, episode steps: 6, steps per second: 109, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001437, mean_q: 0.961690\n",
      " 3941/10000: episode: 272, duration: 0.057s, episode steps: 6, steps per second: 105, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001448, mean_q: 0.961670\n",
      " 3949/10000: episode: 273, duration: 0.072s, episode steps: 8, steps per second: 112, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.375 [0.000, 2.000], mean observation: 6.625 [1.000, 15.000], loss: 0.001386, mean_q: 0.962142\n",
      " 3955/10000: episode: 274, duration: 0.055s, episode steps: 6, steps per second: 109, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001441, mean_q: 0.962800\n",
      " 3961/10000: episode: 275, duration: 0.055s, episode steps: 6, steps per second: 110, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001841, mean_q: 0.962894\n",
      " 3967/10000: episode: 276, duration: 0.057s, episode steps: 6, steps per second: 106, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.002275, mean_q: 0.962213\n",
      " 3973/10000: episode: 277, duration: 0.054s, episode steps: 6, steps per second: 111, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000165, mean_q: 0.962105\n",
      " 3982/10000: episode: 278, duration: 0.077s, episode steps: 9, steps per second: 116, episode reward: 1.000, mean reward: 0.111 [0.000, 1.000], mean action: 1.556 [0.000, 3.000], mean observation: 6.000 [1.000, 15.000], loss: 0.001048, mean_q: 0.962018\n",
      " 3988/10000: episode: 279, duration: 0.055s, episode steps: 6, steps per second: 110, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.500 [0.000, 2.000], mean observation: 3.167 [1.000, 7.000], loss: 0.001289, mean_q: 0.962260\n",
      " 4000/10000: episode: 280, duration: 0.103s, episode steps: 12, steps per second: 116, episode reward: 1.000, mean reward: 0.083 [0.000, 1.000], mean action: 1.750 [0.000, 3.000], mean observation: 4.833 [0.000, 15.000], loss: 0.001637, mean_q: 0.962479\n",
      " 4002/10000: episode: 281, duration: 0.024s, episode steps: 2, steps per second: 84, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.500 [1.000, 2.000], mean observation: 3.000 [1.000, 5.000], loss: 0.001283, mean_q: 0.963078\n",
      " 4008/10000: episode: 282, duration: 0.054s, episode steps: 6, steps per second: 111, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.002381, mean_q: 0.962169\n",
      " 4014/10000: episode: 283, duration: 0.055s, episode steps: 6, steps per second: 110, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001580, mean_q: 0.962229\n",
      " 4020/10000: episode: 284, duration: 0.055s, episode steps: 6, steps per second: 110, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001164, mean_q: 0.962431\n",
      " 4026/10000: episode: 285, duration: 0.056s, episode steps: 6, steps per second: 108, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001701, mean_q: 0.963095\n",
      " 4032/10000: episode: 286, duration: 0.055s, episode steps: 6, steps per second: 109, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001276, mean_q: 0.963491\n",
      " 4038/10000: episode: 287, duration: 0.055s, episode steps: 6, steps per second: 109, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001002, mean_q: 0.963111\n",
      " 4044/10000: episode: 288, duration: 0.054s, episode steps: 6, steps per second: 111, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001422, mean_q: 0.963998\n",
      " 4050/10000: episode: 289, duration: 0.057s, episode steps: 6, steps per second: 106, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001287, mean_q: 0.963795\n",
      " 4056/10000: episode: 290, duration: 0.056s, episode steps: 6, steps per second: 107, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001004, mean_q: 0.963406\n",
      " 4062/10000: episode: 291, duration: 0.055s, episode steps: 6, steps per second: 108, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001726, mean_q: 0.963139\n",
      " 4068/10000: episode: 292, duration: 0.056s, episode steps: 6, steps per second: 108, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001140, mean_q: 0.963299\n",
      " 4074/10000: episode: 293, duration: 0.057s, episode steps: 6, steps per second: 106, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001561, mean_q: 0.963183\n",
      " 4082/10000: episode: 294, duration: 0.071s, episode steps: 8, steps per second: 113, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.375 [0.000, 2.000], mean observation: 6.375 [1.000, 15.000], loss: 0.000886, mean_q: 0.963621\n",
      " 4088/10000: episode: 295, duration: 0.054s, episode steps: 6, steps per second: 111, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001283, mean_q: 0.962886\n",
      " 4094/10000: episode: 296, duration: 0.055s, episode steps: 6, steps per second: 110, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001434, mean_q: 0.963697\n",
      " 4098/10000: episode: 297, duration: 0.040s, episode steps: 4, steps per second: 100, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.750 [1.000, 2.000], mean observation: 4.000 [1.000, 7.000], loss: 0.000854, mean_q: 0.963651\n",
      " 4106/10000: episode: 298, duration: 0.070s, episode steps: 8, steps per second: 114, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.625 [1.000, 3.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001280, mean_q: 0.964358\n",
      " 4112/10000: episode: 299, duration: 0.054s, episode steps: 6, steps per second: 110, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001431, mean_q: 0.964805\n",
      " 4118/10000: episode: 300, duration: 0.054s, episode steps: 6, steps per second: 111, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001841, mean_q: 0.964665\n",
      " 4125/10000: episode: 301, duration: 0.065s, episode steps: 7, steps per second: 108, episode reward: 1.000, mean reward: 0.143 [0.000, 1.000], mean action: 1.286 [0.000, 2.000], mean observation: 9.571 [4.000, 15.000], loss: 0.001105, mean_q: 0.964673\n",
      " 4131/10000: episode: 302, duration: 0.054s, episode steps: 6, steps per second: 111, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.002268, mean_q: 0.964848\n",
      " 4137/10000: episode: 303, duration: 0.055s, episode steps: 6, steps per second: 110, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.002004, mean_q: 0.963268\n",
      " 4144/10000: episode: 304, duration: 0.124s, episode steps: 7, steps per second: 56, episode reward: 1.000, mean reward: 0.143 [0.000, 1.000], mean action: 1.429 [1.000, 2.000], mean observation: 8.857 [1.000, 15.000], loss: 0.001115, mean_q: 0.963332\n",
      " 4146/10000: episode: 305, duration: 0.025s, episode steps: 2, steps per second: 81, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.500 [1.000, 2.000], mean observation: 3.000 [1.000, 5.000], loss: 0.001721, mean_q: 0.963490\n",
      " 4152/10000: episode: 306, duration: 0.055s, episode steps: 6, steps per second: 108, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001425, mean_q: 0.962527\n",
      " 4158/10000: episode: 307, duration: 0.055s, episode steps: 6, steps per second: 109, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001012, mean_q: 0.963385\n",
      " 4165/10000: episode: 308, duration: 0.063s, episode steps: 7, steps per second: 112, episode reward: 1.000, mean reward: 0.143 [0.000, 1.000], mean action: 1.286 [0.000, 2.000], mean observation: 6.857 [0.000, 15.000], loss: 0.001481, mean_q: 0.964116\n",
      " 4171/10000: episode: 309, duration: 0.057s, episode steps: 6, steps per second: 106, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001859, mean_q: 0.964176\n",
      " 4179/10000: episode: 310, duration: 0.070s, episode steps: 8, steps per second: 114, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.625 [1.000, 3.000], mean observation: 7.000 [1.000, 15.000], loss: 0.000658, mean_q: 0.963381\n",
      " 4185/10000: episode: 311, duration: 0.054s, episode steps: 6, steps per second: 111, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001286, mean_q: 0.963534\n",
      " 4192/10000: episode: 312, duration: 0.063s, episode steps: 7, steps per second: 112, episode reward: 1.000, mean reward: 0.143 [0.000, 1.000], mean action: 1.714 [1.000, 3.000], mean observation: 7.143 [1.000, 15.000], loss: 0.001368, mean_q: 0.963457\n",
      " 4196/10000: episode: 313, duration: 0.041s, episode steps: 4, steps per second: 98, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.250 [0.000, 2.000], mean observation: 3.500 [1.000, 6.000], loss: 0.001933, mean_q: 0.962919\n",
      " 4200/10000: episode: 314, duration: 0.039s, episode steps: 4, steps per second: 103, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.750 [1.000, 2.000], mean observation: 4.000 [1.000, 7.000], loss: 0.001284, mean_q: 0.963000\n",
      " 4206/10000: episode: 315, duration: 0.055s, episode steps: 6, steps per second: 109, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001852, mean_q: 0.962905\n",
      " 4214/10000: episode: 316, duration: 0.070s, episode steps: 8, steps per second: 114, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.625 [1.000, 3.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001075, mean_q: 0.963573\n",
      " 4220/10000: episode: 317, duration: 0.055s, episode steps: 6, steps per second: 108, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.002021, mean_q: 0.963578\n",
      " 4226/10000: episode: 318, duration: 0.054s, episode steps: 6, steps per second: 111, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000583, mean_q: 0.963922\n",
      " 4232/10000: episode: 319, duration: 0.054s, episode steps: 6, steps per second: 111, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001432, mean_q: 0.963689\n",
      " 4236/10000: episode: 320, duration: 0.039s, episode steps: 4, steps per second: 103, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.250 [0.000, 2.000], mean observation: 3.500 [1.000, 6.000], loss: 0.001719, mean_q: 0.963323\n",
      " 4242/10000: episode: 321, duration: 0.056s, episode steps: 6, steps per second: 107, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 10.500 [4.000, 15.000], loss: 0.000873, mean_q: 0.963462\n",
      " 4248/10000: episode: 322, duration: 0.056s, episode steps: 6, steps per second: 106, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000720, mean_q: 0.963998\n",
      " 4254/10000: episode: 323, duration: 0.055s, episode steps: 6, steps per second: 109, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001555, mean_q: 0.963951\n",
      " 4261/10000: episode: 324, duration: 0.063s, episode steps: 7, steps per second: 111, episode reward: 1.000, mean reward: 0.143 [0.000, 1.000], mean action: 1.714 [1.000, 3.000], mean observation: 7.000 [1.000, 15.000], loss: 0.001110, mean_q: 0.965365\n",
      " 4267/10000: episode: 325, duration: 0.055s, episode steps: 6, steps per second: 109, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001284, mean_q: 0.965516\n",
      " 4269/10000: episode: 326, duration: 0.024s, episode steps: 2, steps per second: 83, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.500 [1.000, 2.000], mean observation: 3.000 [1.000, 5.000], loss: 0.002146, mean_q: 0.963564\n",
      " 4277/10000: episode: 327, duration: 0.072s, episode steps: 8, steps per second: 111, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.375 [0.000, 2.000], mean observation: 6.375 [1.000, 15.000], loss: 0.001295, mean_q: 0.964398\n",
      " 4285/10000: episode: 328, duration: 0.070s, episode steps: 8, steps per second: 114, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.375 [0.000, 2.000], mean observation: 6.375 [1.000, 15.000], loss: 0.001939, mean_q: 0.963901\n",
      " 4291/10000: episode: 329, duration: 0.055s, episode steps: 6, steps per second: 109, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001584, mean_q: 0.963129\n",
      " 4297/10000: episode: 330, duration: 0.054s, episode steps: 6, steps per second: 111, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000718, mean_q: 0.962798\n",
      " 4304/10000: episode: 331, duration: 0.063s, episode steps: 7, steps per second: 110, episode reward: 1.000, mean reward: 0.143 [0.000, 1.000], mean action: 1.714 [1.000, 3.000], mean observation: 7.000 [1.000, 15.000], loss: 0.001232, mean_q: 0.963740\n",
      " 4310/10000: episode: 332, duration: 0.056s, episode steps: 6, steps per second: 107, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001425, mean_q: 0.964372\n",
      " 4317/10000: episode: 333, duration: 0.118s, episode steps: 7, steps per second: 60, episode reward: 1.000, mean reward: 0.143 [0.000, 1.000], mean action: 1.714 [1.000, 3.000], mean observation: 6.857 [0.000, 15.000], loss: 0.001722, mean_q: 0.964535\n",
      " 4319/10000: episode: 334, duration: 0.028s, episode steps: 2, steps per second: 71, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.500 [1.000, 2.000], mean observation: 3.000 [1.000, 5.000], loss: 0.000866, mean_q: 0.963913\n",
      " 4325/10000: episode: 335, duration: 0.056s, episode steps: 6, steps per second: 106, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000859, mean_q: 0.963823\n",
      " 4333/10000: episode: 336, duration: 0.070s, episode steps: 8, steps per second: 115, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.375 [0.000, 2.000], mean observation: 6.375 [1.000, 15.000], loss: 0.000862, mean_q: 0.964223\n",
      " 4339/10000: episode: 337, duration: 0.054s, episode steps: 6, steps per second: 110, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 10.500 [4.000, 15.000], loss: 0.001015, mean_q: 0.964216\n",
      " 4345/10000: episode: 338, duration: 0.056s, episode steps: 6, steps per second: 106, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001710, mean_q: 0.963740\n",
      " 4351/10000: episode: 339, duration: 0.057s, episode steps: 6, steps per second: 105, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001279, mean_q: 0.964400\n",
      " 4356/10000: episode: 340, duration: 0.047s, episode steps: 5, steps per second: 107, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.600 [1.000, 2.000], mean observation: 6.000 [1.000, 11.000], loss: 0.001200, mean_q: 0.964347\n",
      " 4358/10000: episode: 341, duration: 0.023s, episode steps: 2, steps per second: 85, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.500 [1.000, 2.000], mean observation: 3.000 [1.000, 5.000], loss: 0.002131, mean_q: 0.964926\n",
      " 4364/10000: episode: 342, duration: 0.056s, episode steps: 6, steps per second: 107, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000727, mean_q: 0.964495\n",
      " 4370/10000: episode: 343, duration: 0.055s, episode steps: 6, steps per second: 109, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001718, mean_q: 0.964301\n",
      " 4376/10000: episode: 344, duration: 0.057s, episode steps: 6, steps per second: 105, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001151, mean_q: 0.963975\n",
      " 4381/10000: episode: 345, duration: 0.047s, episode steps: 5, steps per second: 107, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.600 [1.000, 2.000], mean observation: 6.000 [1.000, 11.000], loss: 0.001198, mean_q: 0.964035\n",
      " 4385/10000: episode: 346, duration: 0.040s, episode steps: 4, steps per second: 101, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.750 [1.000, 2.000], mean observation: 4.000 [1.000, 7.000], loss: 0.001080, mean_q: 0.964650\n",
      " 4391/10000: episode: 347, duration: 0.055s, episode steps: 6, steps per second: 109, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001142, mean_q: 0.965075\n",
      " 4395/10000: episode: 348, duration: 0.039s, episode steps: 4, steps per second: 103, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.750 [1.000, 2.000], mean observation: 4.000 [1.000, 7.000], loss: 0.001509, mean_q: 0.965267\n",
      " 4401/10000: episode: 349, duration: 0.057s, episode steps: 6, steps per second: 106, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001160, mean_q: 0.964212\n",
      " 4403/10000: episode: 350, duration: 0.024s, episode steps: 2, steps per second: 84, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.500 [1.000, 2.000], mean observation: 3.000 [1.000, 5.000], loss: 0.001261, mean_q: 0.964354\n",
      " 4411/10000: episode: 351, duration: 0.070s, episode steps: 8, steps per second: 114, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.625 [1.000, 3.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001395, mean_q: 0.964091\n",
      " 4417/10000: episode: 352, duration: 0.055s, episode steps: 6, steps per second: 109, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.002150, mean_q: 0.963340\n",
      " 4423/10000: episode: 353, duration: 0.055s, episode steps: 6, steps per second: 109, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000716, mean_q: 0.963526\n",
      " 4429/10000: episode: 354, duration: 0.056s, episode steps: 6, steps per second: 108, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000997, mean_q: 0.963617\n",
      " 4435/10000: episode: 355, duration: 0.055s, episode steps: 6, steps per second: 110, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001012, mean_q: 0.964839\n",
      " 4439/10000: episode: 356, duration: 0.039s, episode steps: 4, steps per second: 103, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.750 [1.000, 2.000], mean observation: 4.000 [1.000, 7.000], loss: 0.001069, mean_q: 0.964752\n",
      " 4445/10000: episode: 357, duration: 0.055s, episode steps: 6, steps per second: 110, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001276, mean_q: 0.963654\n",
      " 4447/10000: episode: 358, duration: 0.026s, episode steps: 2, steps per second: 78, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.500 [1.000, 2.000], mean observation: 3.000 [1.000, 5.000], loss: 0.000442, mean_q: 0.964516\n",
      " 4453/10000: episode: 359, duration: 0.055s, episode steps: 6, steps per second: 109, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001010, mean_q: 0.964962\n",
      " 4459/10000: episode: 360, duration: 0.057s, episode steps: 6, steps per second: 106, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001561, mean_q: 0.964550\n",
      " 4469/10000: episode: 361, duration: 0.089s, episode steps: 10, steps per second: 113, episode reward: 1.000, mean reward: 0.100 [0.000, 1.000], mean action: 1.500 [0.000, 3.000], mean observation: 6.500 [0.000, 15.000], loss: 0.001037, mean_q: 0.964286\n",
      " 4473/10000: episode: 362, duration: 0.040s, episode steps: 4, steps per second: 100, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.250 [0.000, 2.000], mean observation: 3.500 [1.000, 6.000], loss: 0.000858, mean_q: 0.964702\n",
      " 4479/10000: episode: 363, duration: 0.055s, episode steps: 6, steps per second: 109, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001581, mean_q: 0.963455\n",
      " 4485/10000: episode: 364, duration: 0.056s, episode steps: 6, steps per second: 107, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000872, mean_q: 0.963576\n",
      " 4493/10000: episode: 365, duration: 0.072s, episode steps: 8, steps per second: 112, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.625 [1.000, 3.000], mean observation: 7.000 [1.000, 15.000], loss: 0.001389, mean_q: 0.963256\n",
      " 4499/10000: episode: 366, duration: 0.055s, episode steps: 6, steps per second: 110, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001863, mean_q: 0.964217\n",
      " 4505/10000: episode: 367, duration: 0.054s, episode steps: 6, steps per second: 111, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.833 [1.000, 3.000], mean observation: 3.333 [0.000, 7.000], loss: 0.001142, mean_q: 0.964636\n",
      " 4515/10000: episode: 368, duration: 0.085s, episode steps: 10, steps per second: 117, episode reward: 1.000, mean reward: 0.100 [0.000, 1.000], mean action: 1.300 [0.000, 2.000], mean observation: 5.600 [1.000, 15.000], loss: 0.000791, mean_q: 0.965010\n",
      " 4523/10000: episode: 369, duration: 0.071s, episode steps: 8, steps per second: 112, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.375 [0.000, 2.000], mean observation: 6.375 [1.000, 15.000], loss: 0.000970, mean_q: 0.964921\n",
      " 4529/10000: episode: 370, duration: 0.056s, episode steps: 6, steps per second: 108, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001002, mean_q: 0.964742\n",
      " 4537/10000: episode: 371, duration: 0.070s, episode steps: 8, steps per second: 114, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.625 [1.000, 3.000], mean observation: 7.000 [1.000, 15.000], loss: 0.000862, mean_q: 0.964447\n",
      " 4544/10000: episode: 372, duration: 0.063s, episode steps: 7, steps per second: 112, episode reward: 1.000, mean reward: 0.143 [0.000, 1.000], mean action: 1.714 [1.000, 3.000], mean observation: 7.143 [1.000, 15.000], loss: 0.000617, mean_q: 0.964451\n",
      " 4550/10000: episode: 373, duration: 0.057s, episode steps: 6, steps per second: 106, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.002981, mean_q: 0.964663\n",
      " 4556/10000: episode: 374, duration: 0.055s, episode steps: 6, steps per second: 110, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000711, mean_q: 0.964035\n",
      " 4562/10000: episode: 375, duration: 0.056s, episode steps: 6, steps per second: 107, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.002008, mean_q: 0.964206\n",
      " 4568/10000: episode: 376, duration: 0.055s, episode steps: 6, steps per second: 109, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001289, mean_q: 0.964033\n",
      " 4578/10000: episode: 377, duration: 0.090s, episode steps: 10, steps per second: 112, episode reward: 1.000, mean reward: 0.100 [0.000, 1.000], mean action: 1.500 [0.000, 3.000], mean observation: 7.300 [0.000, 15.000], loss: 0.001386, mean_q: 0.963150\n",
      " 4583/10000: episode: 378, duration: 0.048s, episode steps: 5, steps per second: 104, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.600 [1.000, 2.000], mean observation: 6.000 [1.000, 11.000], loss: 0.000882, mean_q: 0.963326\n",
      " 4589/10000: episode: 379, duration: 0.055s, episode steps: 6, steps per second: 109, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.002268, mean_q: 0.964394\n",
      " 4595/10000: episode: 380, duration: 0.055s, episode steps: 6, steps per second: 109, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000718, mean_q: 0.964468\n",
      " 4601/10000: episode: 381, duration: 0.056s, episode steps: 6, steps per second: 107, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001458, mean_q: 0.964046\n",
      " 4607/10000: episode: 382, duration: 0.055s, episode steps: 6, steps per second: 110, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001566, mean_q: 0.964447\n",
      " 4612/10000: episode: 383, duration: 0.047s, episode steps: 5, steps per second: 107, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.600 [1.000, 2.000], mean observation: 6.000 [1.000, 11.000], loss: 0.001206, mean_q: 0.964211\n",
      " 4618/10000: episode: 384, duration: 0.054s, episode steps: 6, steps per second: 111, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001422, mean_q: 0.963858\n",
      " 4624/10000: episode: 385, duration: 0.055s, episode steps: 6, steps per second: 109, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001131, mean_q: 0.964226\n",
      " 4630/10000: episode: 386, duration: 0.055s, episode steps: 6, steps per second: 108, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000855, mean_q: 0.964962\n",
      " 4636/10000: episode: 387, duration: 0.054s, episode steps: 6, steps per second: 111, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001150, mean_q: 0.965084\n",
      " 4642/10000: episode: 388, duration: 0.054s, episode steps: 6, steps per second: 110, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.002406, mean_q: 0.964781\n",
      " 4648/10000: episode: 389, duration: 0.117s, episode steps: 6, steps per second: 51, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001122, mean_q: 0.963973\n",
      " 4654/10000: episode: 390, duration: 0.055s, episode steps: 6, steps per second: 110, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000857, mean_q: 0.964873\n",
      " 4660/10000: episode: 391, duration: 0.055s, episode steps: 6, steps per second: 108, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.833 [1.000, 3.000], mean observation: 5.333 [1.000, 11.000], loss: 0.000449, mean_q: 0.964768\n",
      " 4667/10000: episode: 392, duration: 0.064s, episode steps: 7, steps per second: 110, episode reward: 1.000, mean reward: 0.143 [0.000, 1.000], mean action: 1.429 [1.000, 2.000], mean observation: 8.857 [1.000, 15.000], loss: 0.002071, mean_q: 0.965029\n",
      " 4673/10000: episode: 393, duration: 0.056s, episode steps: 6, steps per second: 108, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001292, mean_q: 0.964269\n",
      " 4679/10000: episode: 394, duration: 0.056s, episode steps: 6, steps per second: 108, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001290, mean_q: 0.964246\n",
      " 4684/10000: episode: 395, duration: 0.048s, episode steps: 5, steps per second: 104, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.600 [1.000, 2.000], mean observation: 6.000 [1.000, 11.000], loss: 0.001559, mean_q: 0.964414\n",
      " 4690/10000: episode: 396, duration: 0.055s, episode steps: 6, steps per second: 109, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001971, mean_q: 0.963645\n",
      " 4696/10000: episode: 397, duration: 0.057s, episode steps: 6, steps per second: 106, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000870, mean_q: 0.964075\n",
      " 4704/10000: episode: 398, duration: 0.070s, episode steps: 8, steps per second: 114, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.625 [1.000, 3.000], mean observation: 7.000 [1.000, 15.000], loss: 0.000751, mean_q: 0.965390\n",
      " 4712/10000: episode: 399, duration: 0.070s, episode steps: 8, steps per second: 114, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.625 [1.000, 3.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000967, mean_q: 0.965981\n",
      " 4718/10000: episode: 400, duration: 0.054s, episode steps: 6, steps per second: 110, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001139, mean_q: 0.966435\n",
      " 4727/10000: episode: 401, duration: 0.080s, episode steps: 9, steps per second: 113, episode reward: 1.000, mean reward: 0.111 [0.000, 1.000], mean action: 1.222 [0.000, 2.000], mean observation: 8.333 [0.000, 15.000], loss: 0.000600, mean_q: 0.966289\n",
      " 4733/10000: episode: 402, duration: 0.055s, episode steps: 6, steps per second: 110, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001001, mean_q: 0.965023\n",
      " 4739/10000: episode: 403, duration: 0.055s, episode steps: 6, steps per second: 110, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001703, mean_q: 0.965195\n",
      " 4745/10000: episode: 404, duration: 0.055s, episode steps: 6, steps per second: 110, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000985, mean_q: 0.964224\n",
      " 4751/10000: episode: 405, duration: 0.056s, episode steps: 6, steps per second: 107, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 10.500 [4.000, 15.000], loss: 0.001712, mean_q: 0.964698\n",
      " 4762/10000: episode: 406, duration: 0.094s, episode steps: 11, steps per second: 117, episode reward: 1.000, mean reward: 0.091 [0.000, 1.000], mean action: 1.182 [0.000, 2.000], mean observation: 6.909 [0.000, 15.000], loss: 0.001488, mean_q: 0.965643\n",
      " 4770/10000: episode: 407, duration: 0.071s, episode steps: 8, steps per second: 113, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.375 [0.000, 2.000], mean observation: 6.625 [1.000, 15.000], loss: 0.001698, mean_q: 0.965685\n",
      " 4772/10000: episode: 408, duration: 0.023s, episode steps: 2, steps per second: 86, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.500 [1.000, 2.000], mean observation: 3.000 [1.000, 5.000], loss: 0.001303, mean_q: 0.965429\n",
      " 4779/10000: episode: 409, duration: 0.065s, episode steps: 7, steps per second: 108, episode reward: 1.000, mean reward: 0.143 [0.000, 1.000], mean action: 1.714 [1.000, 3.000], mean observation: 6.857 [0.000, 15.000], loss: 0.000885, mean_q: 0.965772\n",
      " 4785/10000: episode: 410, duration: 0.055s, episode steps: 6, steps per second: 109, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001564, mean_q: 0.964229\n",
      " 4791/10000: episode: 411, duration: 0.057s, episode steps: 6, steps per second: 105, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001021, mean_q: 0.964720\n",
      " 4799/10000: episode: 412, duration: 0.071s, episode steps: 8, steps per second: 112, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.625 [1.000, 3.000], mean observation: 7.000 [1.000, 15.000], loss: 0.001806, mean_q: 0.964229\n",
      " 4805/10000: episode: 413, duration: 0.057s, episode steps: 6, steps per second: 106, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001319, mean_q: 0.964607\n",
      " 4811/10000: episode: 414, duration: 0.056s, episode steps: 6, steps per second: 108, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001142, mean_q: 0.963751\n",
      " 4818/10000: episode: 415, duration: 0.062s, episode steps: 7, steps per second: 112, episode reward: 1.000, mean reward: 0.143 [0.000, 1.000], mean action: 1.714 [1.000, 3.000], mean observation: 9.000 [0.000, 15.000], loss: 0.001721, mean_q: 0.963282\n",
      " 4824/10000: episode: 416, duration: 0.113s, episode steps: 6, steps per second: 53, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001004, mean_q: 0.963271\n",
      " 4833/10000: episode: 417, duration: 0.082s, episode steps: 9, steps per second: 110, episode reward: 1.000, mean reward: 0.111 [0.000, 1.000], mean action: 1.778 [1.000, 3.000], mean observation: 7.222 [1.000, 15.000], loss: 0.001145, mean_q: 0.964689\n",
      " 4839/10000: episode: 418, duration: 0.054s, episode steps: 6, steps per second: 111, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001006, mean_q: 0.964843\n",
      " 4845/10000: episode: 419, duration: 0.055s, episode steps: 6, steps per second: 110, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001284, mean_q: 0.964674\n",
      " 4851/10000: episode: 420, duration: 0.056s, episode steps: 6, steps per second: 108, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001991, mean_q: 0.965127\n",
      " 4853/10000: episode: 421, duration: 0.024s, episode steps: 2, steps per second: 83, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.500 [1.000, 2.000], mean observation: 3.000 [1.000, 5.000], loss: 0.000864, mean_q: 0.963914\n",
      " 4859/10000: episode: 422, duration: 0.054s, episode steps: 6, steps per second: 111, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001008, mean_q: 0.964729\n",
      " 4865/10000: episode: 423, duration: 0.055s, episode steps: 6, steps per second: 110, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001292, mean_q: 0.964511\n",
      " 4871/10000: episode: 424, duration: 0.056s, episode steps: 6, steps per second: 107, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000590, mean_q: 0.964184\n",
      " 4877/10000: episode: 425, duration: 0.057s, episode steps: 6, steps per second: 106, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001733, mean_q: 0.964265\n",
      " 4883/10000: episode: 426, duration: 0.055s, episode steps: 6, steps per second: 108, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001562, mean_q: 0.964220\n",
      " 4889/10000: episode: 427, duration: 0.057s, episode steps: 6, steps per second: 106, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001266, mean_q: 0.964984\n",
      " 4895/10000: episode: 428, duration: 0.055s, episode steps: 6, steps per second: 108, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000441, mean_q: 0.965059\n",
      " 4901/10000: episode: 429, duration: 0.057s, episode steps: 6, steps per second: 105, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001145, mean_q: 0.966032\n",
      " 4907/10000: episode: 430, duration: 0.055s, episode steps: 6, steps per second: 108, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.002008, mean_q: 0.965243\n",
      " 4913/10000: episode: 431, duration: 0.055s, episode steps: 6, steps per second: 110, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001136, mean_q: 0.965089\n",
      " 4919/10000: episode: 432, duration: 0.054s, episode steps: 6, steps per second: 110, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000441, mean_q: 0.965363\n",
      " 4925/10000: episode: 433, duration: 0.059s, episode steps: 6, steps per second: 101, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001420, mean_q: 0.965669\n",
      " 4934/10000: episode: 434, duration: 0.078s, episode steps: 9, steps per second: 116, episode reward: 1.000, mean reward: 0.111 [0.000, 1.000], mean action: 1.333 [0.000, 2.000], mean observation: 7.444 [1.000, 15.000], loss: 0.001331, mean_q: 0.965763\n",
      " 4940/10000: episode: 435, duration: 0.054s, episode steps: 6, steps per second: 110, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001740, mean_q: 0.966054\n",
      " 4945/10000: episode: 436, duration: 0.047s, episode steps: 5, steps per second: 107, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.600 [1.000, 2.000], mean observation: 6.000 [1.000, 11.000], loss: 0.000684, mean_q: 0.965469\n",
      " 4951/10000: episode: 437, duration: 0.057s, episode steps: 6, steps per second: 106, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001005, mean_q: 0.965232\n",
      " 4958/10000: episode: 438, duration: 0.062s, episode steps: 7, steps per second: 112, episode reward: 1.000, mean reward: 0.143 [0.000, 1.000], mean action: 1.429 [1.000, 2.000], mean observation: 8.857 [1.000, 15.000], loss: 0.001333, mean_q: 0.964981\n",
      " 4966/10000: episode: 439, duration: 0.070s, episode steps: 8, steps per second: 115, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.625 [1.000, 3.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001083, mean_q: 0.965265\n",
      " 4974/10000: episode: 440, duration: 0.071s, episode steps: 8, steps per second: 113, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.625 [1.000, 3.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001389, mean_q: 0.964848\n",
      " 4982/10000: episode: 441, duration: 0.074s, episode steps: 8, steps per second: 109, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.375 [0.000, 2.000], mean observation: 8.750 [1.000, 15.000], loss: 0.001159, mean_q: 0.964875\n",
      " 4986/10000: episode: 442, duration: 0.040s, episode steps: 4, steps per second: 99, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.750 [1.000, 2.000], mean observation: 4.000 [1.000, 7.000], loss: 0.001054, mean_q: 0.965004\n",
      " 4993/10000: episode: 443, duration: 0.062s, episode steps: 7, steps per second: 112, episode reward: 1.000, mean reward: 0.143 [0.000, 1.000], mean action: 1.714 [1.000, 3.000], mean observation: 7.000 [1.000, 15.000], loss: 0.001593, mean_q: 0.965048\n",
      " 4998/10000: episode: 444, duration: 0.048s, episode steps: 5, steps per second: 104, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.600 [1.000, 2.000], mean observation: 6.000 [1.000, 11.000], loss: 0.002546, mean_q: 0.965126\n",
      " 5004/10000: episode: 445, duration: 0.055s, episode steps: 6, steps per second: 109, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001158, mean_q: 0.964176\n",
      " 5014/10000: episode: 446, duration: 0.090s, episode steps: 10, steps per second: 111, episode reward: 1.000, mean reward: 0.100 [0.000, 1.000], mean action: 1.300 [0.000, 2.000], mean observation: 10.200 [1.000, 15.000], loss: 0.000614, mean_q: 0.964159\n",
      " 5020/10000: episode: 447, duration: 0.055s, episode steps: 6, steps per second: 109, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001849, mean_q: 0.965607\n",
      " 5024/10000: episode: 448, duration: 0.041s, episode steps: 4, steps per second: 98, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.250 [0.000, 2.000], mean observation: 3.500 [1.000, 6.000], loss: 0.001081, mean_q: 0.965585\n",
      " 5030/10000: episode: 449, duration: 0.055s, episode steps: 6, steps per second: 109, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001137, mean_q: 0.965542\n",
      " 5036/10000: episode: 450, duration: 0.054s, episode steps: 6, steps per second: 110, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.002130, mean_q: 0.965765\n",
      " 5042/10000: episode: 451, duration: 0.055s, episode steps: 6, steps per second: 109, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001432, mean_q: 0.965277\n",
      " 5048/10000: episode: 452, duration: 0.056s, episode steps: 6, steps per second: 108, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001145, mean_q: 0.964203\n",
      " 5054/10000: episode: 453, duration: 0.055s, episode steps: 6, steps per second: 110, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001018, mean_q: 0.964537\n",
      " 5060/10000: episode: 454, duration: 0.054s, episode steps: 6, steps per second: 110, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001282, mean_q: 0.964487\n",
      " 5068/10000: episode: 455, duration: 0.070s, episode steps: 8, steps per second: 114, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.625 [1.000, 3.000], mean observation: 7.000 [1.000, 15.000], loss: 0.000659, mean_q: 0.965179\n",
      " 5074/10000: episode: 456, duration: 0.057s, episode steps: 6, steps per second: 105, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001853, mean_q: 0.965241\n",
      " 5083/10000: episode: 457, duration: 0.078s, episode steps: 9, steps per second: 115, episode reward: 1.000, mean reward: 0.111 [0.000, 1.000], mean action: 1.444 [0.000, 3.000], mean observation: 6.222 [0.000, 15.000], loss: 0.000489, mean_q: 0.965488\n",
      " 5089/10000: episode: 458, duration: 0.055s, episode steps: 6, steps per second: 109, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001572, mean_q: 0.965379\n",
      " 5095/10000: episode: 459, duration: 0.055s, episode steps: 6, steps per second: 109, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001293, mean_q: 0.965711\n",
      " 5101/10000: episode: 460, duration: 0.058s, episode steps: 6, steps per second: 104, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001835, mean_q: 0.965347\n",
      " 5107/10000: episode: 461, duration: 0.056s, episode steps: 6, steps per second: 106, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001860, mean_q: 0.965544\n",
      " 5117/10000: episode: 462, duration: 0.086s, episode steps: 10, steps per second: 116, episode reward: 1.000, mean reward: 0.100 [0.000, 1.000], mean action: 1.300 [0.000, 2.000], mean observation: 7.100 [0.000, 15.000], loss: 0.001032, mean_q: 0.965037\n",
      " 5123/10000: episode: 463, duration: 0.055s, episode steps: 6, steps per second: 109, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001693, mean_q: 0.965937\n",
      " 5129/10000: episode: 464, duration: 0.057s, episode steps: 6, steps per second: 106, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000717, mean_q: 0.966251\n",
      " 5135/10000: episode: 465, duration: 0.055s, episode steps: 6, steps per second: 109, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001719, mean_q: 0.966407\n",
      " 5137/10000: episode: 466, duration: 0.023s, episode steps: 2, steps per second: 85, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.500 [1.000, 2.000], mean observation: 3.000 [1.000, 5.000], loss: 0.000019, mean_q: 0.966075\n",
      " 5143/10000: episode: 467, duration: 0.054s, episode steps: 6, steps per second: 111, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001151, mean_q: 0.965531\n",
      " 5151/10000: episode: 468, duration: 0.073s, episode steps: 8, steps per second: 110, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.375 [0.000, 2.000], mean observation: 6.375 [1.000, 15.000], loss: 0.001079, mean_q: 0.965502\n",
      " 5157/10000: episode: 469, duration: 0.116s, episode steps: 6, steps per second: 52, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000878, mean_q: 0.965513\n",
      " 5163/10000: episode: 470, duration: 0.055s, episode steps: 6, steps per second: 110, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001299, mean_q: 0.966024\n",
      " 5169/10000: episode: 471, duration: 0.057s, episode steps: 6, steps per second: 106, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000721, mean_q: 0.965031\n",
      " 5175/10000: episode: 472, duration: 0.055s, episode steps: 6, steps per second: 109, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000435, mean_q: 0.965604\n",
      " 5182/10000: episode: 473, duration: 0.062s, episode steps: 7, steps per second: 113, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.429 [0.000, 2.000], mean observation: 4.714 [1.000, 11.000], loss: 0.001590, mean_q: 0.965540\n",
      " 5188/10000: episode: 474, duration: 0.055s, episode steps: 6, steps per second: 110, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001556, mean_q: 0.965651\n",
      " 5196/10000: episode: 475, duration: 0.072s, episode steps: 8, steps per second: 111, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.625 [1.000, 3.000], mean observation: 7.000 [1.000, 15.000], loss: 0.001602, mean_q: 0.966044\n",
      " 5204/10000: episode: 476, duration: 0.070s, episode steps: 8, steps per second: 114, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.625 [1.000, 3.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001278, mean_q: 0.966444\n",
      " 5210/10000: episode: 477, duration: 0.056s, episode steps: 6, steps per second: 106, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000450, mean_q: 0.966461\n",
      " 5216/10000: episode: 478, duration: 0.055s, episode steps: 6, steps per second: 109, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001287, mean_q: 0.966560\n",
      " 5222/10000: episode: 479, duration: 0.057s, episode steps: 6, steps per second: 105, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000990, mean_q: 0.965819\n",
      " 5228/10000: episode: 480, duration: 0.055s, episode steps: 6, steps per second: 109, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001433, mean_q: 0.965516\n",
      " 5236/10000: episode: 481, duration: 0.070s, episode steps: 8, steps per second: 114, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.375 [0.000, 2.000], mean observation: 6.375 [1.000, 15.000], loss: 0.000753, mean_q: 0.965550\n",
      " 5242/10000: episode: 482, duration: 0.060s, episode steps: 6, steps per second: 101, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001430, mean_q: 0.966466\n",
      " 5248/10000: episode: 483, duration: 0.056s, episode steps: 6, steps per second: 106, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001151, mean_q: 0.966801\n",
      " 5254/10000: episode: 484, duration: 0.055s, episode steps: 6, steps per second: 109, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001021, mean_q: 0.966918\n",
      " 5261/10000: episode: 485, duration: 0.063s, episode steps: 7, steps per second: 112, episode reward: 1.000, mean reward: 0.143 [0.000, 1.000], mean action: 1.714 [1.000, 3.000], mean observation: 6.857 [0.000, 15.000], loss: 0.001710, mean_q: 0.965485\n",
      " 5267/10000: episode: 486, duration: 0.054s, episode steps: 6, steps per second: 111, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000855, mean_q: 0.964272\n",
      " 5273/10000: episode: 487, duration: 0.057s, episode steps: 6, steps per second: 105, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001131, mean_q: 0.965350\n",
      " 5279/10000: episode: 488, duration: 0.055s, episode steps: 6, steps per second: 110, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000715, mean_q: 0.966597\n",
      " 5287/10000: episode: 489, duration: 0.070s, episode steps: 8, steps per second: 114, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.375 [0.000, 2.000], mean observation: 9.375 [1.000, 15.000], loss: 0.000958, mean_q: 0.967036\n",
      " 5292/10000: episode: 490, duration: 0.047s, episode steps: 5, steps per second: 106, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 2.000 [1.000, 3.000], mean observation: 3.200 [0.000, 7.000], loss: 0.000686, mean_q: 0.966829\n",
      " 5298/10000: episode: 491, duration: 0.056s, episode steps: 6, steps per second: 106, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000857, mean_q: 0.966849\n",
      " 5306/10000: episode: 492, duration: 0.071s, episode steps: 8, steps per second: 112, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.375 [0.000, 2.000], mean observation: 6.625 [1.000, 15.000], loss: 0.001180, mean_q: 0.966312\n",
      " 5316/10000: episode: 493, duration: 0.088s, episode steps: 10, steps per second: 114, episode reward: 1.000, mean reward: 0.100 [0.000, 1.000], mean action: 1.500 [0.000, 3.000], mean observation: 9.700 [1.000, 15.000], loss: 0.001202, mean_q: 0.966231\n",
      " 5324/10000: episode: 494, duration: 0.073s, episode steps: 8, steps per second: 110, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.625 [1.000, 3.000], mean observation: 6.500 [0.000, 15.000], loss: 0.001393, mean_q: 0.966273\n",
      " 5330/10000: episode: 495, duration: 0.055s, episode steps: 6, steps per second: 109, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001020, mean_q: 0.965739\n",
      " 5336/10000: episode: 496, duration: 0.114s, episode steps: 6, steps per second: 53, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001135, mean_q: 0.965195\n",
      " 5342/10000: episode: 497, duration: 0.056s, episode steps: 6, steps per second: 107, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001021, mean_q: 0.966415\n",
      " 5348/10000: episode: 498, duration: 0.056s, episode steps: 6, steps per second: 107, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000845, mean_q: 0.966266\n",
      " 5356/10000: episode: 499, duration: 0.070s, episode steps: 8, steps per second: 115, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.375 [0.000, 2.000], mean observation: 9.375 [1.000, 15.000], loss: 0.000859, mean_q: 0.966592\n",
      " 5365/10000: episode: 500, duration: 0.078s, episode steps: 9, steps per second: 116, episode reward: 1.000, mean reward: 0.111 [0.000, 1.000], mean action: 1.778 [1.000, 3.000], mean observation: 6.444 [1.000, 15.000], loss: 0.001150, mean_q: 0.967220\n",
      " 5371/10000: episode: 501, duration: 0.056s, episode steps: 6, steps per second: 106, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 10.500 [4.000, 15.000], loss: 0.000585, mean_q: 0.966998\n",
      " 5377/10000: episode: 502, duration: 0.054s, episode steps: 6, steps per second: 110, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001003, mean_q: 0.966113\n",
      " 5383/10000: episode: 503, duration: 0.055s, episode steps: 6, steps per second: 110, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001011, mean_q: 0.967108\n",
      " 5389/10000: episode: 504, duration: 0.058s, episode steps: 6, steps per second: 104, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001422, mean_q: 0.966896\n",
      " 5395/10000: episode: 505, duration: 0.055s, episode steps: 6, steps per second: 109, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001863, mean_q: 0.966188\n",
      " 5401/10000: episode: 506, duration: 0.057s, episode steps: 6, steps per second: 106, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000870, mean_q: 0.966563\n",
      " 5406/10000: episode: 507, duration: 0.047s, episode steps: 5, steps per second: 107, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.600 [1.000, 2.000], mean observation: 6.000 [1.000, 11.000], loss: 0.000527, mean_q: 0.966452\n",
      " 5412/10000: episode: 508, duration: 0.057s, episode steps: 6, steps per second: 105, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.002283, mean_q: 0.966700\n",
      " 5419/10000: episode: 509, duration: 0.065s, episode steps: 7, steps per second: 108, episode reward: 1.000, mean reward: 0.143 [0.000, 1.000], mean action: 1.714 [1.000, 3.000], mean observation: 7.000 [1.000, 15.000], loss: 0.001213, mean_q: 0.965318\n",
      " 5425/10000: episode: 510, duration: 0.055s, episode steps: 6, steps per second: 108, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000575, mean_q: 0.965865\n",
      " 5431/10000: episode: 511, duration: 0.056s, episode steps: 6, steps per second: 108, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000995, mean_q: 0.965321\n",
      " 5437/10000: episode: 512, duration: 0.056s, episode steps: 6, steps per second: 107, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001140, mean_q: 0.966661\n",
      " 5443/10000: episode: 513, duration: 0.055s, episode steps: 6, steps per second: 109, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001158, mean_q: 0.966919\n",
      " 5449/10000: episode: 514, duration: 0.055s, episode steps: 6, steps per second: 110, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000995, mean_q: 0.966873\n",
      " 5455/10000: episode: 515, duration: 0.055s, episode steps: 6, steps per second: 110, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001431, mean_q: 0.966442\n",
      " 5461/10000: episode: 516, duration: 0.055s, episode steps: 6, steps per second: 108, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000720, mean_q: 0.966295\n",
      " 5468/10000: episode: 517, duration: 0.063s, episode steps: 7, steps per second: 111, episode reward: 1.000, mean reward: 0.143 [0.000, 1.000], mean action: 1.429 [1.000, 2.000], mean observation: 8.857 [1.000, 15.000], loss: 0.000264, mean_q: 0.966513\n",
      " 5473/10000: episode: 518, duration: 0.047s, episode steps: 5, steps per second: 107, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.600 [1.000, 2.000], mean observation: 6.000 [1.000, 11.000], loss: 0.000696, mean_q: 0.966877\n",
      " 5481/10000: episode: 519, duration: 0.070s, episode steps: 8, steps per second: 115, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.375 [0.000, 2.000], mean observation: 9.375 [1.000, 15.000], loss: 0.000551, mean_q: 0.966045\n",
      " 5487/10000: episode: 520, duration: 0.056s, episode steps: 6, steps per second: 108, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000992, mean_q: 0.965706\n",
      " 5495/10000: episode: 521, duration: 0.070s, episode steps: 8, steps per second: 114, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.625 [1.000, 3.000], mean observation: 7.000 [1.000, 15.000], loss: 0.000863, mean_q: 0.967253\n",
      " 5501/10000: episode: 522, duration: 0.054s, episode steps: 6, steps per second: 111, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001286, mean_q: 0.967492\n",
      " 5508/10000: episode: 523, duration: 0.064s, episode steps: 7, steps per second: 110, episode reward: 1.000, mean reward: 0.143 [0.000, 1.000], mean action: 1.429 [1.000, 2.000], mean observation: 8.857 [1.000, 15.000], loss: 0.000625, mean_q: 0.967901\n",
      " 5514/10000: episode: 524, duration: 0.056s, episode steps: 6, steps per second: 108, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000867, mean_q: 0.967012\n",
      " 5520/10000: episode: 525, duration: 0.055s, episode steps: 6, steps per second: 108, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000875, mean_q: 0.966624\n",
      " 5526/10000: episode: 526, duration: 0.055s, episode steps: 6, steps per second: 110, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000872, mean_q: 0.966493\n",
      " 5532/10000: episode: 527, duration: 0.057s, episode steps: 6, steps per second: 106, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000442, mean_q: 0.967123\n",
      " 5538/10000: episode: 528, duration: 0.071s, episode steps: 6, steps per second: 84, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 10.500 [4.000, 15.000], loss: 0.000857, mean_q: 0.966778\n",
      " 5546/10000: episode: 529, duration: 0.088s, episode steps: 8, steps per second: 90, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.375 [0.000, 2.000], mean observation: 8.750 [1.000, 15.000], loss: 0.001077, mean_q: 0.967216\n",
      " 5552/10000: episode: 530, duration: 0.054s, episode steps: 6, steps per second: 111, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000574, mean_q: 0.967395\n",
      " 5558/10000: episode: 531, duration: 0.055s, episode steps: 6, steps per second: 110, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001013, mean_q: 0.967165\n",
      " 5562/10000: episode: 532, duration: 0.041s, episode steps: 4, steps per second: 98, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.250 [0.000, 2.000], mean observation: 3.500 [1.000, 6.000], loss: 0.000639, mean_q: 0.967926\n",
      " 5568/10000: episode: 533, duration: 0.054s, episode steps: 6, steps per second: 110, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000994, mean_q: 0.967717\n",
      " 5576/10000: episode: 534, duration: 0.070s, episode steps: 8, steps per second: 114, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.625 [1.000, 3.000], mean observation: 7.000 [1.000, 15.000], loss: 0.001497, mean_q: 0.967003\n",
      " 5582/10000: episode: 535, duration: 0.055s, episode steps: 6, steps per second: 109, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001426, mean_q: 0.966514\n",
      " 5588/10000: episode: 536, duration: 0.056s, episode steps: 6, steps per second: 108, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000572, mean_q: 0.966980\n",
      " 5590/10000: episode: 537, duration: 0.024s, episode steps: 2, steps per second: 85, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.500 [1.000, 2.000], mean observation: 3.000 [1.000, 5.000], loss: 0.000461, mean_q: 0.966748\n",
      " 5596/10000: episode: 538, duration: 0.054s, episode steps: 6, steps per second: 111, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001007, mean_q: 0.966590\n",
      " 5604/10000: episode: 539, duration: 0.073s, episode steps: 8, steps per second: 110, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.375 [0.000, 2.000], mean observation: 6.125 [0.000, 15.000], loss: 0.001710, mean_q: 0.965853\n",
      " 5610/10000: episode: 540, duration: 0.055s, episode steps: 6, steps per second: 110, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001718, mean_q: 0.966033\n",
      " 5615/10000: episode: 541, duration: 0.051s, episode steps: 5, steps per second: 99, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.600 [1.000, 2.000], mean observation: 6.000 [1.000, 11.000], loss: 0.000693, mean_q: 0.966100\n",
      " 5621/10000: episode: 542, duration: 0.054s, episode steps: 6, steps per second: 110, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000877, mean_q: 0.966855\n",
      " 5623/10000: episode: 543, duration: 0.024s, episode steps: 2, steps per second: 85, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.500 [1.000, 2.000], mean observation: 3.000 [1.000, 5.000], loss: 0.000854, mean_q: 0.966524\n",
      " 5625/10000: episode: 544, duration: 0.024s, episode steps: 2, steps per second: 85, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.500 [1.000, 2.000], mean observation: 3.000 [1.000, 5.000], loss: 0.001316, mean_q: 0.966954\n",
      " 5633/10000: episode: 545, duration: 0.072s, episode steps: 8, steps per second: 110, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.625 [1.000, 3.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000987, mean_q: 0.967065\n",
      " 5639/10000: episode: 546, duration: 0.055s, episode steps: 6, steps per second: 109, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000726, mean_q: 0.966697\n",
      " 5645/10000: episode: 547, duration: 0.055s, episode steps: 6, steps per second: 110, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000865, mean_q: 0.965963\n",
      " 5651/10000: episode: 548, duration: 0.055s, episode steps: 6, steps per second: 109, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000716, mean_q: 0.966161\n",
      " 5657/10000: episode: 549, duration: 0.056s, episode steps: 6, steps per second: 107, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001008, mean_q: 0.966338\n",
      " 5663/10000: episode: 550, duration: 0.112s, episode steps: 6, steps per second: 54, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001009, mean_q: 0.967069\n",
      " 5669/10000: episode: 551, duration: 0.058s, episode steps: 6, steps per second: 104, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001143, mean_q: 0.966743\n",
      " 5675/10000: episode: 552, duration: 0.056s, episode steps: 6, steps per second: 107, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000443, mean_q: 0.966731\n",
      " 5681/10000: episode: 553, duration: 0.055s, episode steps: 6, steps per second: 109, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001160, mean_q: 0.966664\n",
      " 5688/10000: episode: 554, duration: 0.062s, episode steps: 7, steps per second: 113, episode reward: 1.000, mean reward: 0.143 [0.000, 1.000], mean action: 1.714 [1.000, 3.000], mean observation: 6.857 [0.000, 15.000], loss: 0.000748, mean_q: 0.966366\n",
      " 5694/10000: episode: 555, duration: 0.054s, episode steps: 6, steps per second: 111, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001421, mean_q: 0.966511\n",
      " 5696/10000: episode: 556, duration: 0.023s, episode steps: 2, steps per second: 86, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.500 [1.000, 2.000], mean observation: 3.000 [1.000, 5.000], loss: 0.001705, mean_q: 0.966318\n",
      " 5704/10000: episode: 557, duration: 0.073s, episode steps: 8, steps per second: 110, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.375 [0.000, 2.000], mean observation: 9.375 [1.000, 15.000], loss: 0.000228, mean_q: 0.966881\n",
      " 5712/10000: episode: 558, duration: 0.072s, episode steps: 8, steps per second: 112, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.375 [0.000, 2.000], mean observation: 6.125 [0.000, 15.000], loss: 0.000861, mean_q: 0.966985\n",
      " 5718/10000: episode: 559, duration: 0.055s, episode steps: 6, steps per second: 110, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000435, mean_q: 0.966431\n",
      " 5724/10000: episode: 560, duration: 0.057s, episode steps: 6, steps per second: 105, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001419, mean_q: 0.966732\n",
      " 5730/10000: episode: 561, duration: 0.056s, episode steps: 6, steps per second: 107, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001292, mean_q: 0.966646\n",
      " 5737/10000: episode: 562, duration: 0.063s, episode steps: 7, steps per second: 111, episode reward: 1.000, mean reward: 0.143 [0.000, 1.000], mean action: 1.714 [1.000, 3.000], mean observation: 6.857 [0.000, 15.000], loss: 0.001340, mean_q: 0.966392\n",
      " 5743/10000: episode: 563, duration: 0.055s, episode steps: 6, steps per second: 110, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000868, mean_q: 0.967048\n",
      " 5751/10000: episode: 564, duration: 0.070s, episode steps: 8, steps per second: 114, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.625 [1.000, 3.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000754, mean_q: 0.966956\n",
      " 5757/10000: episode: 565, duration: 0.056s, episode steps: 6, steps per second: 107, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001151, mean_q: 0.967484\n",
      " 5763/10000: episode: 566, duration: 0.055s, episode steps: 6, steps per second: 109, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001590, mean_q: 0.967996\n",
      " 5769/10000: episode: 567, duration: 0.054s, episode steps: 6, steps per second: 110, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000728, mean_q: 0.966766\n",
      " 5777/10000: episode: 568, duration: 0.070s, episode steps: 8, steps per second: 114, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.875 [1.000, 3.000], mean observation: 6.000 [0.000, 15.000], loss: 0.001073, mean_q: 0.966806\n",
      " 5783/10000: episode: 569, duration: 0.057s, episode steps: 6, steps per second: 105, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000996, mean_q: 0.966355\n",
      " 5789/10000: episode: 570, duration: 0.055s, episode steps: 6, steps per second: 109, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001001, mean_q: 0.966238\n",
      " 5795/10000: episode: 571, duration: 0.055s, episode steps: 6, steps per second: 109, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000862, mean_q: 0.967128\n",
      " 5801/10000: episode: 572, duration: 0.054s, episode steps: 6, steps per second: 110, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001439, mean_q: 0.967455\n",
      " 5807/10000: episode: 573, duration: 0.057s, episode steps: 6, steps per second: 106, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000718, mean_q: 0.966918\n",
      " 5811/10000: episode: 574, duration: 0.040s, episode steps: 4, steps per second: 101, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.750 [1.000, 2.000], mean observation: 4.000 [1.000, 7.000], loss: 0.000859, mean_q: 0.966337\n",
      " 5817/10000: episode: 575, duration: 0.055s, episode steps: 6, steps per second: 110, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001025, mean_q: 0.967107\n",
      " 5825/10000: episode: 576, duration: 0.070s, episode steps: 8, steps per second: 114, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.375 [0.000, 2.000], mean observation: 6.625 [1.000, 15.000], loss: 0.000649, mean_q: 0.967373\n",
      " 5831/10000: episode: 577, duration: 0.057s, episode steps: 6, steps per second: 105, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.002010, mean_q: 0.967391\n",
      " 5837/10000: episode: 578, duration: 0.056s, episode steps: 6, steps per second: 108, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000873, mean_q: 0.966963\n",
      " 5843/10000: episode: 579, duration: 0.116s, episode steps: 6, steps per second: 52, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001301, mean_q: 0.966470\n",
      " 5849/10000: episode: 580, duration: 0.057s, episode steps: 6, steps per second: 106, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000713, mean_q: 0.966132\n",
      " 5855/10000: episode: 581, duration: 0.055s, episode steps: 6, steps per second: 108, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 10.500 [4.000, 15.000], loss: 0.001009, mean_q: 0.967098\n",
      " 5863/10000: episode: 582, duration: 0.070s, episode steps: 8, steps per second: 115, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.625 [1.000, 3.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000762, mean_q: 0.967270\n",
      " 5869/10000: episode: 583, duration: 0.057s, episode steps: 6, steps per second: 106, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001004, mean_q: 0.967132\n",
      " 5875/10000: episode: 584, duration: 0.056s, episode steps: 6, steps per second: 107, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000855, mean_q: 0.967061\n",
      " 5881/10000: episode: 585, duration: 0.055s, episode steps: 6, steps per second: 109, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001004, mean_q: 0.966964\n",
      " 5887/10000: episode: 586, duration: 0.055s, episode steps: 6, steps per second: 110, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001568, mean_q: 0.966689\n",
      " 5893/10000: episode: 587, duration: 0.055s, episode steps: 6, steps per second: 110, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001150, mean_q: 0.966587\n",
      " 5899/10000: episode: 588, duration: 0.057s, episode steps: 6, steps per second: 106, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000866, mean_q: 0.965851\n",
      " 5905/10000: episode: 589, duration: 0.055s, episode steps: 6, steps per second: 109, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000575, mean_q: 0.966174\n",
      " 5911/10000: episode: 590, duration: 0.054s, episode steps: 6, steps per second: 110, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000730, mean_q: 0.966996\n",
      " 5917/10000: episode: 591, duration: 0.054s, episode steps: 6, steps per second: 110, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000992, mean_q: 0.965685\n",
      " 5923/10000: episode: 592, duration: 0.056s, episode steps: 6, steps per second: 106, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000861, mean_q: 0.966696\n",
      " 5929/10000: episode: 593, duration: 0.056s, episode steps: 6, steps per second: 108, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 10.500 [4.000, 15.000], loss: 0.000303, mean_q: 0.967147\n",
      " 5935/10000: episode: 594, duration: 0.055s, episode steps: 6, steps per second: 109, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001160, mean_q: 0.968001\n",
      " 5945/10000: episode: 595, duration: 0.087s, episode steps: 10, steps per second: 115, episode reward: 1.000, mean reward: 0.100 [0.000, 1.000], mean action: 1.500 [0.000, 3.000], mean observation: 7.800 [1.000, 15.000], loss: 0.001283, mean_q: 0.968012\n",
      " 5951/10000: episode: 596, duration: 0.057s, episode steps: 6, steps per second: 105, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001141, mean_q: 0.966741\n",
      " 5957/10000: episode: 597, duration: 0.055s, episode steps: 6, steps per second: 108, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001005, mean_q: 0.966901\n",
      " 5965/10000: episode: 598, duration: 0.070s, episode steps: 8, steps per second: 114, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.375 [0.000, 2.000], mean observation: 8.750 [1.000, 15.000], loss: 0.001080, mean_q: 0.967390\n",
      " 5972/10000: episode: 599, duration: 0.064s, episode steps: 7, steps per second: 110, episode reward: 1.000, mean reward: 0.143 [0.000, 1.000], mean action: 1.429 [1.000, 2.000], mean observation: 8.857 [1.000, 15.000], loss: 0.000617, mean_q: 0.966796\n",
      " 5980/10000: episode: 600, duration: 0.072s, episode steps: 8, steps per second: 111, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.375 [0.000, 2.000], mean observation: 6.625 [1.000, 15.000], loss: 0.001293, mean_q: 0.967357\n",
      " 5988/10000: episode: 601, duration: 0.070s, episode steps: 8, steps per second: 114, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.625 [1.000, 3.000], mean observation: 7.000 [1.000, 15.000], loss: 0.000538, mean_q: 0.967433\n",
      " 5994/10000: episode: 602, duration: 0.054s, episode steps: 6, steps per second: 111, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000861, mean_q: 0.966743\n",
      " 6000/10000: episode: 603, duration: 0.055s, episode steps: 6, steps per second: 109, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000874, mean_q: 0.967170\n",
      " 6006/10000: episode: 604, duration: 0.056s, episode steps: 6, steps per second: 106, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.500 [0.000, 3.000], mean observation: 5.000 [1.000, 10.000], loss: 0.001009, mean_q: 0.967449\n",
      " 6014/10000: episode: 605, duration: 0.071s, episode steps: 8, steps per second: 113, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.375 [0.000, 2.000], mean observation: 9.375 [1.000, 15.000], loss: 0.000538, mean_q: 0.966850\n",
      " 6023/10000: episode: 606, duration: 0.078s, episode steps: 9, steps per second: 115, episode reward: 1.000, mean reward: 0.111 [0.000, 1.000], mean action: 1.556 [0.000, 3.000], mean observation: 5.556 [0.000, 15.000], loss: 0.001411, mean_q: 0.967442\n",
      " 6029/10000: episode: 607, duration: 0.055s, episode steps: 6, steps per second: 109, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001004, mean_q: 0.967426\n",
      " 6036/10000: episode: 608, duration: 0.065s, episode steps: 7, steps per second: 107, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.000 [0.000, 2.000], mean observation: 7.571 [1.000, 13.000], loss: 0.000997, mean_q: 0.967310\n",
      " 6042/10000: episode: 609, duration: 0.056s, episode steps: 6, steps per second: 106, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001287, mean_q: 0.966779\n",
      " 6048/10000: episode: 610, duration: 0.056s, episode steps: 6, steps per second: 107, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001026, mean_q: 0.966565\n",
      " 6055/10000: episode: 611, duration: 0.066s, episode steps: 7, steps per second: 106, episode reward: 1.000, mean reward: 0.143 [0.000, 1.000], mean action: 1.286 [0.000, 2.000], mean observation: 6.857 [0.000, 15.000], loss: 0.001220, mean_q: 0.966546\n",
      " 6062/10000: episode: 612, duration: 0.064s, episode steps: 7, steps per second: 109, episode reward: 1.000, mean reward: 0.143 [0.000, 1.000], mean action: 1.286 [0.000, 2.000], mean observation: 6.857 [0.000, 15.000], loss: 0.001337, mean_q: 0.966848\n",
      " 6068/10000: episode: 613, duration: 0.056s, episode steps: 6, steps per second: 108, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001281, mean_q: 0.967896\n",
      " 6074/10000: episode: 614, duration: 0.056s, episode steps: 6, steps per second: 108, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000869, mean_q: 0.967946\n",
      " 6080/10000: episode: 615, duration: 0.057s, episode steps: 6, steps per second: 106, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001130, mean_q: 0.967453\n",
      " 6086/10000: episode: 616, duration: 0.056s, episode steps: 6, steps per second: 107, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001006, mean_q: 0.966368\n",
      " 6093/10000: episode: 617, duration: 0.063s, episode steps: 7, steps per second: 111, episode reward: 1.000, mean reward: 0.143 [0.000, 1.000], mean action: 1.286 [0.000, 2.000], mean observation: 6.857 [0.000, 15.000], loss: 0.000618, mean_q: 0.966991\n",
      " 6104/10000: episode: 618, duration: 0.095s, episode steps: 11, steps per second: 116, episode reward: 1.000, mean reward: 0.091 [0.000, 1.000], mean action: 1.818 [0.000, 3.000], mean observation: 4.455 [0.000, 15.000], loss: 0.001020, mean_q: 0.966868\n",
      " 6110/10000: episode: 619, duration: 0.055s, episode steps: 6, steps per second: 108, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001577, mean_q: 0.967057\n",
      " 6116/10000: episode: 620, duration: 0.055s, episode steps: 6, steps per second: 108, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000706, mean_q: 0.967488\n",
      " 6124/10000: episode: 621, duration: 0.071s, episode steps: 8, steps per second: 112, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.375 [0.000, 2.000], mean observation: 6.125 [0.000, 15.000], loss: 0.001068, mean_q: 0.967629\n",
      " 6132/10000: episode: 622, duration: 0.075s, episode steps: 8, steps per second: 106, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.625 [1.000, 3.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001401, mean_q: 0.968204\n",
      " 6138/10000: episode: 623, duration: 0.055s, episode steps: 6, steps per second: 109, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001005, mean_q: 0.967925\n",
      " 6144/10000: episode: 624, duration: 0.056s, episode steps: 6, steps per second: 106, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001003, mean_q: 0.967608\n",
      " 6150/10000: episode: 625, duration: 0.057s, episode steps: 6, steps per second: 105, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001421, mean_q: 0.966947\n",
      " 6156/10000: episode: 626, duration: 0.057s, episode steps: 6, steps per second: 105, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000294, mean_q: 0.967418\n",
      " 6160/10000: episode: 627, duration: 0.041s, episode steps: 4, steps per second: 96, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.750 [1.000, 2.000], mean observation: 4.000 [1.000, 7.000], loss: 0.000660, mean_q: 0.966824\n",
      " 6166/10000: episode: 628, duration: 0.057s, episode steps: 6, steps per second: 106, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001297, mean_q: 0.967343\n",
      " 6176/10000: episode: 629, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 1.000, mean reward: 0.100 [0.000, 1.000], mean action: 1.700 [1.000, 3.000], mean observation: 7.600 [0.000, 15.000], loss: 0.000686, mean_q: 0.966888\n",
      " 6182/10000: episode: 630, duration: 0.056s, episode steps: 6, steps per second: 108, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000718, mean_q: 0.967458\n",
      " 6188/10000: episode: 631, duration: 0.056s, episode steps: 6, steps per second: 108, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001145, mean_q: 0.967974\n",
      " 6194/10000: episode: 632, duration: 0.055s, episode steps: 6, steps per second: 109, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001147, mean_q: 0.966850\n",
      " 6201/10000: episode: 633, duration: 0.064s, episode steps: 7, steps per second: 109, episode reward: 1.000, mean reward: 0.143 [0.000, 1.000], mean action: 1.429 [1.000, 2.000], mean observation: 8.857 [1.000, 15.000], loss: 0.001482, mean_q: 0.966713\n",
      " 6207/10000: episode: 634, duration: 0.059s, episode steps: 6, steps per second: 101, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001152, mean_q: 0.967204\n",
      " 6213/10000: episode: 635, duration: 0.056s, episode steps: 6, steps per second: 107, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001009, mean_q: 0.967038\n",
      " 6219/10000: episode: 636, duration: 0.057s, episode steps: 6, steps per second: 106, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000859, mean_q: 0.966936\n",
      " 6225/10000: episode: 637, duration: 0.057s, episode steps: 6, steps per second: 105, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001132, mean_q: 0.966627\n",
      " 6232/10000: episode: 638, duration: 0.064s, episode steps: 7, steps per second: 110, episode reward: 1.000, mean reward: 0.143 [0.000, 1.000], mean action: 1.286 [0.000, 2.000], mean observation: 6.857 [0.000, 15.000], loss: 0.000862, mean_q: 0.967692\n",
      " 6237/10000: episode: 639, duration: 0.048s, episode steps: 5, steps per second: 103, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.600 [1.000, 2.000], mean observation: 6.000 [1.000, 11.000], loss: 0.001028, mean_q: 0.967893\n",
      " 6243/10000: episode: 640, duration: 0.056s, episode steps: 6, steps per second: 107, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000587, mean_q: 0.968066\n",
      " 6249/10000: episode: 641, duration: 0.057s, episode steps: 6, steps per second: 105, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001288, mean_q: 0.967315\n",
      " 6255/10000: episode: 642, duration: 0.056s, episode steps: 6, steps per second: 106, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000997, mean_q: 0.967502\n",
      " 6263/10000: episode: 643, duration: 0.072s, episode steps: 8, steps per second: 111, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.625 [1.000, 3.000], mean observation: 9.000 [1.000, 15.000], loss: 0.000964, mean_q: 0.968000\n",
      " 6269/10000: episode: 644, duration: 0.056s, episode steps: 6, steps per second: 107, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001014, mean_q: 0.967210\n",
      " 6278/10000: episode: 645, duration: 0.081s, episode steps: 9, steps per second: 111, episode reward: 1.000, mean reward: 0.111 [0.000, 1.000], mean action: 1.333 [0.000, 2.000], mean observation: 9.889 [1.000, 15.000], loss: 0.001237, mean_q: 0.967680\n",
      " 6284/10000: episode: 646, duration: 0.057s, episode steps: 6, steps per second: 106, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001278, mean_q: 0.968448\n",
      " 6290/10000: episode: 647, duration: 0.055s, episode steps: 6, steps per second: 108, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001304, mean_q: 0.967921\n",
      " 6298/10000: episode: 648, duration: 0.072s, episode steps: 8, steps per second: 112, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.375 [0.000, 2.000], mean observation: 6.375 [1.000, 15.000], loss: 0.000856, mean_q: 0.968239\n",
      " 6304/10000: episode: 649, duration: 0.057s, episode steps: 6, steps per second: 105, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001013, mean_q: 0.967463\n",
      " 6312/10000: episode: 650, duration: 0.071s, episode steps: 8, steps per second: 112, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.375 [0.000, 2.000], mean observation: 6.625 [1.000, 15.000], loss: 0.001066, mean_q: 0.967674\n",
      " 6318/10000: episode: 651, duration: 0.056s, episode steps: 6, steps per second: 108, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001138, mean_q: 0.967160\n",
      " 6324/10000: episode: 652, duration: 0.055s, episode steps: 6, steps per second: 109, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000305, mean_q: 0.967346\n",
      " 6330/10000: episode: 653, duration: 0.057s, episode steps: 6, steps per second: 104, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000579, mean_q: 0.967823\n",
      " 6338/10000: episode: 654, duration: 0.071s, episode steps: 8, steps per second: 112, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.625 [1.000, 3.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000868, mean_q: 0.968251\n",
      " 6344/10000: episode: 655, duration: 0.056s, episode steps: 6, steps per second: 108, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000727, mean_q: 0.967492\n",
      " 6350/10000: episode: 656, duration: 0.114s, episode steps: 6, steps per second: 53, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001448, mean_q: 0.967008\n",
      " 6356/10000: episode: 657, duration: 0.065s, episode steps: 6, steps per second: 93, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000730, mean_q: 0.967730\n",
      " 6362/10000: episode: 658, duration: 0.057s, episode steps: 6, steps per second: 106, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001162, mean_q: 0.967170\n",
      " 6368/10000: episode: 659, duration: 0.056s, episode steps: 6, steps per second: 107, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001156, mean_q: 0.966824\n",
      " 6374/10000: episode: 660, duration: 0.059s, episode steps: 6, steps per second: 102, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000867, mean_q: 0.966540\n",
      " 6380/10000: episode: 661, duration: 0.056s, episode steps: 6, steps per second: 106, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000858, mean_q: 0.966662\n",
      " 6388/10000: episode: 662, duration: 0.072s, episode steps: 8, steps per second: 111, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.625 [1.000, 3.000], mean observation: 6.500 [0.000, 15.000], loss: 0.001075, mean_q: 0.966980\n",
      " 6394/10000: episode: 663, duration: 0.055s, episode steps: 6, steps per second: 109, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001136, mean_q: 0.967123\n",
      " 6404/10000: episode: 664, duration: 0.088s, episode steps: 10, steps per second: 113, episode reward: 1.000, mean reward: 0.100 [0.000, 1.000], mean action: 1.400 [0.000, 3.000], mean observation: 7.200 [0.000, 15.000], loss: 0.000858, mean_q: 0.967621\n",
      " 6409/10000: episode: 665, duration: 0.047s, episode steps: 5, steps per second: 106, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.600 [1.000, 2.000], mean observation: 6.000 [1.000, 11.000], loss: 0.001706, mean_q: 0.967129\n",
      " 6417/10000: episode: 666, duration: 0.072s, episode steps: 8, steps per second: 112, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.625 [1.000, 3.000], mean observation: 9.000 [1.000, 15.000], loss: 0.001065, mean_q: 0.967026\n",
      " 6425/10000: episode: 667, duration: 0.072s, episode steps: 8, steps per second: 112, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.625 [1.000, 3.000], mean observation: 7.000 [1.000, 15.000], loss: 0.001174, mean_q: 0.967269\n",
      " 6431/10000: episode: 668, duration: 0.057s, episode steps: 6, steps per second: 106, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000439, mean_q: 0.968049\n",
      " 6439/10000: episode: 669, duration: 0.072s, episode steps: 8, steps per second: 111, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.875 [1.000, 3.000], mean observation: 6.000 [0.000, 15.000], loss: 0.001087, mean_q: 0.967692\n",
      " 6445/10000: episode: 670, duration: 0.056s, episode steps: 6, steps per second: 107, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001283, mean_q: 0.967466\n",
      " 6451/10000: episode: 671, duration: 0.056s, episode steps: 6, steps per second: 107, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001157, mean_q: 0.967027\n",
      " 6457/10000: episode: 672, duration: 0.057s, episode steps: 6, steps per second: 106, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001003, mean_q: 0.967155\n",
      " 6459/10000: episode: 673, duration: 0.024s, episode steps: 2, steps per second: 84, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.500 [1.000, 2.000], mean observation: 3.000 [1.000, 5.000], loss: 0.000435, mean_q: 0.966530\n",
      " 6465/10000: episode: 674, duration: 0.057s, episode steps: 6, steps per second: 106, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001013, mean_q: 0.967581\n",
      " 6471/10000: episode: 675, duration: 0.057s, episode steps: 6, steps per second: 106, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000729, mean_q: 0.966824\n",
      " 6475/10000: episode: 676, duration: 0.040s, episode steps: 4, steps per second: 100, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.250 [0.000, 2.000], mean observation: 3.500 [1.000, 6.000], loss: 0.001084, mean_q: 0.966631\n",
      " 6477/10000: episode: 677, duration: 0.024s, episode steps: 2, steps per second: 82, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.500 [1.000, 2.000], mean observation: 3.000 [1.000, 5.000], loss: 0.002100, mean_q: 0.968132\n",
      " 6483/10000: episode: 678, duration: 0.058s, episode steps: 6, steps per second: 104, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000722, mean_q: 0.967300\n",
      " 6491/10000: episode: 679, duration: 0.071s, episode steps: 8, steps per second: 113, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.625 [1.000, 3.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000853, mean_q: 0.967627\n",
      " 6497/10000: episode: 680, duration: 0.056s, episode steps: 6, steps per second: 107, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000714, mean_q: 0.967857\n",
      " 6503/10000: episode: 681, duration: 0.056s, episode steps: 6, steps per second: 108, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000719, mean_q: 0.968402\n",
      " 6511/10000: episode: 682, duration: 0.074s, episode steps: 8, steps per second: 108, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.375 [0.000, 2.000], mean observation: 9.375 [1.000, 15.000], loss: 0.001273, mean_q: 0.967619\n",
      " 6515/10000: episode: 683, duration: 0.040s, episode steps: 4, steps per second: 101, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.750 [1.000, 2.000], mean observation: 4.000 [1.000, 7.000], loss: 0.001729, mean_q: 0.967485\n",
      " 6521/10000: episode: 684, duration: 0.056s, episode steps: 6, steps per second: 107, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000866, mean_q: 0.968103\n",
      " 6527/10000: episode: 685, duration: 0.057s, episode steps: 6, steps per second: 106, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001149, mean_q: 0.967062\n",
      " 6533/10000: episode: 686, duration: 0.056s, episode steps: 6, steps per second: 107, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001278, mean_q: 0.967676\n",
      " 6539/10000: episode: 687, duration: 0.056s, episode steps: 6, steps per second: 107, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000709, mean_q: 0.967087\n",
      " 6547/10000: episode: 688, duration: 0.072s, episode steps: 8, steps per second: 111, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.375 [1.000, 2.000], mean observation: 9.500 [1.000, 15.000], loss: 0.000546, mean_q: 0.967594\n",
      " 6554/10000: episode: 689, duration: 0.064s, episode steps: 7, steps per second: 109, episode reward: 1.000, mean reward: 0.143 [0.000, 1.000], mean action: 1.714 [1.000, 3.000], mean observation: 6.857 [0.000, 15.000], loss: 0.000737, mean_q: 0.967268\n",
      " 6560/10000: episode: 690, duration: 0.057s, episode steps: 6, steps per second: 106, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000876, mean_q: 0.967906\n",
      " 6566/10000: episode: 691, duration: 0.056s, episode steps: 6, steps per second: 106, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000872, mean_q: 0.967479\n",
      " 6572/10000: episode: 692, duration: 0.056s, episode steps: 6, steps per second: 108, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001287, mean_q: 0.967323\n",
      " 6578/10000: episode: 693, duration: 0.056s, episode steps: 6, steps per second: 107, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000433, mean_q: 0.967359\n",
      " 6585/10000: episode: 694, duration: 0.069s, episode steps: 7, steps per second: 101, episode reward: 1.000, mean reward: 0.143 [0.000, 1.000], mean action: 1.429 [1.000, 2.000], mean observation: 8.857 [1.000, 15.000], loss: 0.000979, mean_q: 0.967768\n",
      " 6591/10000: episode: 695, duration: 0.057s, episode steps: 6, steps per second: 105, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001429, mean_q: 0.967829\n",
      " 6595/10000: episode: 696, duration: 0.041s, episode steps: 4, steps per second: 98, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.250 [0.000, 2.000], mean observation: 3.500 [1.000, 6.000], loss: 0.000861, mean_q: 0.967857\n",
      " 6600/10000: episode: 697, duration: 0.047s, episode steps: 5, steps per second: 106, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.600 [1.000, 2.000], mean observation: 6.000 [1.000, 11.000], loss: 0.000879, mean_q: 0.967616\n",
      " 6606/10000: episode: 698, duration: 0.055s, episode steps: 6, steps per second: 109, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000862, mean_q: 0.967428\n",
      " 6614/10000: episode: 699, duration: 0.074s, episode steps: 8, steps per second: 108, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.625 [1.000, 3.000], mean observation: 6.500 [0.000, 15.000], loss: 0.000751, mean_q: 0.967534\n",
      " 6620/10000: episode: 700, duration: 0.056s, episode steps: 6, steps per second: 107, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001009, mean_q: 0.967215\n",
      " 6626/10000: episode: 701, duration: 0.056s, episode steps: 6, steps per second: 108, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000448, mean_q: 0.967129\n",
      " 6632/10000: episode: 702, duration: 0.056s, episode steps: 6, steps per second: 107, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000729, mean_q: 0.966846\n",
      " 6636/10000: episode: 703, duration: 0.041s, episode steps: 4, steps per second: 97, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.250 [0.000, 2.000], mean observation: 3.500 [1.000, 6.000], loss: 0.001084, mean_q: 0.967291\n",
      " 6642/10000: episode: 704, duration: 0.056s, episode steps: 6, steps per second: 107, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000868, mean_q: 0.967501\n",
      " 6649/10000: episode: 705, duration: 0.064s, episode steps: 7, steps per second: 109, episode reward: 1.000, mean reward: 0.143 [0.000, 1.000], mean action: 1.286 [0.000, 2.000], mean observation: 6.857 [0.000, 15.000], loss: 0.000982, mean_q: 0.968066\n",
      " 6655/10000: episode: 706, duration: 0.057s, episode steps: 6, steps per second: 105, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001151, mean_q: 0.967385\n",
      " 6663/10000: episode: 707, duration: 0.074s, episode steps: 8, steps per second: 108, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.375 [0.000, 2.000], mean observation: 6.625 [1.000, 15.000], loss: 0.000541, mean_q: 0.967444\n",
      " 6669/10000: episode: 708, duration: 0.056s, episode steps: 6, steps per second: 106, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001000, mean_q: 0.967670\n",
      " 6675/10000: episode: 709, duration: 0.057s, episode steps: 6, steps per second: 106, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000861, mean_q: 0.966919\n",
      " 6677/10000: episode: 710, duration: 0.024s, episode steps: 2, steps per second: 84, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.500 [1.000, 2.000], mean observation: 3.000 [1.000, 5.000], loss: 0.000439, mean_q: 0.966999\n",
      " 6683/10000: episode: 711, duration: 0.123s, episode steps: 6, steps per second: 49, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001139, mean_q: 0.967542\n",
      " 6689/10000: episode: 712, duration: 0.056s, episode steps: 6, steps per second: 108, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000858, mean_q: 0.967005\n",
      " 6695/10000: episode: 713, duration: 0.056s, episode steps: 6, steps per second: 107, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000865, mean_q: 0.966818\n",
      " 6703/10000: episode: 714, duration: 0.071s, episode steps: 8, steps per second: 113, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.375 [0.000, 2.000], mean observation: 6.125 [0.000, 15.000], loss: 0.000545, mean_q: 0.967482\n",
      " 6709/10000: episode: 715, duration: 0.056s, episode steps: 6, steps per second: 106, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000581, mean_q: 0.967318\n",
      " 6716/10000: episode: 716, duration: 0.062s, episode steps: 7, steps per second: 112, episode reward: 1.000, mean reward: 0.143 [0.000, 1.000], mean action: 1.714 [1.000, 3.000], mean observation: 7.143 [1.000, 15.000], loss: 0.001092, mean_q: 0.967608\n",
      " 6722/10000: episode: 717, duration: 0.054s, episode steps: 6, steps per second: 110, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000848, mean_q: 0.968719\n",
      " 6728/10000: episode: 718, duration: 0.055s, episode steps: 6, steps per second: 110, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000862, mean_q: 0.968773\n",
      " 6734/10000: episode: 719, duration: 0.057s, episode steps: 6, steps per second: 106, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000584, mean_q: 0.968274\n",
      " 6740/10000: episode: 720, duration: 0.055s, episode steps: 6, steps per second: 109, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000438, mean_q: 0.968031\n",
      " 6746/10000: episode: 721, duration: 0.055s, episode steps: 6, steps per second: 110, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000571, mean_q: 0.967673\n",
      " 6752/10000: episode: 722, duration: 0.055s, episode steps: 6, steps per second: 109, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000294, mean_q: 0.968239\n",
      " 6758/10000: episode: 723, duration: 0.057s, episode steps: 6, steps per second: 105, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000583, mean_q: 0.967552\n",
      " 6764/10000: episode: 724, duration: 0.057s, episode steps: 6, steps per second: 106, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000577, mean_q: 0.968365\n",
      " 6769/10000: episode: 725, duration: 0.048s, episode steps: 5, steps per second: 104, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.600 [1.000, 2.000], mean observation: 6.000 [1.000, 11.000], loss: 0.001366, mean_q: 0.967795\n",
      " 6771/10000: episode: 726, duration: 0.024s, episode steps: 2, steps per second: 82, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.500 [1.000, 2.000], mean observation: 3.000 [1.000, 5.000], loss: 0.001699, mean_q: 0.968626\n",
      " 6778/10000: episode: 727, duration: 0.064s, episode steps: 7, steps per second: 110, episode reward: 1.000, mean reward: 0.143 [0.000, 1.000], mean action: 1.714 [1.000, 3.000], mean observation: 7.000 [1.000, 15.000], loss: 0.000503, mean_q: 0.968132\n",
      " 6784/10000: episode: 728, duration: 0.057s, episode steps: 6, steps per second: 105, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001434, mean_q: 0.968404\n",
      " 6790/10000: episode: 729, duration: 0.056s, episode steps: 6, steps per second: 107, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000293, mean_q: 0.967700\n",
      " 6796/10000: episode: 730, duration: 0.054s, episode steps: 6, steps per second: 110, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000863, mean_q: 0.967606\n",
      " 6802/10000: episode: 731, duration: 0.056s, episode steps: 6, steps per second: 108, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000732, mean_q: 0.967920\n",
      " 6808/10000: episode: 732, duration: 0.056s, episode steps: 6, steps per second: 108, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000726, mean_q: 0.967055\n",
      " 6810/10000: episode: 733, duration: 0.024s, episode steps: 2, steps per second: 83, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.500 [1.000, 2.000], mean observation: 3.000 [1.000, 5.000], loss: 0.001284, mean_q: 0.966743\n",
      " 6818/10000: episode: 734, duration: 0.071s, episode steps: 8, steps per second: 112, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.625 [1.000, 3.000], mean observation: 9.000 [1.000, 15.000], loss: 0.000745, mean_q: 0.967550\n",
      " 6824/10000: episode: 735, duration: 0.055s, episode steps: 6, steps per second: 109, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001008, mean_q: 0.968350\n",
      " 6830/10000: episode: 736, duration: 0.055s, episode steps: 6, steps per second: 109, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001012, mean_q: 0.968534\n",
      " 6837/10000: episode: 737, duration: 0.064s, episode steps: 7, steps per second: 109, episode reward: 1.000, mean reward: 0.143 [0.000, 1.000], mean action: 1.286 [0.000, 2.000], mean observation: 6.857 [0.000, 15.000], loss: 0.000853, mean_q: 0.967992\n",
      " 6841/10000: episode: 738, duration: 0.039s, episode steps: 4, steps per second: 103, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.250 [0.000, 2.000], mean observation: 3.500 [1.000, 6.000], loss: 0.000648, mean_q: 0.968245\n",
      " 6847/10000: episode: 739, duration: 0.054s, episode steps: 6, steps per second: 111, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000873, mean_q: 0.968919\n",
      " 6853/10000: episode: 740, duration: 0.056s, episode steps: 6, steps per second: 107, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001144, mean_q: 0.967333\n",
      " 6859/10000: episode: 741, duration: 0.117s, episode steps: 6, steps per second: 51, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.167 [0.000, 2.000], mean observation: 2.833 [1.000, 6.000], loss: 0.000445, mean_q: 0.967702\n",
      " 6865/10000: episode: 742, duration: 0.056s, episode steps: 6, steps per second: 107, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000877, mean_q: 0.967211\n",
      " 6871/10000: episode: 743, duration: 0.056s, episode steps: 6, steps per second: 106, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001429, mean_q: 0.968031\n",
      " 6877/10000: episode: 744, duration: 0.056s, episode steps: 6, steps per second: 107, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001008, mean_q: 0.968304\n",
      " 6884/10000: episode: 745, duration: 0.064s, episode steps: 7, steps per second: 110, episode reward: 1.000, mean reward: 0.143 [0.000, 1.000], mean action: 1.714 [1.000, 3.000], mean observation: 7.143 [1.000, 15.000], loss: 0.001098, mean_q: 0.968277\n",
      " 6890/10000: episode: 746, duration: 0.056s, episode steps: 6, steps per second: 107, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000579, mean_q: 0.968258\n",
      " 6896/10000: episode: 747, duration: 0.057s, episode steps: 6, steps per second: 105, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000999, mean_q: 0.968633\n",
      " 6905/10000: episode: 748, duration: 0.078s, episode steps: 9, steps per second: 115, episode reward: 1.000, mean reward: 0.111 [0.000, 1.000], mean action: 1.778 [1.000, 3.000], mean observation: 7.222 [1.000, 15.000], loss: 0.000573, mean_q: 0.967924\n",
      " 6911/10000: episode: 749, duration: 0.055s, episode steps: 6, steps per second: 110, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001142, mean_q: 0.968210\n",
      " 6915/10000: episode: 750, duration: 0.039s, episode steps: 4, steps per second: 102, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.750 [1.000, 2.000], mean observation: 4.000 [1.000, 7.000], loss: 0.001073, mean_q: 0.968215\n",
      " 6921/10000: episode: 751, duration: 0.055s, episode steps: 6, steps per second: 108, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000719, mean_q: 0.968704\n",
      " 6927/10000: episode: 752, duration: 0.055s, episode steps: 6, steps per second: 109, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001149, mean_q: 0.968593\n",
      " 6934/10000: episode: 753, duration: 0.063s, episode steps: 7, steps per second: 111, episode reward: 1.000, mean reward: 0.143 [0.000, 1.000], mean action: 1.429 [1.000, 2.000], mean observation: 8.857 [1.000, 15.000], loss: 0.000501, mean_q: 0.967716\n",
      " 6940/10000: episode: 754, duration: 0.054s, episode steps: 6, steps per second: 111, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000856, mean_q: 0.967646\n",
      " 6946/10000: episode: 755, duration: 0.056s, episode steps: 6, steps per second: 107, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000725, mean_q: 0.967784\n",
      " 6952/10000: episode: 756, duration: 0.055s, episode steps: 6, steps per second: 108, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000718, mean_q: 0.968011\n",
      " 6958/10000: episode: 757, duration: 0.055s, episode steps: 6, steps per second: 109, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000997, mean_q: 0.967862\n",
      " 6964/10000: episode: 758, duration: 0.056s, episode steps: 6, steps per second: 106, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000714, mean_q: 0.968494\n",
      " 6970/10000: episode: 759, duration: 0.059s, episode steps: 6, steps per second: 102, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000995, mean_q: 0.968436\n",
      " 6976/10000: episode: 760, duration: 0.056s, episode steps: 6, steps per second: 108, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000435, mean_q: 0.968404\n",
      " 6982/10000: episode: 761, duration: 0.056s, episode steps: 6, steps per second: 107, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001143, mean_q: 0.968390\n",
      " 6988/10000: episode: 762, duration: 0.055s, episode steps: 6, steps per second: 108, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001143, mean_q: 0.968831\n",
      " 6990/10000: episode: 763, duration: 0.024s, episode steps: 2, steps per second: 85, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.500 [1.000, 2.000], mean observation: 3.000 [1.000, 5.000], loss: 0.000418, mean_q: 0.968633\n",
      " 6996/10000: episode: 764, duration: 0.057s, episode steps: 6, steps per second: 105, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000717, mean_q: 0.968364\n",
      " 7000/10000: episode: 765, duration: 0.039s, episode steps: 4, steps per second: 103, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.250 [0.000, 2.000], mean observation: 3.500 [1.000, 6.000], loss: 0.000855, mean_q: 0.969208\n",
      " 7006/10000: episode: 766, duration: 0.055s, episode steps: 6, steps per second: 110, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001018, mean_q: 0.968715\n",
      " 7013/10000: episode: 767, duration: 0.062s, episode steps: 7, steps per second: 113, episode reward: 1.000, mean reward: 0.143 [0.000, 1.000], mean action: 1.286 [0.000, 2.000], mean observation: 6.857 [0.000, 15.000], loss: 0.001947, mean_q: 0.968502\n",
      " 7019/10000: episode: 768, duration: 0.056s, episode steps: 6, steps per second: 107, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000428, mean_q: 0.967259\n",
      " 7025/10000: episode: 769, duration: 0.055s, episode steps: 6, steps per second: 108, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000870, mean_q: 0.967673\n",
      " 7031/10000: episode: 770, duration: 0.055s, episode steps: 6, steps per second: 109, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000439, mean_q: 0.968847\n",
      " 7037/10000: episode: 771, duration: 0.054s, episode steps: 6, steps per second: 110, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.500 [0.000, 2.000], mean observation: 3.167 [1.000, 7.000], loss: 0.000865, mean_q: 0.967739\n",
      " 7045/10000: episode: 772, duration: 0.072s, episode steps: 8, steps per second: 111, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.375 [0.000, 2.000], mean observation: 6.375 [1.000, 15.000], loss: 0.000545, mean_q: 0.968383\n",
      " 7051/10000: episode: 773, duration: 0.055s, episode steps: 6, steps per second: 109, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000576, mean_q: 0.968193\n",
      " 7057/10000: episode: 774, duration: 0.055s, episode steps: 6, steps per second: 110, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001162, mean_q: 0.968210\n",
      " 7063/10000: episode: 775, duration: 0.054s, episode steps: 6, steps per second: 110, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000425, mean_q: 0.968919\n",
      " 7069/10000: episode: 776, duration: 0.056s, episode steps: 6, steps per second: 108, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001147, mean_q: 0.968796\n",
      " 7075/10000: episode: 777, duration: 0.056s, episode steps: 6, steps per second: 106, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000724, mean_q: 0.968910\n",
      " 7083/10000: episode: 778, duration: 0.072s, episode steps: 8, steps per second: 112, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.375 [0.000, 2.000], mean observation: 6.625 [1.000, 15.000], loss: 0.001165, mean_q: 0.967970\n",
      " 7094/10000: episode: 779, duration: 0.096s, episode steps: 11, steps per second: 114, episode reward: 1.000, mean reward: 0.091 [0.000, 1.000], mean action: 1.364 [0.000, 3.000], mean observation: 5.545 [0.000, 15.000], loss: 0.000772, mean_q: 0.967758\n",
      " 7100/10000: episode: 780, duration: 0.057s, episode steps: 6, steps per second: 106, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000433, mean_q: 0.968173\n",
      " 7106/10000: episode: 781, duration: 0.055s, episode steps: 6, steps per second: 109, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001005, mean_q: 0.967978\n",
      " 7112/10000: episode: 782, duration: 0.055s, episode steps: 6, steps per second: 109, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000584, mean_q: 0.968366\n",
      " 7119/10000: episode: 783, duration: 0.064s, episode steps: 7, steps per second: 109, episode reward: 1.000, mean reward: 0.143 [0.000, 1.000], mean action: 1.714 [1.000, 3.000], mean observation: 7.143 [1.000, 15.000], loss: 0.000961, mean_q: 0.969045\n",
      " 7127/10000: episode: 784, duration: 0.070s, episode steps: 8, steps per second: 114, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.375 [0.000, 2.000], mean observation: 8.750 [1.000, 15.000], loss: 0.000327, mean_q: 0.968980\n",
      " 7133/10000: episode: 785, duration: 0.055s, episode steps: 6, steps per second: 109, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001016, mean_q: 0.968763\n",
      " 7139/10000: episode: 786, duration: 0.055s, episode steps: 6, steps per second: 109, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001143, mean_q: 0.968663\n",
      " 7145/10000: episode: 787, duration: 0.057s, episode steps: 6, steps per second: 106, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000588, mean_q: 0.968428\n",
      " 7151/10000: episode: 788, duration: 0.055s, episode steps: 6, steps per second: 110, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000872, mean_q: 0.968146\n",
      " 7157/10000: episode: 789, duration: 0.055s, episode steps: 6, steps per second: 110, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000728, mean_q: 0.968214\n",
      " 7163/10000: episode: 790, duration: 0.055s, episode steps: 6, steps per second: 110, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000718, mean_q: 0.968014\n",
      " 7171/10000: episode: 791, duration: 0.074s, episode steps: 8, steps per second: 109, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.375 [0.000, 2.000], mean observation: 6.125 [0.000, 15.000], loss: 0.000642, mean_q: 0.967881\n",
      " 7177/10000: episode: 792, duration: 0.055s, episode steps: 6, steps per second: 108, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000435, mean_q: 0.968639\n",
      " 7183/10000: episode: 793, duration: 0.057s, episode steps: 6, steps per second: 105, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000726, mean_q: 0.968812\n",
      " 7189/10000: episode: 794, duration: 0.118s, episode steps: 6, steps per second: 51, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000729, mean_q: 0.968273\n",
      " 7196/10000: episode: 795, duration: 0.064s, episode steps: 7, steps per second: 110, episode reward: 1.000, mean reward: 0.143 [0.000, 1.000], mean action: 1.714 [1.000, 3.000], mean observation: 6.857 [0.000, 15.000], loss: 0.000737, mean_q: 0.968313\n",
      " 7202/10000: episode: 796, duration: 0.056s, episode steps: 6, steps per second: 107, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000153, mean_q: 0.968932\n",
      " 7208/10000: episode: 797, duration: 0.055s, episode steps: 6, steps per second: 109, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001278, mean_q: 0.969203\n",
      " 7214/10000: episode: 798, duration: 0.057s, episode steps: 6, steps per second: 106, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000302, mean_q: 0.969185\n",
      " 7220/10000: episode: 799, duration: 0.055s, episode steps: 6, steps per second: 108, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000441, mean_q: 0.968100\n",
      " 7226/10000: episode: 800, duration: 0.054s, episode steps: 6, steps per second: 110, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001162, mean_q: 0.968666\n",
      " 7234/10000: episode: 801, duration: 0.070s, episode steps: 8, steps per second: 114, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.500 [0.000, 3.000], mean observation: 6.000 [0.000, 15.000], loss: 0.000535, mean_q: 0.968102\n",
      " 7240/10000: episode: 802, duration: 0.057s, episode steps: 6, steps per second: 105, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000424, mean_q: 0.967959\n",
      " 7246/10000: episode: 803, duration: 0.055s, episode steps: 6, steps per second: 110, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000720, mean_q: 0.968727\n",
      " 7252/10000: episode: 804, duration: 0.055s, episode steps: 6, steps per second: 110, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001138, mean_q: 0.968341\n",
      " 7258/10000: episode: 805, duration: 0.054s, episode steps: 6, steps per second: 111, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000851, mean_q: 0.968821\n",
      " 7270/10000: episode: 806, duration: 0.104s, episode steps: 12, steps per second: 116, episode reward: 1.000, mean reward: 0.083 [0.000, 1.000], mean action: 1.417 [0.000, 3.000], mean observation: 5.000 [0.000, 15.000], loss: 0.000294, mean_q: 0.968609\n",
      " 7276/10000: episode: 807, duration: 0.055s, episode steps: 6, steps per second: 109, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001419, mean_q: 0.969043\n",
      " 7282/10000: episode: 808, duration: 0.054s, episode steps: 6, steps per second: 111, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000719, mean_q: 0.968653\n",
      " 7284/10000: episode: 809, duration: 0.027s, episode steps: 2, steps per second: 75, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.500 [1.000, 2.000], mean observation: 3.000 [1.000, 5.000], loss: 0.000440, mean_q: 0.969491\n",
      " 7290/10000: episode: 810, duration: 0.054s, episode steps: 6, steps per second: 111, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001010, mean_q: 0.968608\n",
      " 7296/10000: episode: 811, duration: 0.056s, episode steps: 6, steps per second: 106, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001274, mean_q: 0.968281\n",
      " 7302/10000: episode: 812, duration: 0.055s, episode steps: 6, steps per second: 110, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000712, mean_q: 0.968285\n",
      " 7308/10000: episode: 813, duration: 0.056s, episode steps: 6, steps per second: 106, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000445, mean_q: 0.968781\n",
      " 7314/10000: episode: 814, duration: 0.055s, episode steps: 6, steps per second: 108, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000715, mean_q: 0.969229\n",
      " 7320/10000: episode: 815, duration: 0.056s, episode steps: 6, steps per second: 107, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001001, mean_q: 0.969039\n",
      " 7326/10000: episode: 816, duration: 0.055s, episode steps: 6, steps per second: 109, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000441, mean_q: 0.968914\n",
      " 7333/10000: episode: 817, duration: 0.063s, episode steps: 7, steps per second: 112, episode reward: 1.000, mean reward: 0.143 [0.000, 1.000], mean action: 1.429 [1.000, 2.000], mean observation: 8.857 [1.000, 15.000], loss: 0.000620, mean_q: 0.968363\n",
      " 7341/10000: episode: 818, duration: 0.070s, episode steps: 8, steps per second: 114, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.375 [0.000, 2.000], mean observation: 6.125 [0.000, 15.000], loss: 0.000874, mean_q: 0.968759\n",
      " 7347/10000: episode: 819, duration: 0.056s, episode steps: 6, steps per second: 108, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000439, mean_q: 0.968739\n",
      " 7353/10000: episode: 820, duration: 0.055s, episode steps: 6, steps per second: 110, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001148, mean_q: 0.968437\n",
      " 7359/10000: episode: 821, duration: 0.055s, episode steps: 6, steps per second: 110, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000425, mean_q: 0.968303\n",
      " 7365/10000: episode: 822, duration: 0.117s, episode steps: 6, steps per second: 51, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000293, mean_q: 0.968993\n",
      " 7371/10000: episode: 823, duration: 0.057s, episode steps: 6, steps per second: 106, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000598, mean_q: 0.968584\n",
      " 7377/10000: episode: 824, duration: 0.055s, episode steps: 6, steps per second: 109, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000866, mean_q: 0.968700\n",
      " 7383/10000: episode: 825, duration: 0.055s, episode steps: 6, steps per second: 109, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000574, mean_q: 0.967499\n",
      " 7389/10000: episode: 826, duration: 0.057s, episode steps: 6, steps per second: 104, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000725, mean_q: 0.967416\n",
      " 7396/10000: episode: 827, duration: 0.064s, episode steps: 7, steps per second: 109, episode reward: 1.000, mean reward: 0.143 [0.000, 1.000], mean action: 1.714 [1.000, 3.000], mean observation: 6.857 [0.000, 15.000], loss: 0.000502, mean_q: 0.968210\n",
      " 7402/10000: episode: 828, duration: 0.058s, episode steps: 6, steps per second: 104, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000854, mean_q: 0.968984\n",
      " 7412/10000: episode: 829, duration: 0.090s, episode steps: 10, steps per second: 111, episode reward: 1.000, mean reward: 0.100 [0.000, 1.000], mean action: 1.700 [1.000, 3.000], mean observation: 9.900 [4.000, 15.000], loss: 0.000954, mean_q: 0.969007\n",
      " 7417/10000: episode: 830, duration: 0.048s, episode steps: 5, steps per second: 103, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.600 [1.000, 2.000], mean observation: 6.000 [1.000, 11.000], loss: 0.000351, mean_q: 0.968880\n",
      " 7423/10000: episode: 831, duration: 0.056s, episode steps: 6, steps per second: 108, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000582, mean_q: 0.969209\n",
      " 7429/10000: episode: 832, duration: 0.056s, episode steps: 6, steps per second: 108, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001312, mean_q: 0.967963\n",
      " 7435/10000: episode: 833, duration: 0.057s, episode steps: 6, steps per second: 105, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000849, mean_q: 0.968461\n",
      " 7443/10000: episode: 834, duration: 0.072s, episode steps: 8, steps per second: 111, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.375 [0.000, 2.000], mean observation: 8.750 [1.000, 15.000], loss: 0.000849, mean_q: 0.968819\n",
      " 7449/10000: episode: 835, duration: 0.055s, episode steps: 6, steps per second: 109, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000854, mean_q: 0.968602\n",
      " 7455/10000: episode: 836, duration: 0.055s, episode steps: 6, steps per second: 109, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000159, mean_q: 0.968340\n",
      " 7459/10000: episode: 837, duration: 0.041s, episode steps: 4, steps per second: 97, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.750 [1.000, 2.000], mean observation: 4.000 [1.000, 7.000], loss: 0.001499, mean_q: 0.968122\n",
      " 7465/10000: episode: 838, duration: 0.056s, episode steps: 6, steps per second: 108, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000868, mean_q: 0.967707\n",
      " 7469/10000: episode: 839, duration: 0.040s, episode steps: 4, steps per second: 101, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.250 [0.000, 2.000], mean observation: 3.500 [1.000, 6.000], loss: 0.000225, mean_q: 0.967988\n",
      " 7479/10000: episode: 840, duration: 0.087s, episode steps: 10, steps per second: 115, episode reward: 1.000, mean reward: 0.100 [0.000, 1.000], mean action: 1.700 [1.000, 3.000], mean observation: 7.200 [1.000, 15.000], loss: 0.000438, mean_q: 0.968577\n",
      " 7485/10000: episode: 841, duration: 0.057s, episode steps: 6, steps per second: 105, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001002, mean_q: 0.968994\n",
      " 7493/10000: episode: 842, duration: 0.074s, episode steps: 8, steps per second: 108, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.625 [1.000, 3.000], mean observation: 6.500 [0.000, 15.000], loss: 0.000659, mean_q: 0.968069\n",
      " 7502/10000: episode: 843, duration: 0.080s, episode steps: 9, steps per second: 113, episode reward: 1.000, mean reward: 0.111 [0.000, 1.000], mean action: 1.556 [0.000, 3.000], mean observation: 5.889 [1.000, 15.000], loss: 0.001236, mean_q: 0.968790\n",
      " 7509/10000: episode: 844, duration: 0.064s, episode steps: 7, steps per second: 109, episode reward: 1.000, mean reward: 0.143 [0.000, 1.000], mean action: 1.286 [0.000, 2.000], mean observation: 6.857 [0.000, 15.000], loss: 0.000383, mean_q: 0.968377\n",
      " 7515/10000: episode: 845, duration: 0.056s, episode steps: 6, steps per second: 107, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000718, mean_q: 0.968350\n",
      " 7521/10000: episode: 846, duration: 0.055s, episode steps: 6, steps per second: 109, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001125, mean_q: 0.968019\n",
      " 7526/10000: episode: 847, duration: 0.048s, episode steps: 5, steps per second: 105, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.600 [1.000, 2.000], mean observation: 6.000 [1.000, 11.000], loss: 0.001539, mean_q: 0.968791\n",
      " 7536/10000: episode: 848, duration: 0.089s, episode steps: 10, steps per second: 113, episode reward: 1.000, mean reward: 0.100 [0.000, 1.000], mean action: 1.500 [0.000, 3.000], mean observation: 9.400 [1.000, 15.000], loss: 0.000688, mean_q: 0.968821\n",
      " 7544/10000: episode: 849, duration: 0.071s, episode steps: 8, steps per second: 112, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.625 [1.000, 3.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000764, mean_q: 0.968673\n",
      " 7548/10000: episode: 850, duration: 0.039s, episode steps: 4, steps per second: 101, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.750 [1.000, 2.000], mean observation: 4.000 [1.000, 7.000], loss: 0.001291, mean_q: 0.969167\n",
      " 7556/10000: episode: 851, duration: 0.070s, episode steps: 8, steps per second: 114, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.375 [0.000, 2.000], mean observation: 9.375 [1.000, 15.000], loss: 0.000958, mean_q: 0.968814\n",
      " 7566/10000: episode: 852, duration: 0.088s, episode steps: 10, steps per second: 113, episode reward: 1.000, mean reward: 0.100 [0.000, 1.000], mean action: 1.700 [1.000, 3.000], mean observation: 9.600 [1.000, 15.000], loss: 0.000611, mean_q: 0.969272\n",
      " 7572/10000: episode: 853, duration: 0.056s, episode steps: 6, steps per second: 108, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000703, mean_q: 0.969240\n",
      " 7578/10000: episode: 854, duration: 0.055s, episode steps: 6, steps per second: 110, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000712, mean_q: 0.968848\n",
      " 7586/10000: episode: 855, duration: 0.071s, episode steps: 8, steps per second: 113, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.625 [1.000, 3.000], mean observation: 7.000 [1.000, 15.000], loss: 0.000330, mean_q: 0.969274\n",
      " 7590/10000: episode: 856, duration: 0.041s, episode steps: 4, steps per second: 98, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.250 [0.000, 2.000], mean observation: 3.500 [1.000, 6.000], loss: 0.000659, mean_q: 0.968622\n",
      " 7596/10000: episode: 857, duration: 0.056s, episode steps: 6, steps per second: 108, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001150, mean_q: 0.968407\n",
      " 7602/10000: episode: 858, duration: 0.057s, episode steps: 6, steps per second: 106, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000570, mean_q: 0.968195\n",
      " 7608/10000: episode: 859, duration: 0.056s, episode steps: 6, steps per second: 108, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001003, mean_q: 0.968136\n",
      " 7614/10000: episode: 860, duration: 0.058s, episode steps: 6, steps per second: 104, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001001, mean_q: 0.968432\n",
      " 7621/10000: episode: 861, duration: 0.065s, episode steps: 7, steps per second: 108, episode reward: 1.000, mean reward: 0.143 [0.000, 1.000], mean action: 1.286 [0.000, 2.000], mean observation: 6.857 [0.000, 15.000], loss: 0.000739, mean_q: 0.969211\n",
      " 7627/10000: episode: 862, duration: 0.057s, episode steps: 6, steps per second: 104, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001008, mean_q: 0.969087\n",
      " 7633/10000: episode: 863, duration: 0.056s, episode steps: 6, steps per second: 107, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000565, mean_q: 0.969592\n",
      " 7639/10000: episode: 864, duration: 0.057s, episode steps: 6, steps per second: 105, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000718, mean_q: 0.968750\n",
      " 7645/10000: episode: 865, duration: 0.055s, episode steps: 6, steps per second: 108, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001287, mean_q: 0.967903\n",
      " 7650/10000: episode: 866, duration: 0.047s, episode steps: 5, steps per second: 106, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.600 [1.000, 2.000], mean observation: 6.000 [1.000, 11.000], loss: 0.000860, mean_q: 0.968496\n",
      " 7656/10000: episode: 867, duration: 0.056s, episode steps: 6, steps per second: 108, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001138, mean_q: 0.968356\n",
      " 7662/10000: episode: 868, duration: 0.056s, episode steps: 6, steps per second: 108, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000716, mean_q: 0.968835\n",
      " 7668/10000: episode: 869, duration: 0.056s, episode steps: 6, steps per second: 107, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000587, mean_q: 0.969360\n",
      " 7676/10000: episode: 870, duration: 0.072s, episode steps: 8, steps per second: 110, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.625 [1.000, 3.000], mean observation: 9.000 [1.000, 15.000], loss: 0.001290, mean_q: 0.968648\n",
      " 7682/10000: episode: 871, duration: 0.056s, episode steps: 6, steps per second: 107, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000727, mean_q: 0.967894\n",
      " 7688/10000: episode: 872, duration: 0.057s, episode steps: 6, steps per second: 105, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001134, mean_q: 0.968790\n",
      " 7694/10000: episode: 873, duration: 0.110s, episode steps: 6, steps per second: 54, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000990, mean_q: 0.968248\n",
      " 7701/10000: episode: 874, duration: 0.074s, episode steps: 7, steps per second: 94, episode reward: 1.000, mean reward: 0.143 [0.000, 1.000], mean action: 1.714 [1.000, 3.000], mean observation: 6.857 [0.000, 15.000], loss: 0.000747, mean_q: 0.969227\n",
      " 7707/10000: episode: 875, duration: 0.057s, episode steps: 6, steps per second: 105, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000724, mean_q: 0.968081\n",
      " 7713/10000: episode: 876, duration: 0.057s, episode steps: 6, steps per second: 104, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000293, mean_q: 0.968336\n",
      " 7719/10000: episode: 877, duration: 0.056s, episode steps: 6, steps per second: 106, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000724, mean_q: 0.968770\n",
      " 7726/10000: episode: 878, duration: 0.065s, episode steps: 7, steps per second: 107, episode reward: 1.000, mean reward: 0.143 [0.000, 1.000], mean action: 1.714 [1.000, 3.000], mean observation: 7.143 [1.000, 15.000], loss: 0.000860, mean_q: 0.968386\n",
      " 7737/10000: episode: 879, duration: 0.098s, episode steps: 11, steps per second: 113, episode reward: 1.000, mean reward: 0.091 [0.000, 1.000], mean action: 1.364 [0.000, 3.000], mean observation: 5.545 [0.000, 15.000], loss: 0.000705, mean_q: 0.968661\n",
      " 7743/10000: episode: 880, duration: 0.056s, episode steps: 6, steps per second: 107, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000433, mean_q: 0.968970\n",
      " 7749/10000: episode: 881, duration: 0.056s, episode steps: 6, steps per second: 108, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001012, mean_q: 0.969767\n",
      " 7755/10000: episode: 882, duration: 0.056s, episode steps: 6, steps per second: 107, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000291, mean_q: 0.969391\n",
      " 7761/10000: episode: 883, duration: 0.057s, episode steps: 6, steps per second: 105, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000859, mean_q: 0.969284\n",
      " 7767/10000: episode: 884, duration: 0.056s, episode steps: 6, steps per second: 106, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000869, mean_q: 0.969137\n",
      " 7773/10000: episode: 885, duration: 0.056s, episode steps: 6, steps per second: 107, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000992, mean_q: 0.969831\n",
      " 7779/10000: episode: 886, duration: 0.055s, episode steps: 6, steps per second: 109, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001002, mean_q: 0.968725\n",
      " 7785/10000: episode: 887, duration: 0.058s, episode steps: 6, steps per second: 104, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001012, mean_q: 0.968709\n",
      " 7793/10000: episode: 888, duration: 0.073s, episode steps: 8, steps per second: 110, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.625 [1.000, 3.000], mean observation: 7.000 [1.000, 15.000], loss: 0.000866, mean_q: 0.968617\n",
      " 7799/10000: episode: 889, duration: 0.056s, episode steps: 6, steps per second: 107, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000866, mean_q: 0.968911\n",
      " 7805/10000: episode: 890, duration: 0.059s, episode steps: 6, steps per second: 102, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000715, mean_q: 0.968282\n",
      " 7811/10000: episode: 891, duration: 0.057s, episode steps: 6, steps per second: 105, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001271, mean_q: 0.968717\n",
      " 7819/10000: episode: 892, duration: 0.074s, episode steps: 8, steps per second: 108, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.375 [0.000, 2.000], mean observation: 6.375 [1.000, 15.000], loss: 0.000435, mean_q: 0.968607\n",
      " 7825/10000: episode: 893, duration: 0.056s, episode steps: 6, steps per second: 108, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001153, mean_q: 0.969180\n",
      " 7831/10000: episode: 894, duration: 0.058s, episode steps: 6, steps per second: 104, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000292, mean_q: 0.968969\n",
      " 7837/10000: episode: 895, duration: 0.057s, episode steps: 6, steps per second: 105, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001577, mean_q: 0.968992\n",
      " 7843/10000: episode: 896, duration: 0.056s, episode steps: 6, steps per second: 108, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000296, mean_q: 0.968125\n",
      " 7849/10000: episode: 897, duration: 0.056s, episode steps: 6, steps per second: 107, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000593, mean_q: 0.968685\n",
      " 7855/10000: episode: 898, duration: 0.055s, episode steps: 6, steps per second: 109, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000435, mean_q: 0.969688\n",
      " 7861/10000: episode: 899, duration: 0.056s, episode steps: 6, steps per second: 107, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000290, mean_q: 0.969074\n",
      " 7867/10000: episode: 900, duration: 0.056s, episode steps: 6, steps per second: 108, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001003, mean_q: 0.969666\n",
      " 7873/10000: episode: 901, duration: 0.112s, episode steps: 6, steps per second: 54, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000293, mean_q: 0.969530\n",
      " 7879/10000: episode: 902, duration: 0.059s, episode steps: 6, steps per second: 102, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001304, mean_q: 0.967914\n",
      " 7885/10000: episode: 903, duration: 0.056s, episode steps: 6, steps per second: 108, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000438, mean_q: 0.968241\n",
      " 7891/10000: episode: 904, duration: 0.058s, episode steps: 6, steps per second: 103, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.167 [0.000, 2.000], mean observation: 2.833 [1.000, 6.000], loss: 0.000724, mean_q: 0.968178\n",
      " 7897/10000: episode: 905, duration: 0.055s, episode steps: 6, steps per second: 108, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000866, mean_q: 0.968361\n",
      " 7903/10000: episode: 906, duration: 0.056s, episode steps: 6, steps per second: 106, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001003, mean_q: 0.969322\n",
      " 7911/10000: episode: 907, duration: 0.073s, episode steps: 8, steps per second: 110, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.625 [1.000, 3.000], mean observation: 9.000 [1.000, 15.000], loss: 0.000642, mean_q: 0.968547\n",
      " 7919/10000: episode: 908, duration: 0.071s, episode steps: 8, steps per second: 113, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.625 [1.000, 3.000], mean observation: 6.500 [0.000, 15.000], loss: 0.001170, mean_q: 0.968985\n",
      " 7925/10000: episode: 909, duration: 0.056s, episode steps: 6, steps per second: 108, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000440, mean_q: 0.969017\n",
      " 7931/10000: episode: 910, duration: 0.058s, episode steps: 6, steps per second: 104, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000724, mean_q: 0.968888\n",
      " 7937/10000: episode: 911, duration: 0.056s, episode steps: 6, steps per second: 106, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000570, mean_q: 0.969083\n",
      " 7943/10000: episode: 912, duration: 0.055s, episode steps: 6, steps per second: 108, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001427, mean_q: 0.968697\n",
      " 7951/10000: episode: 913, duration: 0.072s, episode steps: 8, steps per second: 112, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.625 [1.000, 3.000], mean observation: 9.000 [1.000, 15.000], loss: 0.000540, mean_q: 0.968722\n",
      " 7957/10000: episode: 914, duration: 0.057s, episode steps: 6, steps per second: 106, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000714, mean_q: 0.968346\n",
      " 7963/10000: episode: 915, duration: 0.056s, episode steps: 6, steps per second: 107, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000572, mean_q: 0.969322\n",
      " 7970/10000: episode: 916, duration: 0.063s, episode steps: 7, steps per second: 112, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.429 [0.000, 2.000], mean observation: 5.000 [1.000, 11.000], loss: 0.000498, mean_q: 0.969370\n",
      " 7977/10000: episode: 917, duration: 0.063s, episode steps: 7, steps per second: 112, episode reward: 1.000, mean reward: 0.143 [0.000, 1.000], mean action: 1.714 [1.000, 3.000], mean observation: 7.143 [1.000, 15.000], loss: 0.000623, mean_q: 0.968833\n",
      " 7985/10000: episode: 918, duration: 0.073s, episode steps: 8, steps per second: 109, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.375 [0.000, 2.000], mean observation: 9.375 [1.000, 15.000], loss: 0.001293, mean_q: 0.969832\n",
      " 7991/10000: episode: 919, duration: 0.055s, episode steps: 6, steps per second: 109, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001147, mean_q: 0.969198\n",
      " 7998/10000: episode: 920, duration: 0.063s, episode steps: 7, steps per second: 111, episode reward: 1.000, mean reward: 0.143 [0.000, 1.000], mean action: 1.714 [1.000, 3.000], mean observation: 7.000 [1.000, 15.000], loss: 0.000370, mean_q: 0.968901\n",
      " 8006/10000: episode: 921, duration: 0.071s, episode steps: 8, steps per second: 113, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.375 [0.000, 2.000], mean observation: 9.375 [1.000, 15.000], loss: 0.000330, mean_q: 0.969125\n",
      " 8012/10000: episode: 922, duration: 0.057s, episode steps: 6, steps per second: 106, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000569, mean_q: 0.969282\n",
      " 8018/10000: episode: 923, duration: 0.056s, episode steps: 6, steps per second: 107, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000705, mean_q: 0.969912\n",
      " 8024/10000: episode: 924, duration: 0.056s, episode steps: 6, steps per second: 107, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000987, mean_q: 0.969974\n",
      " 8030/10000: episode: 925, duration: 0.056s, episode steps: 6, steps per second: 107, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000851, mean_q: 0.969663\n",
      " 8036/10000: episode: 926, duration: 0.058s, episode steps: 6, steps per second: 104, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000716, mean_q: 0.969624\n",
      " 8038/10000: episode: 927, duration: 0.025s, episode steps: 2, steps per second: 81, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.500 [1.000, 2.000], mean observation: 3.000 [1.000, 5.000], loss: 0.001293, mean_q: 0.969437\n",
      " 8044/10000: episode: 928, duration: 0.056s, episode steps: 6, steps per second: 107, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000717, mean_q: 0.969296\n",
      " 8049/10000: episode: 929, duration: 0.047s, episode steps: 5, steps per second: 106, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.600 [1.000, 2.000], mean observation: 6.000 [1.000, 11.000], loss: 0.000345, mean_q: 0.969530\n",
      " 8055/10000: episode: 930, duration: 0.056s, episode steps: 6, steps per second: 108, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000574, mean_q: 0.969851\n",
      " 8061/10000: episode: 931, duration: 0.057s, episode steps: 6, steps per second: 105, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001006, mean_q: 0.969604\n",
      " 8067/10000: episode: 932, duration: 0.055s, episode steps: 6, steps per second: 110, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000291, mean_q: 0.970724\n",
      " 8073/10000: episode: 933, duration: 0.054s, episode steps: 6, steps per second: 110, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000999, mean_q: 0.970569\n",
      " 8079/10000: episode: 934, duration: 0.055s, episode steps: 6, steps per second: 109, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000296, mean_q: 0.969886\n",
      " 8087/10000: episode: 935, duration: 0.073s, episode steps: 8, steps per second: 109, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.375 [0.000, 2.000], mean observation: 6.625 [1.000, 15.000], loss: 0.000759, mean_q: 0.970019\n",
      " 8093/10000: episode: 936, duration: 0.056s, episode steps: 6, steps per second: 107, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000720, mean_q: 0.969515\n",
      " 8099/10000: episode: 937, duration: 0.055s, episode steps: 6, steps per second: 109, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000880, mean_q: 0.969530\n",
      " 8105/10000: episode: 938, duration: 0.055s, episode steps: 6, steps per second: 109, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001142, mean_q: 0.969765\n",
      " 8111/10000: episode: 939, duration: 0.057s, episode steps: 6, steps per second: 105, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000850, mean_q: 0.969980\n",
      " 8118/10000: episode: 940, duration: 0.064s, episode steps: 7, steps per second: 110, episode reward: 1.000, mean reward: 0.143 [0.000, 1.000], mean action: 1.286 [0.000, 2.000], mean observation: 6.857 [0.000, 15.000], loss: 0.000867, mean_q: 0.969353\n",
      " 8125/10000: episode: 941, duration: 0.066s, episode steps: 7, steps per second: 106, episode reward: 1.000, mean reward: 0.143 [0.000, 1.000], mean action: 1.429 [1.000, 2.000], mean observation: 8.857 [1.000, 15.000], loss: 0.000503, mean_q: 0.969697\n",
      " 8131/10000: episode: 942, duration: 0.055s, episode steps: 6, steps per second: 109, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000295, mean_q: 0.969633\n",
      " 8137/10000: episode: 943, duration: 0.057s, episode steps: 6, steps per second: 105, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000717, mean_q: 0.969369\n",
      " 8143/10000: episode: 944, duration: 0.056s, episode steps: 6, steps per second: 107, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000433, mean_q: 0.969477\n",
      " 8149/10000: episode: 945, duration: 0.055s, episode steps: 6, steps per second: 108, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000152, mean_q: 0.970025\n",
      " 8155/10000: episode: 946, duration: 0.056s, episode steps: 6, steps per second: 108, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000434, mean_q: 0.969956\n",
      " 8159/10000: episode: 947, duration: 0.040s, episode steps: 4, steps per second: 100, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.250 [0.000, 2.000], mean observation: 3.500 [1.000, 6.000], loss: 0.000860, mean_q: 0.969234\n",
      " 8165/10000: episode: 948, duration: 0.056s, episode steps: 6, steps per second: 106, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000288, mean_q: 0.969942\n",
      " 8171/10000: episode: 949, duration: 0.056s, episode steps: 6, steps per second: 108, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001154, mean_q: 0.968968\n",
      " 8177/10000: episode: 950, duration: 0.055s, episode steps: 6, steps per second: 109, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000150, mean_q: 0.969634\n",
      " 8183/10000: episode: 951, duration: 0.057s, episode steps: 6, steps per second: 106, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000433, mean_q: 0.969442\n",
      " 8185/10000: episode: 952, duration: 0.024s, episode steps: 2, steps per second: 83, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.500 [1.000, 2.000], mean observation: 3.000 [1.000, 5.000], loss: 0.000008, mean_q: 0.969109\n",
      " 8189/10000: episode: 953, duration: 0.039s, episode steps: 4, steps per second: 102, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.250 [0.000, 2.000], mean observation: 3.500 [1.000, 6.000], loss: 0.001083, mean_q: 0.969736\n",
      " 8195/10000: episode: 954, duration: 0.055s, episode steps: 6, steps per second: 109, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000723, mean_q: 0.968895\n",
      " 8201/10000: episode: 955, duration: 0.107s, episode steps: 6, steps per second: 56, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000580, mean_q: 0.969296\n",
      " 8207/10000: episode: 956, duration: 0.068s, episode steps: 6, steps per second: 88, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000573, mean_q: 0.968770\n",
      " 8213/10000: episode: 957, duration: 0.056s, episode steps: 6, steps per second: 107, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000150, mean_q: 0.969154\n",
      " 8219/10000: episode: 958, duration: 0.059s, episode steps: 6, steps per second: 102, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000586, mean_q: 0.969410\n",
      " 8225/10000: episode: 959, duration: 0.059s, episode steps: 6, steps per second: 102, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000427, mean_q: 0.969558\n",
      " 8231/10000: episode: 960, duration: 0.056s, episode steps: 6, steps per second: 107, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001142, mean_q: 0.969230\n",
      " 8237/10000: episode: 961, duration: 0.056s, episode steps: 6, steps per second: 107, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000434, mean_q: 0.968699\n",
      " 8244/10000: episode: 962, duration: 0.064s, episode steps: 7, steps per second: 109, episode reward: 1.000, mean reward: 0.143 [0.000, 1.000], mean action: 1.714 [1.000, 3.000], mean observation: 7.143 [1.000, 15.000], loss: 0.000743, mean_q: 0.969389\n",
      " 8250/10000: episode: 963, duration: 0.057s, episode steps: 6, steps per second: 106, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000861, mean_q: 0.970160\n",
      " 8256/10000: episode: 964, duration: 0.057s, episode steps: 6, steps per second: 106, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000433, mean_q: 0.969170\n",
      " 8262/10000: episode: 965, duration: 0.055s, episode steps: 6, steps per second: 108, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000573, mean_q: 0.969403\n",
      " 8270/10000: episode: 966, duration: 0.071s, episode steps: 8, steps per second: 113, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.625 [1.000, 3.000], mean observation: 6.500 [0.000, 15.000], loss: 0.000335, mean_q: 0.969311\n",
      " 8276/10000: episode: 967, duration: 0.057s, episode steps: 6, steps per second: 106, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000583, mean_q: 0.969302\n",
      " 8284/10000: episode: 968, duration: 0.072s, episode steps: 8, steps per second: 111, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.625 [1.000, 3.000], mean observation: 6.500 [0.000, 15.000], loss: 0.000652, mean_q: 0.969369\n",
      " 8290/10000: episode: 969, duration: 0.058s, episode steps: 6, steps per second: 104, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000852, mean_q: 0.969451\n",
      " 8298/10000: episode: 970, duration: 0.071s, episode steps: 8, steps per second: 113, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.625 [1.000, 3.000], mean observation: 9.000 [1.000, 15.000], loss: 0.000966, mean_q: 0.969186\n",
      " 8306/10000: episode: 971, duration: 0.073s, episode steps: 8, steps per second: 109, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.375 [0.000, 2.000], mean observation: 9.375 [1.000, 15.000], loss: 0.000334, mean_q: 0.969824\n",
      " 8312/10000: episode: 972, duration: 0.056s, episode steps: 6, steps per second: 108, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000724, mean_q: 0.969697\n",
      " 8316/10000: episode: 973, duration: 0.039s, episode steps: 4, steps per second: 102, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.250 [0.000, 2.000], mean observation: 3.500 [1.000, 6.000], loss: 0.000646, mean_q: 0.969433\n",
      " 8324/10000: episode: 974, duration: 0.071s, episode steps: 8, steps per second: 113, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.375 [0.000, 2.000], mean observation: 9.375 [1.000, 15.000], loss: 0.000645, mean_q: 0.968811\n",
      " 8330/10000: episode: 975, duration: 0.056s, episode steps: 6, steps per second: 107, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000434, mean_q: 0.968922\n",
      " 8336/10000: episode: 976, duration: 0.056s, episode steps: 6, steps per second: 106, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000560, mean_q: 0.969464\n",
      " 8343/10000: episode: 977, duration: 0.064s, episode steps: 7, steps per second: 109, episode reward: 1.000, mean reward: 0.143 [0.000, 1.000], mean action: 1.286 [0.000, 2.000], mean observation: 6.857 [0.000, 15.000], loss: 0.000377, mean_q: 0.969381\n",
      " 8349/10000: episode: 978, duration: 0.057s, episode steps: 6, steps per second: 105, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000711, mean_q: 0.970102\n",
      " 8355/10000: episode: 979, duration: 0.065s, episode steps: 6, steps per second: 92, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000573, mean_q: 0.969709\n",
      " 8361/10000: episode: 980, duration: 0.055s, episode steps: 6, steps per second: 109, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000715, mean_q: 0.969664\n",
      " 8369/10000: episode: 981, duration: 0.072s, episode steps: 8, steps per second: 111, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.375 [0.000, 2.000], mean observation: 8.750 [1.000, 15.000], loss: 0.000846, mean_q: 0.969883\n",
      " 8375/10000: episode: 982, duration: 0.055s, episode steps: 6, steps per second: 110, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000998, mean_q: 0.969543\n",
      " 8381/10000: episode: 983, duration: 0.116s, episode steps: 6, steps per second: 52, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000585, mean_q: 0.969543\n",
      " 8387/10000: episode: 984, duration: 0.056s, episode steps: 6, steps per second: 107, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001124, mean_q: 0.969895\n",
      " 8393/10000: episode: 985, duration: 0.056s, episode steps: 6, steps per second: 108, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000435, mean_q: 0.970161\n",
      " 8399/10000: episode: 986, duration: 0.055s, episode steps: 6, steps per second: 109, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000592, mean_q: 0.969816\n",
      " 8405/10000: episode: 987, duration: 0.057s, episode steps: 6, steps per second: 105, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000297, mean_q: 0.969522\n",
      " 8411/10000: episode: 988, duration: 0.056s, episode steps: 6, steps per second: 107, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000714, mean_q: 0.969347\n",
      " 8417/10000: episode: 989, duration: 0.055s, episode steps: 6, steps per second: 109, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 10.500 [4.000, 15.000], loss: 0.000286, mean_q: 0.969520\n",
      " 8424/10000: episode: 990, duration: 0.063s, episode steps: 7, steps per second: 111, episode reward: 1.000, mean reward: 0.143 [0.000, 1.000], mean action: 1.714 [1.000, 3.000], mean observation: 7.000 [1.000, 15.000], loss: 0.000733, mean_q: 0.970363\n",
      " 8432/10000: episode: 991, duration: 0.074s, episode steps: 8, steps per second: 109, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.625 [0.000, 3.000], mean observation: 3.875 [0.000, 11.000], loss: 0.000226, mean_q: 0.970067\n",
      " 8439/10000: episode: 992, duration: 0.064s, episode steps: 7, steps per second: 109, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.286 [0.000, 3.000], mean observation: 4.286 [0.000, 10.000], loss: 0.000752, mean_q: 0.969840\n",
      " 8447/10000: episode: 993, duration: 0.072s, episode steps: 8, steps per second: 111, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.625 [1.000, 3.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000443, mean_q: 0.968912\n",
      " 8457/10000: episode: 994, duration: 0.088s, episode steps: 10, steps per second: 113, episode reward: 1.000, mean reward: 0.100 [0.000, 1.000], mean action: 1.500 [0.000, 3.000], mean observation: 6.100 [1.000, 15.000], loss: 0.000520, mean_q: 0.969064\n",
      " 8463/10000: episode: 995, duration: 0.056s, episode steps: 6, steps per second: 107, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000585, mean_q: 0.969427\n",
      " 8469/10000: episode: 996, duration: 0.056s, episode steps: 6, steps per second: 107, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000433, mean_q: 0.969415\n",
      " 8475/10000: episode: 997, duration: 0.055s, episode steps: 6, steps per second: 109, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000431, mean_q: 0.970013\n",
      " 8481/10000: episode: 998, duration: 0.057s, episode steps: 6, steps per second: 106, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001009, mean_q: 0.969065\n",
      " 8487/10000: episode: 999, duration: 0.056s, episode steps: 6, steps per second: 108, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000855, mean_q: 0.969666\n",
      " 8493/10000: episode: 1000, duration: 0.055s, episode steps: 6, steps per second: 108, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000724, mean_q: 0.968828\n",
      " 8499/10000: episode: 1001, duration: 0.057s, episode steps: 6, steps per second: 106, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000291, mean_q: 0.969306\n",
      " 8505/10000: episode: 1002, duration: 0.057s, episode steps: 6, steps per second: 105, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000292, mean_q: 0.969721\n",
      " 8511/10000: episode: 1003, duration: 0.055s, episode steps: 6, steps per second: 108, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000853, mean_q: 0.969405\n",
      " 8517/10000: episode: 1004, duration: 0.055s, episode steps: 6, steps per second: 108, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000726, mean_q: 0.969822\n",
      " 8523/10000: episode: 1005, duration: 0.055s, episode steps: 6, steps per second: 110, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000859, mean_q: 0.968720\n",
      " 8526/10000: episode: 1006, duration: 0.032s, episode steps: 3, steps per second: 94, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 2.000 [1.000, 3.000], mean observation: 2.000 [0.000, 5.000], loss: 0.000296, mean_q: 0.969355\n",
      " 8532/10000: episode: 1007, duration: 0.058s, episode steps: 6, steps per second: 103, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000993, mean_q: 0.969424\n",
      " 8538/10000: episode: 1008, duration: 0.058s, episode steps: 6, steps per second: 103, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000723, mean_q: 0.970024\n",
      " 8547/10000: episode: 1009, duration: 0.080s, episode steps: 9, steps per second: 113, episode reward: 1.000, mean reward: 0.111 [0.000, 1.000], mean action: 1.556 [0.000, 3.000], mean observation: 6.111 [1.000, 15.000], loss: 0.000849, mean_q: 0.969249\n",
      " 8551/10000: episode: 1010, duration: 0.039s, episode steps: 4, steps per second: 101, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.250 [0.000, 2.000], mean observation: 3.500 [1.000, 6.000], loss: 0.001067, mean_q: 0.969282\n",
      " 8561/10000: episode: 1011, duration: 0.090s, episode steps: 10, steps per second: 112, episode reward: 1.000, mean reward: 0.100 [0.000, 1.000], mean action: 1.300 [0.000, 2.000], mean observation: 7.800 [1.000, 15.000], loss: 0.000514, mean_q: 0.969764\n",
      " 8567/10000: episode: 1012, duration: 0.056s, episode steps: 6, steps per second: 106, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001008, mean_q: 0.970094\n",
      " 8575/10000: episode: 1013, duration: 0.071s, episode steps: 8, steps per second: 112, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.625 [1.000, 3.000], mean observation: 7.000 [1.000, 15.000], loss: 0.000538, mean_q: 0.970112\n",
      " 8581/10000: episode: 1014, duration: 0.056s, episode steps: 6, steps per second: 107, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000563, mean_q: 0.969470\n",
      " 8587/10000: episode: 1015, duration: 0.057s, episode steps: 6, steps per second: 106, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001166, mean_q: 0.969302\n",
      " 8593/10000: episode: 1016, duration: 0.055s, episode steps: 6, steps per second: 109, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001009, mean_q: 0.969510\n",
      " 8599/10000: episode: 1017, duration: 0.057s, episode steps: 6, steps per second: 106, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000578, mean_q: 0.969160\n",
      " 8603/10000: episode: 1018, duration: 0.040s, episode steps: 4, steps per second: 101, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.750 [1.000, 2.000], mean observation: 4.000 [1.000, 7.000], loss: 0.000854, mean_q: 0.968599\n",
      " 8609/10000: episode: 1019, duration: 0.055s, episode steps: 6, steps per second: 108, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000719, mean_q: 0.969802\n",
      " 8617/10000: episode: 1020, duration: 0.072s, episode steps: 8, steps per second: 110, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.375 [0.000, 2.000], mean observation: 6.625 [1.000, 15.000], loss: 0.000749, mean_q: 0.970282\n",
      " 8627/10000: episode: 1021, duration: 0.087s, episode steps: 10, steps per second: 114, episode reward: 1.000, mean reward: 0.100 [0.000, 1.000], mean action: 1.300 [0.000, 2.000], mean observation: 7.800 [1.000, 15.000], loss: 0.000523, mean_q: 0.969597\n",
      " 8634/10000: episode: 1022, duration: 0.065s, episode steps: 7, steps per second: 108, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.429 [0.000, 2.000], mean observation: 4.429 [0.000, 11.000], loss: 0.000979, mean_q: 0.969222\n",
      " 8640/10000: episode: 1023, duration: 0.056s, episode steps: 6, steps per second: 107, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000568, mean_q: 0.969465\n",
      " 8648/10000: episode: 1024, duration: 0.073s, episode steps: 8, steps per second: 109, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.875 [1.000, 3.000], mean observation: 6.250 [1.000, 15.000], loss: 0.000432, mean_q: 0.969724\n",
      " 8654/10000: episode: 1025, duration: 0.056s, episode steps: 6, steps per second: 107, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000721, mean_q: 0.969454\n",
      " 8658/10000: episode: 1026, duration: 0.042s, episode steps: 4, steps per second: 96, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.250 [0.000, 2.000], mean observation: 3.500 [1.000, 6.000], loss: 0.001069, mean_q: 0.969144\n",
      " 8664/10000: episode: 1027, duration: 0.056s, episode steps: 6, steps per second: 106, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001139, mean_q: 0.969592\n",
      " 8670/10000: episode: 1028, duration: 0.064s, episode steps: 6, steps per second: 94, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000723, mean_q: 0.970388\n",
      " 8676/10000: episode: 1029, duration: 0.073s, episode steps: 6, steps per second: 83, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000428, mean_q: 0.969023\n",
      " 8682/10000: episode: 1030, duration: 0.061s, episode steps: 6, steps per second: 98, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000566, mean_q: 0.969165\n",
      " 8688/10000: episode: 1031, duration: 0.056s, episode steps: 6, steps per second: 108, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.833 [1.000, 3.000], mean observation: 5.000 [0.000, 11.000], loss: 0.000441, mean_q: 0.970139\n",
      " 8694/10000: episode: 1032, duration: 0.055s, episode steps: 6, steps per second: 108, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000583, mean_q: 0.969887\n",
      " 8700/10000: episode: 1033, duration: 0.055s, episode steps: 6, steps per second: 108, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000446, mean_q: 0.969597\n",
      " 8707/10000: episode: 1034, duration: 0.065s, episode steps: 7, steps per second: 107, episode reward: 1.000, mean reward: 0.143 [0.000, 1.000], mean action: 1.714 [1.000, 3.000], mean observation: 7.000 [1.000, 15.000], loss: 0.000974, mean_q: 0.969234\n",
      " 8713/10000: episode: 1035, duration: 0.114s, episode steps: 6, steps per second: 52, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000433, mean_q: 0.969894\n",
      " 8719/10000: episode: 1036, duration: 0.056s, episode steps: 6, steps per second: 107, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000569, mean_q: 0.970262\n",
      " 8727/10000: episode: 1037, duration: 0.074s, episode steps: 8, steps per second: 108, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.375 [0.000, 2.000], mean observation: 6.375 [1.000, 15.000], loss: 0.000753, mean_q: 0.970259\n",
      " 8734/10000: episode: 1038, duration: 0.063s, episode steps: 7, steps per second: 111, episode reward: 1.000, mean reward: 0.143 [0.000, 1.000], mean action: 1.714 [1.000, 3.000], mean observation: 7.000 [1.000, 15.000], loss: 0.000489, mean_q: 0.969086\n",
      " 8740/10000: episode: 1039, duration: 0.059s, episode steps: 6, steps per second: 102, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000580, mean_q: 0.968841\n",
      " 8746/10000: episode: 1040, duration: 0.056s, episode steps: 6, steps per second: 108, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001009, mean_q: 0.969299\n",
      " 8752/10000: episode: 1041, duration: 0.059s, episode steps: 6, steps per second: 101, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000713, mean_q: 0.970632\n",
      " 8758/10000: episode: 1042, duration: 0.057s, episode steps: 6, steps per second: 105, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000580, mean_q: 0.970222\n",
      " 8764/10000: episode: 1043, duration: 0.058s, episode steps: 6, steps per second: 104, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000427, mean_q: 0.969574\n",
      " 8770/10000: episode: 1044, duration: 0.056s, episode steps: 6, steps per second: 107, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000296, mean_q: 0.970168\n",
      " 8776/10000: episode: 1045, duration: 0.058s, episode steps: 6, steps per second: 104, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000872, mean_q: 0.969639\n",
      " 8782/10000: episode: 1046, duration: 0.056s, episode steps: 6, steps per second: 107, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000435, mean_q: 0.969214\n",
      " 8788/10000: episode: 1047, duration: 0.055s, episode steps: 6, steps per second: 109, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000418, mean_q: 0.969959\n",
      " 8794/10000: episode: 1048, duration: 0.056s, episode steps: 6, steps per second: 107, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000848, mean_q: 0.969878\n",
      " 8798/10000: episode: 1049, duration: 0.041s, episode steps: 4, steps per second: 98, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.750 [1.000, 2.000], mean observation: 4.000 [1.000, 7.000], loss: 0.000652, mean_q: 0.969967\n",
      " 8806/10000: episode: 1050, duration: 0.074s, episode steps: 8, steps per second: 109, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.625 [1.000, 3.000], mean observation: 6.500 [0.000, 15.000], loss: 0.000974, mean_q: 0.970285\n",
      " 8812/10000: episode: 1051, duration: 0.056s, episode steps: 6, steps per second: 107, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000596, mean_q: 0.969463\n",
      " 8818/10000: episode: 1052, duration: 0.056s, episode steps: 6, steps per second: 107, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000425, mean_q: 0.968548\n",
      " 8824/10000: episode: 1053, duration: 0.061s, episode steps: 6, steps per second: 99, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000434, mean_q: 0.969273\n",
      " 8830/10000: episode: 1054, duration: 0.062s, episode steps: 6, steps per second: 97, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000592, mean_q: 0.969647\n",
      " 8836/10000: episode: 1055, duration: 0.059s, episode steps: 6, steps per second: 102, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001018, mean_q: 0.968436\n",
      " 8849/10000: episode: 1056, duration: 0.128s, episode steps: 13, steps per second: 102, episode reward: 1.000, mean reward: 0.077 [0.000, 1.000], mean action: 1.462 [0.000, 3.000], mean observation: 5.000 [0.000, 15.000], loss: 0.000402, mean_q: 0.969120\n",
      " 8857/10000: episode: 1057, duration: 0.078s, episode steps: 8, steps per second: 103, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.625 [1.000, 3.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000853, mean_q: 0.969951\n",
      " 8863/10000: episode: 1058, duration: 0.059s, episode steps: 6, steps per second: 102, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000298, mean_q: 0.969293\n",
      " 8870/10000: episode: 1059, duration: 0.067s, episode steps: 7, steps per second: 104, episode reward: 1.000, mean reward: 0.143 [0.000, 1.000], mean action: 1.714 [1.000, 3.000], mean observation: 6.857 [0.000, 15.000], loss: 0.000872, mean_q: 0.969764\n",
      " 8879/10000: episode: 1060, duration: 0.086s, episode steps: 9, steps per second: 104, episode reward: 1.000, mean reward: 0.111 [0.000, 1.000], mean action: 1.222 [0.000, 2.000], mean observation: 5.667 [0.000, 15.000], loss: 0.000388, mean_q: 0.969044\n",
      " 8888/10000: episode: 1061, duration: 0.088s, episode steps: 9, steps per second: 103, episode reward: 1.000, mean reward: 0.111 [0.000, 1.000], mean action: 1.556 [1.000, 3.000], mean observation: 9.556 [1.000, 15.000], loss: 0.000764, mean_q: 0.969211\n",
      " 8894/10000: episode: 1062, duration: 0.124s, episode steps: 6, steps per second: 48, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000721, mean_q: 0.969542\n",
      " 8900/10000: episode: 1063, duration: 0.056s, episode steps: 6, steps per second: 107, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000845, mean_q: 0.970289\n",
      " 8908/10000: episode: 1064, duration: 0.075s, episode steps: 8, steps per second: 107, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.375 [0.000, 2.000], mean observation: 8.750 [1.000, 15.000], loss: 0.000759, mean_q: 0.970047\n",
      " 8910/10000: episode: 1065, duration: 0.031s, episode steps: 2, steps per second: 65, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.500 [1.000, 2.000], mean observation: 3.000 [1.000, 5.000], loss: 0.000870, mean_q: 0.969696\n",
      " 8916/10000: episode: 1066, duration: 0.081s, episode steps: 6, steps per second: 74, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000584, mean_q: 0.969387\n",
      " 8922/10000: episode: 1067, duration: 0.084s, episode steps: 6, steps per second: 71, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000296, mean_q: 0.969112\n",
      " 8928/10000: episode: 1068, duration: 0.075s, episode steps: 6, steps per second: 80, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000297, mean_q: 0.968263\n",
      " 8936/10000: episode: 1069, duration: 0.098s, episode steps: 8, steps per second: 82, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.625 [1.000, 3.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000537, mean_q: 0.969495\n",
      " 8940/10000: episode: 1070, duration: 0.056s, episode steps: 4, steps per second: 72, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.250 [0.000, 2.000], mean observation: 3.500 [1.000, 6.000], loss: 0.001074, mean_q: 0.969641\n",
      " 8946/10000: episode: 1071, duration: 0.078s, episode steps: 6, steps per second: 77, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000847, mean_q: 0.969271\n",
      " 8953/10000: episode: 1072, duration: 0.088s, episode steps: 7, steps per second: 80, episode reward: 1.000, mean reward: 0.143 [0.000, 1.000], mean action: 1.714 [1.000, 3.000], mean observation: 7.000 [1.000, 15.000], loss: 0.000626, mean_q: 0.969874\n",
      " 8959/10000: episode: 1073, duration: 0.061s, episode steps: 6, steps per second: 98, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000874, mean_q: 0.969045\n",
      " 8966/10000: episode: 1074, duration: 0.082s, episode steps: 7, steps per second: 85, episode reward: 1.000, mean reward: 0.143 [0.000, 1.000], mean action: 1.286 [0.000, 2.000], mean observation: 6.857 [0.000, 15.000], loss: 0.000862, mean_q: 0.968942\n",
      " 8973/10000: episode: 1075, duration: 0.067s, episode steps: 7, steps per second: 104, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.714 [1.000, 3.000], mean observation: 7.714 [1.000, 14.000], loss: 0.000746, mean_q: 0.968863\n",
      " 8981/10000: episode: 1076, duration: 0.074s, episode steps: 8, steps per second: 109, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.625 [1.000, 3.000], mean observation: 9.000 [1.000, 15.000], loss: 0.000750, mean_q: 0.969014\n",
      " 8989/10000: episode: 1077, duration: 0.075s, episode steps: 8, steps per second: 107, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.375 [0.000, 2.000], mean observation: 6.125 [0.000, 15.000], loss: 0.000539, mean_q: 0.969101\n",
      " 8995/10000: episode: 1078, duration: 0.060s, episode steps: 6, steps per second: 101, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000705, mean_q: 0.969166\n",
      " 9003/10000: episode: 1079, duration: 0.073s, episode steps: 8, steps per second: 109, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.375 [0.000, 2.000], mean observation: 9.375 [1.000, 15.000], loss: 0.000437, mean_q: 0.969583\n",
      " 9009/10000: episode: 1080, duration: 0.058s, episode steps: 6, steps per second: 104, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000574, mean_q: 0.969212\n",
      " 9015/10000: episode: 1081, duration: 0.065s, episode steps: 6, steps per second: 92, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000844, mean_q: 0.969120\n",
      " 9022/10000: episode: 1082, duration: 0.084s, episode steps: 7, steps per second: 83, episode reward: 1.000, mean reward: 0.143 [0.000, 1.000], mean action: 1.429 [1.000, 2.000], mean observation: 8.857 [1.000, 15.000], loss: 0.000987, mean_q: 0.969047\n",
      " 9028/10000: episode: 1083, duration: 0.078s, episode steps: 6, steps per second: 77, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000702, mean_q: 0.969093\n",
      " 9034/10000: episode: 1084, duration: 0.077s, episode steps: 6, steps per second: 78, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000445, mean_q: 0.969075\n",
      " 9040/10000: episode: 1085, duration: 0.058s, episode steps: 6, steps per second: 104, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000435, mean_q: 0.969333\n",
      " 9046/10000: episode: 1086, duration: 0.058s, episode steps: 6, steps per second: 104, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000848, mean_q: 0.969798\n",
      " 9052/10000: episode: 1087, duration: 0.056s, episode steps: 6, steps per second: 106, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000568, mean_q: 0.969706\n",
      " 9058/10000: episode: 1088, duration: 0.056s, episode steps: 6, steps per second: 108, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000708, mean_q: 0.970403\n",
      " 9064/10000: episode: 1089, duration: 0.058s, episode steps: 6, steps per second: 103, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000287, mean_q: 0.970246\n",
      " 9070/10000: episode: 1090, duration: 0.057s, episode steps: 6, steps per second: 106, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000730, mean_q: 0.970009\n",
      " 9076/10000: episode: 1091, duration: 0.056s, episode steps: 6, steps per second: 107, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000709, mean_q: 0.969846\n",
      " 9082/10000: episode: 1092, duration: 0.055s, episode steps: 6, steps per second: 109, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000567, mean_q: 0.970082\n",
      " 9088/10000: episode: 1093, duration: 0.060s, episode steps: 6, steps per second: 101, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000865, mean_q: 0.970416\n",
      " 9094/10000: episode: 1094, duration: 0.064s, episode steps: 6, steps per second: 93, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000150, mean_q: 0.970400\n",
      " 9100/10000: episode: 1095, duration: 0.064s, episode steps: 6, steps per second: 94, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000300, mean_q: 0.970350\n",
      " 9106/10000: episode: 1096, duration: 0.064s, episode steps: 6, steps per second: 93, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000293, mean_q: 0.969945\n",
      " 9112/10000: episode: 1097, duration: 0.061s, episode steps: 6, steps per second: 98, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000570, mean_q: 0.970741\n",
      " 9118/10000: episode: 1098, duration: 0.056s, episode steps: 6, steps per second: 106, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000718, mean_q: 0.970841\n",
      " 9124/10000: episode: 1099, duration: 0.057s, episode steps: 6, steps per second: 105, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000727, mean_q: 0.970260\n",
      " 9132/10000: episode: 1100, duration: 0.072s, episode steps: 8, steps per second: 112, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.375 [0.000, 2.000], mean observation: 8.750 [1.000, 15.000], loss: 0.000433, mean_q: 0.970705\n",
      " 9138/10000: episode: 1101, duration: 0.059s, episode steps: 6, steps per second: 102, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000585, mean_q: 0.969979\n",
      " 9144/10000: episode: 1102, duration: 0.059s, episode steps: 6, steps per second: 102, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000714, mean_q: 0.969583\n",
      " 9150/10000: episode: 1103, duration: 0.058s, episode steps: 6, steps per second: 104, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000416, mean_q: 0.970309\n",
      " 9158/10000: episode: 1104, duration: 0.086s, episode steps: 8, steps per second: 93, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.375 [0.000, 2.000], mean observation: 8.750 [1.000, 15.000], loss: 0.000541, mean_q: 0.970894\n",
      " 9164/10000: episode: 1105, duration: 0.072s, episode steps: 6, steps per second: 84, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000585, mean_q: 0.970668\n",
      " 9170/10000: episode: 1106, duration: 0.074s, episode steps: 6, steps per second: 82, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001000, mean_q: 0.970613\n",
      " 9176/10000: episode: 1107, duration: 0.062s, episode steps: 6, steps per second: 96, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000440, mean_q: 0.970549\n",
      " 9182/10000: episode: 1108, duration: 0.058s, episode steps: 6, steps per second: 104, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000285, mean_q: 0.970313\n",
      " 9188/10000: episode: 1109, duration: 0.059s, episode steps: 6, steps per second: 102, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000721, mean_q: 0.970942\n",
      " 9194/10000: episode: 1110, duration: 0.056s, episode steps: 6, steps per second: 107, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000300, mean_q: 0.970416\n",
      " 9200/10000: episode: 1111, duration: 0.056s, episode steps: 6, steps per second: 108, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000305, mean_q: 0.970827\n",
      " 9207/10000: episode: 1112, duration: 0.066s, episode steps: 7, steps per second: 107, episode reward: 1.000, mean reward: 0.143 [0.000, 1.000], mean action: 1.429 [1.000, 2.000], mean observation: 8.857 [1.000, 15.000], loss: 0.000499, mean_q: 0.970415\n",
      " 9213/10000: episode: 1113, duration: 0.060s, episode steps: 6, steps per second: 100, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000578, mean_q: 0.970178\n",
      " 9221/10000: episode: 1114, duration: 0.131s, episode steps: 8, steps per second: 61, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.375 [0.000, 2.000], mean observation: 8.750 [1.000, 15.000], loss: 0.000427, mean_q: 0.970488\n",
      " 9227/10000: episode: 1115, duration: 0.082s, episode steps: 6, steps per second: 73, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000418, mean_q: 0.971014\n",
      " 9231/10000: episode: 1116, duration: 0.053s, episode steps: 4, steps per second: 75, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.250 [0.000, 2.000], mean observation: 3.500 [1.000, 6.000], loss: 0.000437, mean_q: 0.970915\n",
      " 9237/10000: episode: 1117, duration: 0.075s, episode steps: 6, steps per second: 81, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000152, mean_q: 0.971690\n",
      " 9243/10000: episode: 1118, duration: 0.065s, episode steps: 6, steps per second: 92, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000734, mean_q: 0.971465\n",
      " 9249/10000: episode: 1119, duration: 0.059s, episode steps: 6, steps per second: 101, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000436, mean_q: 0.970600\n",
      " 9255/10000: episode: 1120, duration: 0.056s, episode steps: 6, steps per second: 107, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000720, mean_q: 0.971354\n",
      " 9262/10000: episode: 1121, duration: 0.064s, episode steps: 7, steps per second: 109, episode reward: 1.000, mean reward: 0.143 [0.000, 1.000], mean action: 1.714 [1.000, 3.000], mean observation: 7.000 [1.000, 15.000], loss: 0.000973, mean_q: 0.970615\n",
      " 9268/10000: episode: 1122, duration: 0.055s, episode steps: 6, steps per second: 108, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000564, mean_q: 0.971072\n",
      " 9275/10000: episode: 1123, duration: 0.065s, episode steps: 7, steps per second: 107, episode reward: 1.000, mean reward: 0.143 [0.000, 1.000], mean action: 1.429 [1.000, 2.000], mean observation: 8.857 [1.000, 15.000], loss: 0.000377, mean_q: 0.971398\n",
      " 9283/10000: episode: 1124, duration: 0.072s, episode steps: 8, steps per second: 111, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.625 [1.000, 3.000], mean observation: 6.500 [0.000, 15.000], loss: 0.000537, mean_q: 0.970981\n",
      " 9289/10000: episode: 1125, duration: 0.056s, episode steps: 6, steps per second: 108, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000148, mean_q: 0.970468\n",
      " 9295/10000: episode: 1126, duration: 0.058s, episode steps: 6, steps per second: 103, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000565, mean_q: 0.970371\n",
      " 9299/10000: episode: 1127, duration: 0.047s, episode steps: 4, steps per second: 85, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.750 [1.000, 2.000], mean observation: 4.000 [1.000, 7.000], loss: 0.000643, mean_q: 0.970944\n",
      " 9308/10000: episode: 1128, duration: 0.094s, episode steps: 9, steps per second: 96, episode reward: 1.000, mean reward: 0.111 [0.000, 1.000], mean action: 1.444 [0.000, 3.000], mean observation: 5.778 [0.000, 15.000], loss: 0.000684, mean_q: 0.970612\n",
      " 9314/10000: episode: 1129, duration: 0.069s, episode steps: 6, steps per second: 87, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000723, mean_q: 0.970352\n",
      " 9320/10000: episode: 1130, duration: 0.060s, episode steps: 6, steps per second: 99, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000286, mean_q: 0.970470\n",
      " 9324/10000: episode: 1131, duration: 0.041s, episode steps: 4, steps per second: 98, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.750 [1.000, 2.000], mean observation: 4.000 [1.000, 7.000], loss: 0.000643, mean_q: 0.970359\n",
      " 9330/10000: episode: 1132, duration: 0.067s, episode steps: 6, steps per second: 89, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000568, mean_q: 0.970827\n",
      " 9336/10000: episode: 1133, duration: 0.075s, episode steps: 6, steps per second: 80, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000732, mean_q: 0.970577\n",
      " 9342/10000: episode: 1134, duration: 0.074s, episode steps: 6, steps per second: 82, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000714, mean_q: 0.970404\n",
      " 9348/10000: episode: 1135, duration: 0.070s, episode steps: 6, steps per second: 85, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000151, mean_q: 0.970825\n",
      " 9353/10000: episode: 1136, duration: 0.060s, episode steps: 5, steps per second: 83, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.600 [1.000, 2.000], mean observation: 6.000 [1.000, 11.000], loss: 0.000179, mean_q: 0.971073\n",
      " 9359/10000: episode: 1137, duration: 0.063s, episode steps: 6, steps per second: 95, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000421, mean_q: 0.971048\n",
      " 9365/10000: episode: 1138, duration: 0.058s, episode steps: 6, steps per second: 103, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000426, mean_q: 0.971989\n",
      " 9371/10000: episode: 1139, duration: 0.056s, episode steps: 6, steps per second: 106, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000293, mean_q: 0.970916\n",
      " 9378/10000: episode: 1140, duration: 0.065s, episode steps: 7, steps per second: 108, episode reward: 1.000, mean reward: 0.143 [0.000, 1.000], mean action: 1.714 [1.000, 3.000], mean observation: 7.000 [1.000, 15.000], loss: 0.000610, mean_q: 0.971127\n",
      " 9384/10000: episode: 1141, duration: 0.058s, episode steps: 6, steps per second: 104, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000286, mean_q: 0.970504\n",
      " 9390/10000: episode: 1142, duration: 0.071s, episode steps: 6, steps per second: 85, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000157, mean_q: 0.971098\n",
      " 9397/10000: episode: 1143, duration: 0.143s, episode steps: 7, steps per second: 49, episode reward: 1.000, mean reward: 0.143 [0.000, 1.000], mean action: 1.714 [1.000, 3.000], mean observation: 7.143 [1.000, 15.000], loss: 0.000721, mean_q: 0.971343\n",
      " 9407/10000: episode: 1144, duration: 0.125s, episode steps: 10, steps per second: 80, episode reward: 1.000, mean reward: 0.100 [0.000, 1.000], mean action: 1.500 [0.000, 3.000], mean observation: 9.100 [1.000, 15.000], loss: 0.000271, mean_q: 0.971159\n",
      " 9413/10000: episode: 1145, duration: 0.067s, episode steps: 6, steps per second: 90, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.167 [0.000, 2.000], mean observation: 3.167 [1.000, 6.000], loss: 0.000421, mean_q: 0.971373\n",
      " 9419/10000: episode: 1146, duration: 0.064s, episode steps: 6, steps per second: 93, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000294, mean_q: 0.972028\n",
      " 9424/10000: episode: 1147, duration: 0.055s, episode steps: 5, steps per second: 90, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 2.000 [1.000, 3.000], mean observation: 3.400 [1.000, 7.000], loss: 0.000332, mean_q: 0.970763\n",
      " 9432/10000: episode: 1148, duration: 0.074s, episode steps: 8, steps per second: 108, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.375 [0.000, 2.000], mean observation: 8.750 [1.000, 15.000], loss: 0.000545, mean_q: 0.971698\n",
      " 9438/10000: episode: 1149, duration: 0.056s, episode steps: 6, steps per second: 107, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000288, mean_q: 0.971180\n",
      " 9448/10000: episode: 1150, duration: 0.089s, episode steps: 10, steps per second: 113, episode reward: 1.000, mean reward: 0.100 [0.000, 1.000], mean action: 1.500 [0.000, 3.000], mean observation: 6.900 [1.000, 15.000], loss: 0.000357, mean_q: 0.970945\n",
      " 9454/10000: episode: 1151, duration: 0.059s, episode steps: 6, steps per second: 102, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000982, mean_q: 0.971763\n",
      " 9461/10000: episode: 1152, duration: 0.074s, episode steps: 7, steps per second: 94, episode reward: 1.000, mean reward: 0.143 [0.000, 1.000], mean action: 1.286 [0.000, 2.000], mean observation: 6.857 [0.000, 15.000], loss: 0.000131, mean_q: 0.971558\n",
      " 9467/10000: episode: 1153, duration: 0.066s, episode steps: 6, steps per second: 91, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000292, mean_q: 0.971130\n",
      " 9471/10000: episode: 1154, duration: 0.047s, episode steps: 4, steps per second: 85, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.250 [0.000, 2.000], mean observation: 3.500 [1.000, 6.000], loss: 0.000228, mean_q: 0.971554\n",
      " 9477/10000: episode: 1155, duration: 0.070s, episode steps: 6, steps per second: 86, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.500 [0.000, 3.000], mean observation: 3.667 [1.000, 6.000], loss: 0.000003, mean_q: 0.971413\n",
      " 9485/10000: episode: 1156, duration: 0.090s, episode steps: 8, steps per second: 88, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.625 [1.000, 3.000], mean observation: 9.000 [1.000, 15.000], loss: 0.000420, mean_q: 0.971206\n",
      " 9491/10000: episode: 1157, duration: 0.075s, episode steps: 6, steps per second: 80, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000144, mean_q: 0.971839\n",
      " 9497/10000: episode: 1158, duration: 0.073s, episode steps: 6, steps per second: 82, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 10.500 [4.000, 15.000], loss: 0.000438, mean_q: 0.971883\n",
      " 9503/10000: episode: 1159, duration: 0.061s, episode steps: 6, steps per second: 98, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000149, mean_q: 0.971576\n",
      " 9509/10000: episode: 1160, duration: 0.064s, episode steps: 6, steps per second: 94, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000430, mean_q: 0.971604\n",
      " 9515/10000: episode: 1161, duration: 0.063s, episode steps: 6, steps per second: 95, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000147, mean_q: 0.972006\n",
      " 9521/10000: episode: 1162, duration: 0.062s, episode steps: 6, steps per second: 98, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000437, mean_q: 0.971770\n",
      " 9525/10000: episode: 1163, duration: 0.042s, episode steps: 4, steps per second: 96, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.750 [1.000, 2.000], mean observation: 4.000 [1.000, 7.000], loss: 0.000406, mean_q: 0.971994\n",
      " 9531/10000: episode: 1164, duration: 0.058s, episode steps: 6, steps per second: 103, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000283, mean_q: 0.971874\n",
      " 9537/10000: episode: 1165, duration: 0.061s, episode steps: 6, steps per second: 99, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000282, mean_q: 0.972174\n",
      " 9545/10000: episode: 1166, duration: 0.099s, episode steps: 8, steps per second: 81, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.375 [1.000, 2.000], mean observation: 9.500 [1.000, 15.000], loss: 0.000643, mean_q: 0.972088\n",
      " 9551/10000: episode: 1167, duration: 0.077s, episode steps: 6, steps per second: 78, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000280, mean_q: 0.972023\n",
      " 9559/10000: episode: 1168, duration: 0.102s, episode steps: 8, steps per second: 78, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.375 [0.000, 2.000], mean observation: 6.625 [1.000, 15.000], loss: 0.000117, mean_q: 0.971886\n",
      " 9566/10000: episode: 1169, duration: 0.091s, episode steps: 7, steps per second: 77, episode reward: 1.000, mean reward: 0.143 [0.000, 1.000], mean action: 1.429 [1.000, 2.000], mean observation: 8.857 [1.000, 15.000], loss: 0.000010, mean_q: 0.972158\n",
      " 9572/10000: episode: 1170, duration: 0.076s, episode steps: 6, steps per second: 79, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.001145, mean_q: 0.972601\n",
      " 9578/10000: episode: 1171, duration: 0.075s, episode steps: 6, steps per second: 80, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000420, mean_q: 0.971266\n",
      " 9582/10000: episode: 1172, duration: 0.056s, episode steps: 4, steps per second: 72, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.250 [0.000, 2.000], mean observation: 3.500 [1.000, 6.000], loss: 0.000227, mean_q: 0.971321\n",
      " 9588/10000: episode: 1173, duration: 0.081s, episode steps: 6, steps per second: 74, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000561, mean_q: 0.971175\n",
      " 9594/10000: episode: 1174, duration: 0.070s, episode steps: 6, steps per second: 86, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000574, mean_q: 0.971517\n",
      " 9602/10000: episode: 1175, duration: 0.075s, episode steps: 8, steps per second: 107, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.375 [0.000, 2.000], mean observation: 6.125 [0.000, 15.000], loss: 0.000326, mean_q: 0.971635\n",
      " 9606/10000: episode: 1176, duration: 0.041s, episode steps: 4, steps per second: 97, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.750 [1.000, 2.000], mean observation: 4.000 [1.000, 7.000], loss: 0.000632, mean_q: 0.971617\n",
      " 9614/10000: episode: 1177, duration: 0.076s, episode steps: 8, steps per second: 106, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.375 [0.000, 2.000], mean observation: 9.375 [1.000, 15.000], loss: 0.000656, mean_q: 0.971496\n",
      " 9621/10000: episode: 1178, duration: 0.067s, episode steps: 7, steps per second: 105, episode reward: 1.000, mean reward: 0.143 [0.000, 1.000], mean action: 1.429 [1.000, 2.000], mean observation: 8.857 [1.000, 15.000], loss: 0.000609, mean_q: 0.971156\n",
      " 9627/10000: episode: 1179, duration: 0.062s, episode steps: 6, steps per second: 96, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000296, mean_q: 0.971727\n",
      " 9633/10000: episode: 1180, duration: 0.059s, episode steps: 6, steps per second: 101, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000159, mean_q: 0.971124\n",
      " 9640/10000: episode: 1181, duration: 0.066s, episode steps: 7, steps per second: 106, episode reward: 1.000, mean reward: 0.143 [0.000, 1.000], mean action: 1.714 [1.000, 3.000], mean observation: 7.143 [1.000, 15.000], loss: 0.000615, mean_q: 0.970667\n",
      " 9646/10000: episode: 1182, duration: 0.064s, episode steps: 6, steps per second: 94, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000566, mean_q: 0.970639\n",
      " 9652/10000: episode: 1183, duration: 0.073s, episode steps: 6, steps per second: 82, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000155, mean_q: 0.970895\n",
      " 9661/10000: episode: 1184, duration: 0.099s, episode steps: 9, steps per second: 91, episode reward: 1.000, mean reward: 0.111 [0.000, 1.000], mean action: 1.333 [0.000, 2.000], mean observation: 7.000 [0.000, 15.000], loss: 0.000288, mean_q: 0.971285\n",
      " 9667/10000: episode: 1185, duration: 0.067s, episode steps: 6, steps per second: 90, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000436, mean_q: 0.971334\n",
      " 9673/10000: episode: 1186, duration: 0.061s, episode steps: 6, steps per second: 98, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000567, mean_q: 0.971076\n",
      " 9679/10000: episode: 1187, duration: 0.059s, episode steps: 6, steps per second: 101, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000143, mean_q: 0.971390\n",
      " 9684/10000: episode: 1188, duration: 0.050s, episode steps: 5, steps per second: 100, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 2.000 [1.000, 3.000], mean observation: 3.600 [1.000, 7.000], loss: 0.000505, mean_q: 0.971483\n",
      " 9688/10000: episode: 1189, duration: 0.042s, episode steps: 4, steps per second: 96, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.750 [1.000, 2.000], mean observation: 4.000 [1.000, 7.000], loss: 0.000637, mean_q: 0.970860\n",
      " 9695/10000: episode: 1190, duration: 0.066s, episode steps: 7, steps per second: 106, episode reward: 1.000, mean reward: 0.143 [0.000, 1.000], mean action: 1.714 [1.000, 3.000], mean observation: 6.857 [0.000, 15.000], loss: 0.000126, mean_q: 0.971484\n",
      " 9701/10000: episode: 1191, duration: 0.058s, episode steps: 6, steps per second: 104, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000278, mean_q: 0.972226\n",
      " 9707/10000: episode: 1192, duration: 0.057s, episode steps: 6, steps per second: 105, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000294, mean_q: 0.971103\n",
      " 9713/10000: episode: 1193, duration: 0.057s, episode steps: 6, steps per second: 104, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000005, mean_q: 0.971630\n",
      " 9719/10000: episode: 1194, duration: 0.058s, episode steps: 6, steps per second: 103, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000295, mean_q: 0.971298\n",
      " 9725/10000: episode: 1195, duration: 0.057s, episode steps: 6, steps per second: 105, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000293, mean_q: 0.971706\n",
      " 9731/10000: episode: 1196, duration: 0.115s, episode steps: 6, steps per second: 52, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000291, mean_q: 0.971418\n",
      " 9737/10000: episode: 1197, duration: 0.057s, episode steps: 6, steps per second: 105, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000568, mean_q: 0.971667\n",
      " 9746/10000: episode: 1198, duration: 0.080s, episode steps: 9, steps per second: 112, episode reward: 1.000, mean reward: 0.111 [0.000, 1.000], mean action: 1.444 [0.000, 3.000], mean observation: 5.778 [0.000, 15.000], loss: 0.000383, mean_q: 0.971545\n",
      " 9753/10000: episode: 1199, duration: 0.064s, episode steps: 7, steps per second: 109, episode reward: 1.000, mean reward: 0.143 [0.000, 1.000], mean action: 1.714 [1.000, 3.000], mean observation: 7.143 [1.000, 15.000], loss: 0.000247, mean_q: 0.971439\n",
      " 9761/10000: episode: 1200, duration: 0.073s, episode steps: 8, steps per second: 109, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.375 [0.000, 2.000], mean observation: 9.375 [1.000, 15.000], loss: 0.000533, mean_q: 0.971650\n",
      " 9767/10000: episode: 1201, duration: 0.057s, episode steps: 6, steps per second: 106, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000155, mean_q: 0.971698\n",
      " 9770/10000: episode: 1202, duration: 0.033s, episode steps: 3, steps per second: 91, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.000 [0.000, 2.000], mean observation: 2.000 [0.000, 5.000], loss: 0.000850, mean_q: 0.972048\n",
      " 9776/10000: episode: 1203, duration: 0.057s, episode steps: 6, steps per second: 105, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000433, mean_q: 0.971496\n",
      " 9784/10000: episode: 1204, duration: 0.075s, episode steps: 8, steps per second: 106, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.375 [0.000, 2.000], mean observation: 9.375 [1.000, 15.000], loss: 0.000432, mean_q: 0.971293\n",
      " 9790/10000: episode: 1205, duration: 0.058s, episode steps: 6, steps per second: 104, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000568, mean_q: 0.971404\n",
      " 9796/10000: episode: 1206, duration: 0.058s, episode steps: 6, steps per second: 104, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000305, mean_q: 0.971468\n",
      " 9802/10000: episode: 1207, duration: 0.057s, episode steps: 6, steps per second: 105, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000433, mean_q: 0.971258\n",
      " 9808/10000: episode: 1208, duration: 0.059s, episode steps: 6, steps per second: 101, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000573, mean_q: 0.970877\n",
      " 9816/10000: episode: 1209, duration: 0.073s, episode steps: 8, steps per second: 109, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.375 [0.000, 2.000], mean observation: 8.750 [1.000, 15.000], loss: 0.000437, mean_q: 0.971071\n",
      " 9823/10000: episode: 1210, duration: 0.064s, episode steps: 7, steps per second: 109, episode reward: 1.000, mean reward: 0.143 [0.000, 1.000], mean action: 1.714 [1.000, 3.000], mean observation: 7.000 [1.000, 15.000], loss: 0.000489, mean_q: 0.971015\n",
      " 9831/10000: episode: 1211, duration: 0.073s, episode steps: 8, steps per second: 109, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.625 [1.000, 3.000], mean observation: 9.000 [1.000, 15.000], loss: 0.000440, mean_q: 0.970816\n",
      " 9837/10000: episode: 1212, duration: 0.057s, episode steps: 6, steps per second: 105, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000568, mean_q: 0.971626\n",
      " 9843/10000: episode: 1213, duration: 0.058s, episode steps: 6, steps per second: 103, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000590, mean_q: 0.971395\n",
      " 9849/10000: episode: 1214, duration: 0.057s, episode steps: 6, steps per second: 106, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000301, mean_q: 0.971291\n",
      " 9851/10000: episode: 1215, duration: 0.025s, episode steps: 2, steps per second: 79, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.500 [1.000, 2.000], mean observation: 3.000 [1.000, 5.000], loss: 0.000445, mean_q: 0.969800\n",
      " 9858/10000: episode: 1216, duration: 0.068s, episode steps: 7, steps per second: 104, episode reward: 1.000, mean reward: 0.143 [0.000, 1.000], mean action: 1.429 [1.000, 2.000], mean observation: 8.857 [1.000, 15.000], loss: 0.000127, mean_q: 0.970191\n",
      " 9864/10000: episode: 1217, duration: 0.057s, episode steps: 6, steps per second: 105, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000434, mean_q: 0.971011\n",
      " 9870/10000: episode: 1218, duration: 0.059s, episode steps: 6, steps per second: 101, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000151, mean_q: 0.971632\n",
      " 9876/10000: episode: 1219, duration: 0.057s, episode steps: 6, steps per second: 106, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000295, mean_q: 0.971917\n",
      " 9882/10000: episode: 1220, duration: 0.059s, episode steps: 6, steps per second: 103, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000006, mean_q: 0.971402\n",
      " 9888/10000: episode: 1221, duration: 0.057s, episode steps: 6, steps per second: 105, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000150, mean_q: 0.970986\n",
      " 9894/10000: episode: 1222, duration: 0.057s, episode steps: 6, steps per second: 105, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000284, mean_q: 0.971429\n",
      " 9896/10000: episode: 1223, duration: 0.026s, episode steps: 2, steps per second: 77, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.500 [1.000, 2.000], mean observation: 3.000 [1.000, 5.000], loss: 0.000448, mean_q: 0.970751\n",
      " 9902/10000: episode: 1224, duration: 0.057s, episode steps: 6, steps per second: 105, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000290, mean_q: 0.971136\n",
      " 9904/10000: episode: 1225, duration: 0.081s, episode steps: 2, steps per second: 25, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.500 [1.000, 2.000], mean observation: 3.000 [1.000, 5.000], loss: 0.000005, mean_q: 0.972030\n",
      " 9910/10000: episode: 1226, duration: 0.062s, episode steps: 6, steps per second: 97, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000003, mean_q: 0.971655\n",
      " 9916/10000: episode: 1227, duration: 0.057s, episode steps: 6, steps per second: 105, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000292, mean_q: 0.971919\n",
      " 9922/10000: episode: 1228, duration: 0.058s, episode steps: 6, steps per second: 104, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000150, mean_q: 0.971961\n",
      " 9926/10000: episode: 1229, duration: 0.042s, episode steps: 4, steps per second: 95, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.250 [0.000, 2.000], mean observation: 3.500 [1.000, 6.000], loss: 0.000428, mean_q: 0.971240\n",
      " 9934/10000: episode: 1230, duration: 0.073s, episode steps: 8, steps per second: 109, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.625 [1.000, 3.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000541, mean_q: 0.970751\n",
      " 9940/10000: episode: 1231, duration: 0.055s, episode steps: 6, steps per second: 108, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000562, mean_q: 0.971273\n",
      " 9946/10000: episode: 1232, duration: 0.056s, episode steps: 6, steps per second: 108, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000419, mean_q: 0.971911\n",
      " 9952/10000: episode: 1233, duration: 0.058s, episode steps: 6, steps per second: 103, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000147, mean_q: 0.971861\n",
      " 9956/10000: episode: 1234, duration: 0.041s, episode steps: 4, steps per second: 97, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.750 [1.000, 2.000], mean observation: 4.000 [1.000, 7.000], loss: 0.000434, mean_q: 0.971318\n",
      " 9962/10000: episode: 1235, duration: 0.056s, episode steps: 6, steps per second: 106, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000286, mean_q: 0.972033\n",
      " 9970/10000: episode: 1236, duration: 0.075s, episode steps: 8, steps per second: 107, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.625 [1.000, 3.000], mean observation: 9.000 [1.000, 15.000], loss: 0.000316, mean_q: 0.971300\n",
      " 9972/10000: episode: 1237, duration: 0.025s, episode steps: 2, steps per second: 80, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.500 [1.000, 2.000], mean observation: 3.000 [1.000, 5.000], loss: 0.000446, mean_q: 0.971549\n",
      " 9978/10000: episode: 1238, duration: 0.060s, episode steps: 6, steps per second: 100, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000569, mean_q: 0.971831\n",
      " 9986/10000: episode: 1239, duration: 0.074s, episode steps: 8, steps per second: 108, episode reward: 1.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.375 [0.000, 2.000], mean observation: 9.375 [1.000, 15.000], loss: 0.000536, mean_q: 0.971718\n",
      " 9992/10000: episode: 1240, duration: 0.058s, episode steps: 6, steps per second: 104, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000145, mean_q: 0.970519\n",
      " 9994/10000: episode: 1241, duration: 0.025s, episode steps: 2, steps per second: 81, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.500 [1.000, 2.000], mean observation: 3.000 [1.000, 5.000], loss: 0.000006, mean_q: 0.971415\n",
      " 10000/10000: episode: 1242, duration: 0.059s, episode steps: 6, steps per second: 102, episode reward: 1.000, mean reward: 0.167 [0.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 8.000 [1.000, 15.000], loss: 0.000446, mean_q: 0.971682\n",
      "done, took 90.094 seconds\n",
      "Testing for 3 episodes ...\n",
      "Episode 1: reward: 1.000, steps: 6\n",
      "Episode 2: reward: 1.000, steps: 6\n",
      "Episode 3: reward: 1.000, steps: 6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a39fcda58>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#build model for our DQN agent\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(16, 4, input_length=1))\n",
    "model.add(Reshape((4,)))\n",
    "print(model.summary())\n",
    "\n",
    "#parameters for DQN agent\n",
    "memory = SequentialMemory(limit=10000, window_length=1)\n",
    "policy = EpsGreedyQPolicy()\n",
    "actions = env.action_space.n\n",
    "\n",
    "#create instance and compile DQN agent\n",
    "DQN = DQNAgent(model=model, nb_actions=actions, memory=memory, nb_steps_warmup=500, target_model_update=0.01, \n",
    "               policy=policy, enable_double_dqn=False, batch_size=512)\n",
    "DQN.compile(Adam())\n",
    "\n",
    "#train DQN agent\n",
    "metrics = DQN.fit(env, nb_steps=10000, visualize=False, verbose=2, nb_max_episode_steps=100)\n",
    "\n",
    "#test agent on 3 episodes\n",
    "DQN.test(env, nb_episodes=3, visualize=False)\n",
    "\n",
    "#[5] - reference list in report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uncomment below to see steps and reward for each episode\n",
    "\n",
    "# metrics.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.25e-02 1.89e-02 2.96e-01 9.55e-02]\n",
      " [8.59e-02 0.00e+00 3.69e-01 1.20e-01]\n",
      " [1.22e-01 4.80e-01 4.66e-02 1.88e-01]\n",
      " [1.74e-01 0.00e+00 3.22e-03 2.18e-03]\n",
      " [5.73e-04 6.92e-02 0.00e+00 2.34e-03]\n",
      " [0.00e+00 0.00e+00 0.00e+00 0.00e+00]\n",
      " [0.00e+00 6.22e-01 0.00e+00 1.74e-01]\n",
      " [0.00e+00 0.00e+00 0.00e+00 0.00e+00]\n",
      " [2.87e-03 0.00e+00 2.13e-01 1.32e-03]\n",
      " [2.66e-03 3.85e-02 5.09e-01 0.00e+00]\n",
      " [2.16e-01 7.89e-01 0.00e+00 2.45e-01]\n",
      " [0.00e+00 0.00e+00 0.00e+00 0.00e+00]\n",
      " [0.00e+00 0.00e+00 0.00e+00 0.00e+00]\n",
      " [0.00e+00 8.08e-03 5.65e-01 1.14e-02]\n",
      " [2.11e-01 5.68e-01 1.00e+00 3.66e-01]\n",
      " [0.00e+00 0.00e+00 0.00e+00 0.00e+00]]\n"
     ]
    }
   ],
   "source": [
    "#SARSA\n",
    "\n",
    "rewards_all_episodes = []\n",
    "steps_all_episodes = []\n",
    "\n",
    "#set SARSA parameters\n",
    "exploration_rate = 0.9\n",
    "exploration_decay_rate = 0.9999\n",
    "num_episodes = 5000\n",
    "max_steps_per_episode = 100\n",
    "learning_rate = 0.01\n",
    "discount_rate = 0.8\n",
    "\n",
    "#initialize q table with zeros\n",
    "q_table = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "\n",
    "#explore vs exploit using epsilon greedy policy\n",
    "def choose_action(state):\n",
    "    if np.random.uniform(0, 1) < exploration_rate:\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        action = np.argmax(q_table[state,:])\n",
    "    return action\n",
    "\n",
    "#update q table using SARSA algorithm\n",
    "def learn(state, state2, reward, action, action2):\n",
    "    target = reward + discount_rate * q_table[state2, action2]\n",
    "    q_table[state, action] = q_table[state, action] + learning_rate * (target - q_table[state, action])\n",
    "\n",
    "#train agent for 5000 episodes\n",
    "for episode in range(num_episodes):\n",
    "    steps = 0\n",
    "    rewards_current_episode = 0\n",
    "    state = env.reset()\n",
    "    action = choose_action(state)\n",
    "    \n",
    "    #end episode if not over after 100 steps\n",
    "    while steps < max_steps_per_episode:\n",
    "        \n",
    "        state2, reward, done, info = env.step(action)\n",
    "\n",
    "        action2 = choose_action(state2)\n",
    "\n",
    "        learn(state, state2, reward, action, action2)\n",
    "\n",
    "        state = state2\n",
    "        action = action2\n",
    "\n",
    "        rewards_current_episode += reward\n",
    "        steps += 1\n",
    "        exploration_rate = exploration_rate * exploration_decay_rate\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    rewards_all_episodes.append(rewards_current_episode)\n",
    "    steps_all_episodes.append(steps)\n",
    "\n",
    "print(q_table)\n",
    "\n",
    "#[4] - reference list in report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "print(rewards_all_episodes[4900:4950])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uncomment print statements to see values\n",
    "\n",
    "#Calculate average reward per hundred episodes\n",
    "rewards_per_hundred_episodes = np.split(np.array(rewards_all_episodes), num_episodes/100)\n",
    "count = 100\n",
    "# print('avg reward per 100 episodes\\n\\n')\n",
    "for r in rewards_per_hundred_episodes:\n",
    "#     print(count, \" : \", str(sum(r/100)))\n",
    "    count +=100\n",
    "    \n",
    "#Calculate average steps per hundred episodes\n",
    "steps_per_hundred_episodes = np.split(np.array(steps_all_episodes), num_episodes/100)\n",
    "count = 100\n",
    "# print('avg steps per 100 episodes\\n\\n')\n",
    "for r in steps_per_hundred_episodes:\n",
    "#     print(count, \" : \", str(sum(r/100)))\n",
    "    count +=100\n",
    "\n",
    "step_count = 0\n",
    "step_graph = []\n",
    "for s in steps_per_hundred_episodes:\n",
    "    step_graph.append(sum(s/100))\n",
    "    step_count +=100\n",
    "    \n",
    "reward_count = 0\n",
    "reward_graph = []\n",
    "for r in rewards_per_hundred_episodes:\n",
    "    reward_graph.append(sum(r/100))\n",
    "    reward_count +=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1332x756 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot average steps vs reward for SARSA agent\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "color = 'tab:orange'\n",
    "ax1.plot(step_graph, color=color)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "ax1.set_ylabel('Average steps per 100 episodes')\n",
    "ax1.set_xlabel('Episodes (in hundreds)')\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "color = 'tab:blue'\n",
    "ax2.plot(reward_graph, color=color)\n",
    "ax2.set_ylabel('Average reward per 100 episodes')\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.set_size_inches(18.5, 10.5)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
